<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[置顶]]></title>
    <url>%2F2096%2F05%2F21%2F%E7%BD%AE%E9%A1%B6%2F</url>
    <content type="text"><![CDATA[有一些文章是下载的资料为了好归档和博客放在了一起，没有进行格式的整理，以后会进行整理，有的是只建了文章没有内容以后会填坑，见谅。 文章中图片有问题的地方请不要担心，已经正常显示了，不过一个是Hexo版本，一个是Markdown版本 点击阅读全文，查看博客功能更新日志 最近更换next主题，因为原来的主题不支持数学表达式，没做太多配置以后再搞——8.28 新增阅读量功能——2018.11.1 开启侧边栏“关于”，”标签“，”分类“，”归档“功能，开启评论，访问量功能——2019.1.3 开启搜索功能，开启打赏功能，更改代码背景，开启分享功能——2019.1.4 合并，完善某些博客文章——2019.1.7 新增看板娘，秘密更新——2019.7.24]]></content>
      <categories>
        <category>置顶</category>
      </categories>
      <tags>
        <tag>置顶</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[研究生生活记录]]></title>
    <url>%2F2023%2F09%2F23%2F%E7%A0%94%E7%A9%B6%E7%94%9F%E7%94%9F%E6%B4%BB%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[研究生每日生活总结 2019.11 2019.11.1 同学聚会 2019.11.2-3 MLA2019 2019.11.4 读论文《基于深度学习的视频超分辨率算法研究》进度（102/102） 2019.11.5 看TensorFlow教程 吴恩达机器学习第三周课程完成 2019.11.6 吴恩达机器学习第四周课程完成 2019.11.7-8 新增积分统计功能，解决志愿者积分统计异常 2019.11.9-10 使用TensorFlow实现手写数字识别 2019.11.11-12 看吴恩达深度学习视频 2019.11.13-14 看模型 做OpenCV PPT 2019.11.15 着重看SRCNN模型实现 找SRCNN代码 2019.11.16 看FSRNN模型 2019.11.17 拼主机 2019.11.18-24 跑模型SRCNN，FSRCNN，ESPCN，VDSR 看论文《Deep learning for single Image Super Resolution》 吴恩达深度学习课程完成 找慕课资料看pytorch视频 2019.11.25-26 总结《Deep learning for single Image Super Resolution》发展历史 准备CTF 看论文DBPN 跑DBPN模型 看论文SRFBN 运行SRFBN模型代码 看Pytorch课程 2019.10 2019.10.4 养老项目搭建数据库，设计前端页面框架 2019.10.5 养老项目，搭建后台开发环境框架 2019.10.6-2019.10.7 完成前后台基本业务流程 2019.10.8 尝试阅读论文《Deep Image Matting》 英语课 养老项目阶段成果报告 读书《Python计算机视觉编程》P1-P12 2019.10.9 读书《深度学习之PyTorch实战计算机视觉》P1-P40 养老项目整改页面构造，更改任务申请界面与后台逻辑 2019.10.10 养老项目增加志愿者和老人管理页面 2019.10.11 养老项目增加任务管理界面 2019.10.12-2019.10.13 休息 2019.10.14 改bug 慕课两场测试 读书《深度学习之PyTorch实战计算机视觉》P40-P100 2019.10.15-16 读书《深度学习之PyTorch实战计算机视觉》P101-P158 搞懂反向传播和梯度下降 2019.10.17 读书《深度学习之PyTorch实战计算机视觉》P159-P172 2019.10.18 读书《深度学习之PyTorch实战计算机视觉》P72-P180 2019.10.19 看吴恩达视频 达内测试 2019.10.20 完成吴恩达机器学习第一周课程 2019.10.21 学习matlab基本语法知识 2019.10.22-23 改项目 2019.10.24 居委会验收项目 2019.10.25 英语课 大组会 2019.10.26 吴恩达第二周课程完成 2019.10.27 完成编程题 2019.10.28 更改数据库，添加地址 重置密码 更改密码 2019.10.29 个人信息显示 修改密码逻辑 认证过期跳转 2019.10.30 看吴恩达课程 把drcom破解 2019.10.31 读论文《基于深度学习的视频超分辨率算法研究》进度（37/102） 2019.9 2019.9.23 标准日本语29课 吴恩达深度学习第一章2.10-2.18&amp;3.1-3.3 2019.9.24 上英语课 连服务器下载训练集 整理104实验室 2019.9.25 调试github语音处理项目 研究生项目组会 养老项目开会 2019.9.26 英语口语课 思修课 搬到实验室 吴恩达深度学习第一章3.4-3.8 吐槽：实验室的网好卡，据说在修网 2019.9.27 6:00研究生体检 第二大节英语课 养老项目开会近5个小时 开班会选班干部 吴恩达深度学习第一章3.9-3.11 中午因为要开会没睡，好困~ 2019.9.28 吴恩达深度学习第一章4.1-4.8 标准日本语30课 2019.9.29 英语课 调试基本机器学习算法Demo 开养老项目会 2019.9.30——2019.10.3 国庆游玩]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DBPN]]></title>
    <url>%2F2019%2F11%2F28%2F%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%2FDBPN%2F</url>
    <content type="text"><![CDATA[Deep Back-Projection Networks for Single Image Super-resolution office link: https://alterzero.github.io/projects/DBPN.html Abstract最近提出的深超分辨率网络的前馈结构学习低分辨率输入的特征和从低分辨率输入到高分辨率输出的非线性映射。然而，这种方法不能完全解决低分辨率和高分辨率图像之间的相互依赖关系。我们提出了深度反投影网络(DBPN)，利用迭代的上下采样层，这些层以一个单元形成，为投影误差提供一个误差反馈机制。 我们构造了相互连接的上采样单元和下采样单元，每个单元代表不同类型的图像退化和高分辨率组件。 我们也展示了将这个概念扩展到应用最新的深度网络趋势的几个变体，例如递归网络、dense connection和残差学习，以提高性能。实现了soat效果，尤其对例如×8等大尺寸任务的处理。 Introduction]]></content>
  </entry>
  <entry>
    <title><![CDATA[DL For SISR]]></title>
    <url>%2F2019%2F11%2F24%2F%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%2FDL%20For%20SISR%2F</url>
    <content type="text"><![CDATA[使用深度学习进行超分辨率发展历史总结 paper link： https://arxiv.org/abs/1808.03344 这是一篇综述性的文章发表在TMM上，文章总结了自SRCNN以来超分辨率在深度学习上的发展情况 Introduction深度学习是机器学习的分支，在人工智能领域，深度学习表现出比普通机器学习方法更好的表现效果 超分辨率的一个众所周知的问题就是不适定问题，因为讲低分辨率映射到高分辨率是很困难的。先前的 SISR （ Single Image Super-Resolution ）方法有两个主要缺点： 不能明确定义从低分辨率到高分辨率之见的映射 用原生数据建立一个复杂的高维映射是很低效的 不过近几年基于深度学习的超分辨率不论从质量和数量上都有很高的提升 在本论文中作者会对近来基于DL的SISR的算法进行一个大体的review，我们主要关注两个方面： SISR设计的高效的神经网络架构 高效求解最优解 BackgroundSingle Image Super-Resolution超分辨率的工作从重建一个或者多个低分图像重建高分图像。从输入数量来分，超分可以分为单图超分 single image superresolution (SISR) 和多图超分multi-image super-resolution (MISR) 。超分图像在很多领域有很宽泛的用处。图一定义了低分图像模型 y = (x ⊗ k)↓s + n, y表示低分图像LR x为高分图像 k为模糊核 ⊗代表卷积操作 ↓s代表下采样操作的规模 n为独立的噪声参数 这样定义或造成不适定 ill-posed 问题，因为一个低分图像可能会对应多个高分图像。至今主流的SISR算法可以分成三个方向： 基于插值的技术 最邻近元法 双线性内插法 三次内插反 双立方插值和Lanczos resampling方法处理起来很快但是会有精确度的缺点 基于重建的方法（概率论/几何论） 凸集投影法 贝叶斯分析法 迭代反投影法 后验概率 正规化法 这类方法通常采用复杂的先验知识可以很好地保留细节信息，但是当规模增加时表现就会急剧下降，而且这类方法通常很耗费时间。 基于学习的方法 Example-based 邻域嵌入 支持向量回归方法 稀疏表示法 基于学习的方法因为其快速杰出的表现获得了关注通常使用ML学习的方法去分析LR和HR之见的数据关系。 Markov random field (MRF) 马尔科夫随机场模型是第一个用丰富的现实图像合成视觉效果良好图像的方法。之后又有许多方法被提出来用于处理SISR问题。近来基于DL的SISR算法获得了比传统基于重建方法和基于学习的方法更好的表现效果 Deep LearningDeep Architecture For SISR在这部分会主要讨论SISR的一些DL网络结构 SRCNN 作者选用SRCNN模型作为benchmark，SRCNN是一个三层的卷进神经网络，每一层的卷积核分别为64×1×9×9，32×64×5×5， 1 × 32 × 5 × 5 ，损失函数采用 the mean square error (MSE) 均方误差。 尽管SRCNN当时很成功但是存在的以下问题激发了以后研究更高效的模型： 输入SRCNN的图像是双立方插值之后的图像，是很接近高分图像的 可能引起更深的图像结构错误计算 使用插值预处理耗费时间 当下采样scale未知时，使用特定的内插输入是不合理的 我们能否直接使用LR图像作为神经网络的输入直接实现端到端的学习。 SRCNN只是一个三层的网络架构，更复杂的网络结构是否能实现更好的效果 反映HR图像性质的损失函数中的先验项是微不足道的 ，能否整合SISR的参数到其他的网络或者其他SISR算法上去 FSRCNNFSRCNN是第一个使用反卷积层从LR的feature map进行HR重建工作的算法。使用反卷机层有两个主要优点： 一是减少了计算量，因为我们只需要在网络的末端增加分辨率 解决了下采样核未知的问题 反卷积层的结构如下 ESPCN普通的反卷积层已经被整合到很多开源的包比如Caffe和TensorFlow，可以用来解决第一个问题。但是仍然存在问题： 当我们使用最近邻插值时，上采样特征中的点在每个方向重复了好几次，这些像素信息其实是冗余的。因此提出了ESPCN模型 不是像反卷积层那样就通过简单增加feather map来提高分辨率，ESPCN增加了输出特征的通道数来存储额外的像素点来提高分辨率然后重新排列这些像素点并通过特殊的映射来获得HR输出。因为像素的扩展是在通道这个维度上所以用较小的卷积核就足够了。更进一步研究表明如果像素周围用0亚像素填充，那么反卷积层可以被简化成ESPCN的亚像素卷积 VDSR在深度学习中，一般来说更深的神经网络有助于提高表现能力，许多工作专注于增加网络深度上。VDSR就是一个使用深度神经网络处理SISR问题的模型 ) VDSR是一个20层的VGG-net模型，VGG-net网络中把所有卷积核设置成3×3。为了训练这个深度模型，作者把一开始的learning rate设置的很高，让模型加速收敛，并且使用了梯度裁剪去避免梯度爆炸的问题。VDSR主要做出了以下两个贡献： 可以将一个模型用于多尺寸 作者利用了和SRCNN一样的预处理方法作为模型的输入，但是VDSR使用了不同scale的预处理图片一起训练，大scale的因子能为小scale的训练提供更有用的信息。 使用了残差学习 作者认为学习双立方到残差之间的映射能提升表现能力并且能加速收敛。 DRCN为了减少VDSR中的参数提出了DRCN模型，使用了和VDSR一样的卷积核和非线性函数——ReLU。为了克服训练深度循环卷积网络的困难，使用了多监督策略。作者使用16次循环卷积网络，每次卷积的到一个结果，最后结果可以看成16个中间结果的加权平均，因此加权的系数和为1。最终DRCN和VDSR有相似的表现性能 DRCN使用的多监督训练是很重要的，这种策略不仅创建了一个在反向传播过程中梯度平滑传播的最短路径而且还使用了中间的HR图像帮助重建最终图像，虽然这种融合确实产生了完美的效果，但是这种方法仍然有两个缺点： 一旦权重被确定了，在训练的过程中对于不同的输入也不会改变 使用单一的权重没有考虑到像素之见的差异，最好以自适应的方式区别地设置权重。 SRResNet像VGG-net那样简单增加网络深度变的愈加困难，各种基于跳跃连接的深度模型可以变得极其深，在许多任务中获得了 state-of-the-art效果。其中ResNet是最具代表的模型。SRResNet就是基于ResNet应用在超分任务上的模型，16个残差单元（每一个单元包含两个带有残差学习的卷积）每一个单元中的批标准化batch normalization (BN) 是为了稳定训练过程 DRRNDRRN也是一个基于残差网络的模型。 EDSREDSR模型带来了三个改进： 移除了原来残差网络中的BN 残差网络中的BN是为了做分类使用的，在例如SISR的图对图的任务中，因为输入和输出是强关联的，在网络收敛不是成问题的情况下，加入BN是会影响性能的。 除了有规律的深度增加外，EDSR还在很大程度上增加了每一层的输出特征数。 使用了残差缩放，克服宽的残差网络训练带来的困难。 作者在训练×3，和×4的网络时，使用预训练的×2的参数。这种预训练策略加速了训练速度，提升了最终表现能力 MDSR这种高效的预训练策略意味着模型在应对不同scale的任务的时候中间的一些参数可能是相同的。 为了深入研究这种方法EDSR的作者提出了MDSR，可以想VDSR那样处理多尺寸图像。 在MDSR中，用于非线性映射的卷积核在不同尺度上是共享的，只有用于提取特征的前卷积核和最终的亚像素上采样卷积是不同的。训练MDSR每次更新时，随机选择×2、×3、×4的minibatch，只更新MDSR的相应部分。 SRDenseNetDenseNet也是一个基于跳跃链接的高效的网络架构。 在DenseNet中，每个层都与前面的所有表示连接， 单元和块使用bottleneck layers，以减少参数的数量 。ResNet可以让参数重复利用，DenseNet可以支持新特性探索。SRDenseNet基于DenseNet在反卷积层之前，连接来自不同块的所有特征，结果表明，这种方法可以有效地提高性能 MemNetMemNet 使用 递归残差单元替换掉DenseNet block中的普通卷积单元，增加了不同block之间的连接。 作者解释说，同一block内的局部连接类似于短期记忆，不同block的连接类似于长期记忆[ RDN在RDN中基本的卷积单元是稠密连接在一起的类似DenseNet，在RDN块的末尾使用一个bottleneck layer，所有的block使用残差学习。在重建部分之前先前所有的特征使用稠密链接和残差学习融合 SCN&amp;CSCN（稀疏编码）稀疏先验（spare prior）在自然图像处理中被广泛用于提高表现能力上，SCN使用 learned iterative shrinkage and thresholding algorithm (LISTA) —— 一种基于神经网络的稀疏编码近似估计方法 ，去解决传统的稀疏编码（ sparse coding ）在SISR上的耗时推理问题 CSCN是SCN的级联版本，它使用了多重的SCN。 SCN创新性的吧稀疏编码和卷积神经网络结合起来，获得了数量和质量的提升。 MSCN（集成学习） 不同的模型针对不同的SISR图像模式。从集成学习的角度来看，可以集成不同的模型可以获得更好的效果。MSCN就是受这种设想启发被提出来，在CNN基础上开发一个额外的模块，LR作为输入，输出多个相同shape的tensor作为HR。对于每一个原始的HR输出，这些tensor可以被视为自适应的 elementwise 权重。 在MSCN中，每个像素的系数总和不是1，这看起来有点违和。 DEGREE结合了subband reconstruction和ResNet的优点。 在每个残差块中学习到的残差可以用来重建高频细节。 为了模拟subband reconstruction，使用了递归残差块 。 与传统的 supervised subband 恢复方法相比，需要通过不同的filters获得 subband ground truth。 受益于端到端表示学习，此模拟使用递归ResNet避免显式估计中间subband组件。 LapSRN 逐步生成不同尺度的SR。 小scale模型可用于大scale SISR的原始估计。大scale的SISR长期以来是一个非常有挑战性的问题。 解决这个问题的一个简单的方法是通过增加对辅助程序的额外监督来逐步提高分辨率 。 LapSRN使用拉普拉斯算子的金字塔（Laplacian pyramid）结构去重建HR输出。LapSRN如图有两个branch： 特征提取branch 图像重建branch 对于每一个scale，图像重建branch估计当前阶段的原始HR输出， 特征提取branch输出原始估计量和相应的 ground truth之间的残差，并为下一阶段提取有用的表示。 PixelSR 利用条件自回归模型逐像素生成SR 当面临大scale factor严重的必要细节丢失的时，一些研究者认为整合合理的细节可以实现更好的结果。PixelRNN和PixelCNN是今年代表性的自回归生成模型。 PixelRNN和PixelCNN中的当前像素依赖于已经生成的左侧和顶部像素 。 PixelSR是第一个应用PixelCNN来处理SISR的模型 最终的HR使用softmax处理两部分结果来生成 P(y_i|x,y_{]]></content>
  </entry>
  <entry>
    <title><![CDATA[SRCNN]]></title>
    <url>%2F2019%2F11%2F17%2F%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%2FSRCNN%2F</url>
    <content type="text"><![CDATA[SRCNN 论文阅读总结 SRCNN在当时创新性的使用深度神经网络进行超分辨率工作，获得了比传统方法更好的效果。本文按照论文顺序结合自身理解进行简单解读（翻译）。 paper link： http://mmlab.ie.cuhk.edu.hk/projects/SRCNN.html Introduction首先作者在论文Introduction部分提出了自己的算法名称——SRCNN，并有以下几个特点： 网络结构简单，并且比当时一些的方法有更好的准确率 算法甚至在CPU上也有很好的效果 如果增大数据集或者扩大模型可以得到更好的超分效果 Related Work非重点 Convolution Neural Networks for Supe-Resolution此部分是核心SRCNN的核心思想部分 大体构想首先使用双立方插值法将原始低分图像扩展到最终期望的图片大小——个人理解就是类似图片直接拉伸。 作者把这个处理后的图片叫做Y，我们的目的就是对这个Y进行处理，记作F(Y)，然后让F(Y)尽可能与真正的高分图像X相似具体操作为三步（请结合原论文Fig.2阅读）： 首先从Y截取一小片区域patch将其描述成（转换成）一个高维度向量，这些高维向量组成了一组feather map 然后将上面的高维度向量转换成另外一个高维度向量组成feather map（自我理解就是卷积） 最后用这些特征重建高分图片 第一层卷积公式 F_1(Y)=max(0,W_1*Y+B_1) W1大小：c×f1×f1×n1 c：channel 输入图像的通道数如果为RGB=3 f1：直接理解成卷积核大小 n1：卷积核数量 B1:偏置 第二层公式 F_2(Y)=max(0,W_2*F_1(Y)+B_2) W2大小：n1×1×1×n2 n1:根据第一层卷积矩阵公式经过w1卷积之后会有n1层feather map，也可以理解成n1层channel 卷积核大小为1 卷积数量为n2 第三层公式 F(Y)=W_3*F_2(Y)+B_3 W3大小：n2×f3×f3×c c:这个c和第一层卷积里面的c一样，因为我们要把图像恢复成原来的通道数 损失函数最终训练好的图片我们记作$F(Y,Θ)$ 其中 $Θ=\{W_1,W_2,W_3,B_1,B_2,B_3\}$ ,对于真正的高分图像X，论文中定义损失函数为 L(\Theta)=1/n\sum_{i=1}^n{\mid\mid F(Y,Θ)-X_i \mid\mid}^2n为训练样本的数量。使用反向传播法进行随机梯度下降 实验 数据集：Set5，Set14 对比算法： SC(sparecoding) K-SVD NE+LLE NE+NNLS ANR 实验细节 参数设置：f1=9,f3=5,n1=64,n2=32,放大倍数为{2,3,4} 训练环境：GTX770 训练时间：3days]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
      <tags>
        <tag>超分辨率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[各种安装坑]]></title>
    <url>%2F2019%2F11%2F16%2F%E5%AE%89%E8%A3%85%E7%BB%8F%E9%AA%8C%2F%E5%90%84%E7%A7%8D%E5%AE%89%E8%A3%85%E5%9D%91%2F</url>
    <content type="text"><![CDATA[记录安装环境的时候遇到的各种坑 GPU版TensorFlow&amp;Pytorch安装经验写在前面： 一定要版本对应！！！ 不要安最新版！！！ 看官方文档！！！ 不要以为我给的网站界面不好看，就以为是三方的，官网门面只能下最新的，我直接给链接省得去翻官网找历史包。 python version：3.7.4（Anaconda） 网址： https://repo.continuum.io/archive/ 请百度“Anaconda和Python对应版本”再从上面链接找需要的包 配置环境变量 *\Anaconda *\Anaconda\Scripts *\Anaconda\Library\bin 请配置conda镜像源（方法请百度——“配置conda镜像源”） https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/freehttps://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch VisualStudio version：2017 CUDA建议先安装VS，可能没有也没关系，以防万一安上，不用选组件，安装基本的就可以 CUDA version：cuda_10.0.130_411.31_win10 网址： https://developer.nvidia.com/cuda-toolkit-archive 提醒： 不要安装10.1(因为tensorflow&gt;=1.13支持到10.0) 请默认全量安装，并且使用默认路径（它想用C盘就C盘，别自己改） CUDNN version：cudnn-10.0-windows10-x64-v7.4.2.24 网址： https://developer.nvidia.com/rdp/cudnn-archive 不要以为网址里什么都没有，请先登录 请看清和CUDA版本对应 安装7.4.1版本往上的，因为tensorflow支持7.4.1往上的，可以大一点版本但不要太多，保险 下载压缩包有几个目录，安装方法直接复制到对应的CUDA安装目录 配置环境变量请参考下面的博客，不要问为什么配这么多，我也不知道 参考 https://blog.csdn.net/CSUWoOd/article/details/92803061 Tensorflow version：gpu版 2.0.0 参考网址 ：https://tensorflow.google.cn/install/pip?lang=python3 pip install tensorflow-gpu 配置环境变量请把 https://tensorflow.google.cn/install/gpu 这个页面拉到最后 看见四行环境变量代码，最后一个是CUDA环境变量上面配过了不用看 前三行环境变量，请自行手动配置到本机环境变量的path下，不要用命令，因为配置的是临时的！！！ Pytorch version：pytorch-1.2.0-py3.7_cuda100_cudnn7_1.tar.bz2 网站： https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/win-64/ 不要去用官网（www.pytorch.org）给的命令，因为官网用命令安装是最新版本 一定要找好自己的python版本和CUDA版本，单独下载本地安装 Ctrl+F搜索一下“pytorch”找一下自己的python版本和conda版本 下载下来，安装。安装方法请百度——“conda安装本地包” torchvision version：torchvision-0.4.0-py37_cu100.tar.bz2 网站：https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/win-64/ 也是python和cuda版本对应，也是单独下载本地安装，方法同上 cudatoolkit version：10.0 直接使用命令安装：conda install cudatoolkit=10.0 附加项 网站：https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/win-64/ 请把网站里的 cuda100-1.0-0.tar.bz2的包也用上面的本地安装方法安装]]></content>
      <categories>
        <category>安装经验</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CV预备知识]]></title>
    <url>%2F2019%2F10%2F07%2F%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%2FCV%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[CV预备知识 CV基本知识1.当前CV领域主要包括两个大的方向，”低层次的感知” 和 “高层次的认知”。 2.主要的应用领域：视频监控、人脸识别、医学图像分析、自动驾驶、 机器人、AR、VR 3.主要的技术：分类、目标检测（识别)、分割、目标追踪、边缘检测、姿势评估、理解CNN、超分辨率重建、序列学习、特征检测与匹配、图像标定，视频标定、问答系统、图片生成（文本生成图像）、视觉关注性和显著性（质量评价）、人脸识别、3D重建、推荐系统、细粒度图像分析、图像压缩 https://zhuanlan.zhihu.com/p/31116104?refer=ahong007 研究方向图像分类CNN（卷积神经网络） 图像检测R-CNN（区域卷积神经网络） 图像分割FCN（全卷积神经网络） 图像描述RNN（迭代神经网络） 图像问答RNN（迭代神经网络） 图像生成GAN（生成对抗网络） 基本算法LeNet5 CNN CV基础网络 AlexNet-&gt;VGG-&gt;GoogLeNet-&gt;ResNet-&gt;ResNeXt GoogLeNet(Inception V1-&gt;V2-&gt;V3-&gt;V4-&gt;Inception ResNet V1-&gt;V2) R-CNN 让基础网络具备区域输出能力 R-CNN-&gt;SPP-Net-&gt;Fast/Faster R-CNN YOLO-&gt;SSD-&gt;R-FCN FCN 让基础网络做像素输出 FCN-&gt;SegNet/DeconvNet-&gt;DeepLab 语义推断，分割更精确 RNN 具有记忆功能，构建不定长序列数据的模型 Vanilla RNN-&gt;LSTM-&gt;GRU 应用：文本序列，区域序列，视频序列 研究：图片描述，问答，机器翻译 GAN 网络结构 生成器网络（Generator） 判别器网络（Discriminator） 网络路线 无监督：GAN-&gt;DCGAN-wGAN 有监督：SRGAN，SalGAN，RLA 应用 样本数据分布（生成）学习 半监督问题的数据增强 有监督问题的优化(Dynamic loss) DNN 开源框架OpenCV Caffe Theano PyTorch TensorFlow 图像基本知识图像特征 颜色特征 RGB，HSV，Lab 直方图 几何特征 Edge，Corner，Blob 纹理特征 HOG，LBP，Gabor 局部特征 SIFT，SURF，FAST 图像存储 RGB空间 CMY(K)印刷四色 Cyan通道 Magenta通道 Yellow通道 Key通道 HSV/HSL(I) 基于人类视觉概念，画家配色 Hue：色调——颜色种类 Saturation：饱和度——颜色浓淡 Value：明度——颜色明亮度 Lightness（Intensity）：亮度，光照亮度 CIE-XYZ颜色空间 给予人类颜色视觉直接测定 XYZ略对应于红绿蓝 CIE-Lab对色空间 接近人类视觉，致力于感知均匀性 L：亮度 a：红/绿 b：黄/蓝 单通道灰度图 Gray=R*0.3+G*0.59+B*0.11 YUV Y明亮度 U与V存储色度(色讯;chrominance;color) YUV4MPEG2(Y4M) YUV4MPEG2 是一种简单的文件格式，它被设计用来保存原始的YCbCr（如 YCbCr 4:2:0, YCbCr 4:2:2, YCbCr 4:4:4等）数据。YUV的来源就来源于色彩空间YCbCr（常用于数字媒体中的彩色编码），YUV常在模拟PAL制传输时，应用在电视和视频录像带中。 数据集NYUDv2 手写数字数据集：MNIST GAN生成人脸：https://thispersondoesnotexist.com/ 收藏文章神经网络中的梯度下降与反向传播的关系 https://segmentfault.com/a/1190000019862084?utm_source=tag-newest 神经网络—反向传播（手算） https://blog.csdn.net/qq_41004007/article/details/83153830 术语解释SOTA(State of the Art)：当前最佳结果 mIoU：平均交并比 cdf(cumulative distribution function)：累积分布函数 DNN（ Deep Neural Networks ）：深度神经网络 CNN（convolutional neural network ）：卷积神经网络 BP（ back propagation ） 误差逆向传播 前向传播(Forward Propagation) GAN（Generative adversarial network）：生成敌对网络 mean normalization ：均值归一化 feature scaling ：特征缩放 normal equation ：正规方程 学习资料文档 TensorFlow2.0手册 https://github.com/dragen1860/Deep-Learning-with-TensorFlow-book TensorFlow2.0实战代码： https://github.com/dragen1860/TensorFlow-2.x-Tutorials PyTorch手册： https://github.com/dragen1860/Deep-Learning-with-PyTorch-Tutorials 慕课 Python： https://study.163.com/course/introduction.htm?courseId=1209466849#/courseDetail?tab=1 TensorFlow： https://study.163.com/course/introduction/1209092816.htm Pytorch： https://study.163.com/course/courseMain.htm?share=2&amp;shareId=480000001847407&amp;courseId=1208894818&amp;_trace_c_p_k2_=61a9e0a511f7409b92a08d4f4c964330 目标检测： https://coding.imooc.com/class/chapter/298.html#Anchor SR(Super-Resolution)算法 SRCNN：第一个超分辨率的神经网络模型 DBNP：https://github.com/alterzero/DBPN-Pytorch；有代码，有模型，eccv workshop冠军，这个网络效果的确无敌，奈何电脑渣渣，跑不了。 RDN：https://github.com/MingtaoGuo/Residual-Dense-Network-Trained-with-cGAN-for-Super-Resolution 变种https://github.com/958099161/Super_resolution FSRCNN： 相对 SRCNN，这个方法不需要对原始图片使用二项式差值，可以直接对小分辨率图像进行处理。在提取特征值之后，缩小图片，然后经过 mapping、expending、反卷积层，然后得到高分辨率图片。它好处是，缩小图片可以降低训练的时间。 ESPCN： 这个模型是基于小图进行训练。最后提取了 r² 个 Channel 。通过将一个像素扩充为一个3x3的矩阵，模拟为一个像素的矩阵，来达到超分辨率的效果。 VDSR： 它认为原始的低分辨率图片与高分辨率图片之间，低频分量几乎一样，缺失的是高频分量，即图片细节。那么训练的时候，只需要针对高频分量进行训练就行了 DRCN： 它还是分为三层。但是在非线性映射这一层，它使用了一个递归网络，也就是说，数据循环多次地通过该层。将这个循环展开的话，等效于使用同一组参数的多个串联的卷积层。 RED：每一个卷积层都对应一个非卷积层。简单来讲，可以理解为是将一张图片进行了编码，然后紧接着进行解码。它的优势在于解决了梯度消失的问题，而且能恢复出更干净的图片。它和 VDSR 有相似的思路。中间卷积层与反卷积层的训练是针对原始图片与目标图片的残差。最后原图会与训练输出结果相加，得到高分辨率的图片。 DRRN： 它采用了更深的网络结构来提升性能。其中有很多个图片增强层。可以理解为，一张模糊的图片，经过多个增强层，一级级变得更加清晰，最终得出高清图片。 LapSRN：特别之处在于引入了一个分级的网络。每一级都只对原图放大两倍，然后加上残差获得一个结果。如果对图片放大8倍的话，这样处理的性能会更高。同时，在每一级处理时，都可以得到一个输出结果。 SRDenseNet：它引入了一个 Desent Block 的结构。上一层网络训练出的特征值会传递到下一层网络，所有特征串联起来。这样做的好处是减轻梯度消失问题、减少参数数量。而且，后面的层可以复用之前训练得出的特征值，不需要重复训练。 SRGAN：利用感知损失(perceptual loss)和对抗损失(adversarial loss)来提升恢复出的图片的质量。在这个模型中有两个网络，一个是生成网络，另一个是判别网路，前者会生成高分辨率图片，后者则会判断这张图是否是原图，如果结果为“否”，那么前者会再次进行训练、生成，直到可以骗过判别网络。]]></content>
      <tags>
        <tag>数据科学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper]]></title>
    <url>%2F2019%2F01%2F17%2Fjava%2FZookeeper%2F</url>
    <content type="text"><![CDATA[ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务。本文从零开始介绍Zookeeper，希望对你有所帮助。 Zookeeper概述Zookeeper从设计模式角度来理解是一个基于观察者模式的分布式服务管理框架，他负责存储和管理大家都关系的数据，然后接受观察者的注册，一旦这些数据的状态发生变化，Zookeeper就将负责通知已经在Zookeeper上注册那些观察者做出相应的反应。 服务器启动时去Zookeeper注册信息 客户端获取当前在线的服务器列表，并注册监听， 如果服务器结点下线，则Zookeeper可以通知客户端， 然后客户端会重新获取服务器列表，并注册监听 Zookeeper=文件系统+通知机制 Zookeeper特点 上图展示了一个Zookeeper集群，其中有一个Leader和四个Follower。集群有以下特点 Zookeeper：一个领导者，多个跟随者组成的集群 集群中只要有半数以上的结点存活，Zookeeper集群就能正常服务 全局一致性：每个Server保存一份相同的数据副本，Client无论连接到哪个Server，数据都是一致的。 更新请求顺序进行，来自同一个Client的更新请求按其发送顺序依次执行 数据更新原子性，一次数据更新要么成功，要么失败。 实时性，在一定时间范围内，Client能读到最新数据。 Zookeeper 数据结构Zookeeper数据模型的结构整体上可以看作是一棵树，每个结点称作一个ZNode。每一个ZNode默认能够存储1MB数据，每个ZNode都可以通过其路径唯一标识。 Zookeeper应用场景 统一命名服务： 在分布式环境下，经常需要对应用/服务进行统一命名，便于识别。 统一配置管理： 在分布式环境下，配置文件同步非常常见。一般要求一个集群中，所有节点的配置信息是一致的，比如Kafka集群。对配置文件后希望能够快速同步到各个节点上。这些操作都可以交给Zookeeper来实现。将配置信息写入Zookeeper上的ZNode，其他各个客户端服务器监听这个ZNode。一旦Znode中的数据被修改，Zookeeper将通知各个客户端服务器。 统一集群管理： 分布式环境中，实时掌握每个节点的状态是必要的。可根据结点实时状态做出一些调整。Zookeeper可以实现实时监控结点状态变化，可将节点信息写入Zookeeper上的一个ZNode。监听这个ZNode可获取他的实时状 态变化。 服务器动态上下线： 客户端能实时洞察到服务器上下线的变化 软负载均衡 在Zookeeper中记录每台服务器的访问数，让访问数最少的服务器去处理最新的客户端请求。 Zookeeper安装与启动直接去官网下载Zookeeper安装包，是一个扩展名为tar.gz的压缩包，虽然Zookeeper框架最终生产实践都是在Linux环境下，但是作为初学可以直接在windows下直接使用，因为这个压缩包不仅提供了在linux下的.sh相关脚本，也提供了windows环境下.bat脚本，我们直接解压即可使用。 修改配置 我们需要进入Zookeeper目录下的conf目录，会看见一个zoo_sample.cfg文件，我们拷贝一份改为zoo.cfg 修改zoo.cfg的dataDir参数，指定文件夹来存储zookeeper数据，这个文件夹需要提前创建好。 上述操作完成后基本Zookeeper就安装好了，然后我们来启动服务 启动 进入Zookeeper的bin目录可以看到一些脚本。 windows用户直接双击zkServer.bat 直接可以启动 linux用户需要使用zkServer.sh start命令运行 然后启动zkCli.bat linux用户直接运行zkCli.sh 不用start参数 停止 windows用户直接关窗口就行了 linux用户退出zkcli使用quit命令 linux退出zkServer使用zkServer stop命令 附录（cfg参数意义） tickTime=2000：2s钟一次心跳（可以理解成时钟） initLimit=10：用10乘以上面的2s得到20s代表Leader和Follower刚开始通信时候的最大延迟时间 syncLimit=5：5*2s代表集群已经正常启动后Leader和Follower之间的通信延时 clientPort：客户端端口号 集群 为每个机器都放置一份Zookeeper 然后在配置的dataDir目录下生成一个myid文件，在里面填入数字，每台机器的myid必须是唯一的 修改cfg文件增加配置格式为 server.A=B:C:D A是一个数字，表示是第几号服务器，也就是刚才的myid B为A服务器的IP地址 C为A服务器与集群中的Leader服务器交换信息的端口 D为如果集群中Leader服务器挂掉，需要一个新的端口重新进行选举，这个端口就是用来执行选举时服务器相互通信的端口 123server.1=server001:2888:3888server.2=server004:2888:3888server.3=server003:2888:3888 每台机器cfg都是上述配置 重启每台服务器 内部命令进入zkCli之后有一些命令可以使用，你可以直接输入help查看可用命令。 ls / ：查看znode中包含的内容 ls2 / ：查看当前节点详细数据 create /node1 “node1”：创建一个节点 create /node1/node1_1 “node1_1” get /node1/node1_1 ：获取结点信息 create -e /node1/node1_2 “node1_2” ：创建短暂结点 create -s /node1/node1_3 “node1_3”：创建序号结点 set /node1/node1_1 “update_node1_1”： 修改结点 get /node1 watch ：注册监听 node1结点的变化，一次命令只能监听一次变化 ls /node1 watch：监听node路径下的变化，一次命令监听一次变化 delete /node1/node1_3 ：删除节点 stat /node1 ：查看节点状态 czxid：创建事务的zxid ctime：被创建的毫秒数 mzxid：最后更新的事务zxid pZxid：最后更新的子节点zxid cversion：子节点修改次数 dataversion：数据变化号 aclVersion：访问控制列表的变化号 ephemeralOwener：如果是临时结点，这个是znode拥有者的session id。如果不是临时结点为0 dataLength：znode数据长度 numChildren：znode子节点数量 rmr /node1：递归删除结点 Zookeeper内部原理选举机制 半数机制：集群中半数以上机器存活，集群可用。所以Zookeeper适合安装奇数台服务器 Zookeeper虽然在配置文件中没有指定Master和Slave，但是它工作时是有一个节点为Leader其他则作为Follower，Leader是通过内部的选举机制临时产生的。简单来说就是按照服务器启动顺序，选择服务器ID号较大的，当半数以上的机器启动起来后，则会可以产生Leader，后面剩下再启动的机器只能作为Follower。 节点类型ZNode结点分为两大类： 持久（Persistent）节点：客户端与服务器断开连接后，创建的结点不删除 短暂（Ephemeral）节点：客户端与服务器断开连接后，创建的结点自己删除 目录结点可以进行编号，创建Znode时可以设置顺序标识，znode名称后会附加一个值，顺序号是一个单调递增的计数器，由父节点维护。 在分布式系统中，顺序号可以被用于为所有的事件进行全局排序，这样的客户端可以通过顺序号推断时间的顺序。 监听器原理监听流程 首先有一个main线程 在main线程中创建Zookeeper客户端，这时会建立两个线程，一个负责网络连接通信（connect），一个负责监听（listener） 通过connect线程将注册的监听事件发送给Zookeeper 在Zookeeper的注册监听器列表中将注册的监听事件添加到列表中。 Zookeeper监听到有数据或路径变化，就会将这个消息发送给listener线程 listener线程内部调用了process()方法 常见的监听 监听节点数据的变化 get path [watch] 监听子节点增减的变化 ls path [watch] 写数据流程 Client向Zookeeper的Server1上写数据，发送给一个写请求 如果Server1不是Leader，那么Server1会把接收到的请求转发给Leader，Leader将写请求广播给各个Server，比如Server1和Server2，各个Server写成功后就会通知Leader 当Leader收到大多数Server写入成功，就说明数据写成功了，Leader会通知Server1数据写成功了 Server1会进一步通知Client数据写成功了，这时就认为整个写操作成功 Zookeeper API下面的原码展示了如何用Zookeeper的API操作Zookeeper，使用前请先启动Zookeeper服务端。 基本API测试123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package ZookeeperDemo;import org.apache.zookeeper.*;import org.apache.zookeeper.data.Stat;import org.junit.Before;import org.junit.Test;import java.io.IOException;import java.util.List;public class ZookeeperTest &#123; private ZooKeeper zkClient; private String connectString="127.0.0.1:2181";//如有多个服务器请用逗号隔开 private int sessionTimeout=2000; private Watcher watcher=new Watcher() &#123; @Override public void process(WatchedEvent watchedEvent) &#123; List&lt;String&gt; children = null;//获取路径下所有子节点并监控 System.out.println("-------------start--------------"); try &#123; children = zkClient.getChildren("/", true); for (String child:children) &#123; System.out.println(child); &#125; System.out.println("-------------end--------------"); &#125; catch (KeeperException e) &#123; e.printStackTrace(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;; @Before//初始化连接 public void init() throws IOException &#123; zkClient = new ZooKeeper(connectString, sessionTimeout, watcher); &#125; @Test//创建结点 public void createNode() throws KeeperException, InterruptedException &#123; String path = zkClient.create("/cokid", "system".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); System.out.println(path); &#125; @Test//获取子节点，并监控数据的变化 public void getDataAndWatch() throws KeeperException, InterruptedException &#123; Thread.sleep(Long.MAX_VALUE);//让主线程休息不要结束，让监听器线程可以监听到变化 &#125; @Test//判断节点是否存在 public void isExist() throws KeeperException, InterruptedException &#123; Stat stat = zkClient.exists("/cokid", false); System.out.println(stat==null?"not exist":"exist"); &#125;&#125; 动态监听服务器上下线 本例创建临时有序结点，请根据实际情况修改。 代码中的服务端和客户端是相对于实际产品，而这些(所谓的客户端服务端)相对于Zookeeper来说都是客户端 我们使用Zookeeper动态监听，使用时请启动Zookeeper服务在节点目录下创建Servers结点。 首先启动DistributeClient 运行DistributeServer，需要注意的是本例采用从main方法args参数获取服务器编号，运行时请务必添加args参数。 可以运行多个DistributeServer添加不同参数 服务器端 1234567891011121314151617181920212223242526272829303132333435363738394041package ZookeeperDemo;import org.apache.zookeeper.*;import java.io.IOException;public class DistributeServer &#123; public static void main(String[] args) throws IOException, KeeperException, InterruptedException &#123; DistributeServer server=new DistributeServer(); //连接Zookeeper server.getConnect(); //服务器去Zookeeper注册 server.regist(args[0]); //业务逻辑处理 server.business(); &#125; private void business() throws InterruptedException &#123; Thread.sleep(Long.MAX_VALUE); &#125; private void regist(String hostname) throws KeeperException, InterruptedException &#123; String path = zkClient.create("/servers/server", hostname.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); System.out.println(hostname+" is online"); &#125; private ZooKeeper zkClient; private String connectString="127.0.0.1:2181"; private int sessionTimeout=2000; private Watcher watcher=new Watcher() &#123; @Override public void process(WatchedEvent watchedEvent) &#123; &#125; &#125;; private void getConnect() throws IOException &#123; zkClient = new ZooKeeper(connectString, sessionTimeout, watcher); &#125;&#125; 客户端 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package ZookeeperDemo;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooKeeper;import java.io.IOException;import java.util.ArrayList;import java.util.List;public class DistributeClient &#123; public static void main(String[] args) throws IOException, KeeperException, InterruptedException &#123; DistributeClient client = new DistributeClient(); //获取Zookeeper连接 client.getConnect(); //注册监听 client.getChildren(); //业务逻辑处理 client.business(); &#125; private void business() throws InterruptedException &#123; Thread.sleep(Long.MAX_VALUE); &#125; private void getChildren() throws KeeperException, InterruptedException &#123; List&lt;String&gt; children = zkClient.getChildren("/servers", true); //存储服务器节点名称 ArrayList&lt;String&gt; hosts = new ArrayList&lt;&gt;(); for(String child:children)&#123; byte[] data = zkClient.getData("/servers/" + child, false, null); hosts.add(new String(data)); &#125; //将所有在线主机打印到控制台 System.out.println(hosts); &#125; private ZooKeeper zkClient; private String connectString="127.0.0.1:2181"; private int sessionTimeout=2000; private Watcher watcher=new Watcher() &#123; @Override public void process(WatchedEvent watchedEvent) &#123; try &#123; getChildren(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (KeeperException e) &#123; e.printStackTrace(); &#125; &#125; &#125;; private void getConnect() throws IOException &#123; zkClient = new ZooKeeper(connectString, sessionTimeout, watcher); &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka]]></title>
    <url>%2F2019%2F01%2F17%2Fjava%2FKafka%2F</url>
    <content type="text"><![CDATA[现在招聘Java后端越来越严格了，不仅要会基本的Java那一套，还要你会DBA的活，运维的活，测试的活，还有大数据的活你也得懂点。kafka作为Apache下面的高吞吐量的分布式发布订阅消息系统，在大数据方面有很广的应用，在普通的Java开发中也是现在比较重要的中间件。 消息队列加入客户端A向客户端B传输消息，最理想的情况的就是A发送B立即接收，但是这种情况很脆弱，如果B不在线，或者A发送速度和接收速度不统一，那么发送的消息有可能丢失。 我们在A和B之间加上一个消息队列系统。消息队列的接收数据有两种模式： 一种是点对点模式：消费者B主动拉去数据，消息收到后清除队列消息。点对点模式可以自由控制什么时候接收数据，以什么样的速度接收数据，但是本身不知道队列中什么时候有数据，需要一个线程不断监控队列 第二种是发布订阅模式：类似于微信公众号，消息队列直接主动推送给C、D、E客户端。但是推送速度可能和客户端接收速度不匹配。 消息队列的优点： 解耦：A和B不再直接通信 冗余：消息队列本身可以缓冲数据 灵活扩展：可以设置消息队列集群 可恢复性：消息队列可以保存数据 顺序保证： Kafka概述Kafka是一个分布式消息队列系统，由scala语言编写，Kafka对消息保存时根据Topic进行归类，发送消息者成为Producer，消息的接受者成为Consumer，Kafka集群有多个kafka实例组成，每个实例（server）称为broker。无论是kafka集群还是consumer都依赖于zookeeper集群保存一些meta信息保证系统可用性。 Topic 可以理解成一个简单队列 Consumer Group（CG）：这是kafka用来实现topic消息的广播和单播的手段，一个Topic可以有多个Consumer group。topic的消息会复制到所有的CG上（概念上复制，不是真复制），但每个partition只会把消息发给该CG中的一个Consumer。 Broker：一台kafka服务器就是一个broker，一个集群由多个Broker组成，一个Broker可以容纳多个topic。 Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker上，一个topic可以分为多个partition。每个partition是一个有序的队列。partition中每条消息都有一个有序的id。kafka只保证按一个partition中的顺序将消息发个consumer，不保证一个topic整体的顺序。 kafka 安装与启动kafka实际应用更多的是在linux系统上，不过windows用户也不用担心，kafka内部也提供了windows相关操作，windows用户与linux用户用的是同一个安装包。在安装kafka之前请先安装好Zookeeper，本博客Zookeeper文章有介绍。 安装 直接去官网下载kafka安装包，是一个扩展名.tgz的压缩包文件。直接解压即可 在kafka安装目录下新建logs文件夹 进入kafka的config目录编辑server.properties文件 修改broker.id参数，如果是单机本地使用则不用修改，如果是集群使用请确保每一台机器的broker.id不同 修改log.dirs参数为刚才建立logs文件夹的目录，windows用户建议使用 / 这个斜杠。 修改Zookeeper.connect参数，如果是本地使用则不用修改，如果是集群使用，请修改为集群的服务器地址和端口，多台服务器之间用逗号隔开 使用进入kafka下的bin目录windows脚本和linux脚本主要是扩展名上的区别，相同命令都可以执行，下文不再具体区分，相关用户选择对应脚本。请先启动Zookeeper服务端程序。 windows 用户使用kafka-server-start.bat ../../config/server.properties linux用户使用 kafka-server-start.sh ../config/server.properties 因为目录深度不同所以有所区别 kafka-topics.bat —create —zookeeper 127.0.0.1:2181 —partitions 2 —replication-factor 1 —topic first 使用命令创建一个topic，分两个区，一个副本（如果是集群可以设多个副本） kafka-topics.bat —list —zookeeper 127.0.0.1:2181 查看创建的topic 现在进入kafka的logs(存放数据)目录可以看到里面有我们创建的结点 常用命令 kafka-topics.bat —delete —zookeeper 127.0.0.1:2181 —topic first 删除topic kafka-console-producer.bat —broker-list 127.0.0.1:9092 —topic first 生产者 kafka-console-consumer.bat —bootstrap-server 127.0.0.1:9092 —from-beginning —topic first 消费者把first主题中所有数据都读出来（低版本中—bootstrap-server 127.0.0.1:9092修改为—Zookeeper 127.0.0.1:2181因为数据存在Zookeeper中，高版本中数据存在kafka本地中） kafka-topics.bat —list —zookeeper 127.0.0.1:2181 —describe —topic first查看topic详情 Kafka生产写入方式producer采用推模式将消息发布到Broker，每条消息都被追加到分区中，属于顺序写磁盘（顺序写磁盘比随机写内存效率高） 分区​ Kafka集群有多个消息代理服务器（broker-server）组成，发布到Kafka集群的每条消息都有一个类别，用主题（topic）来表示。通常，不同应用产生不同类型的数据，可以设置不同的主题。一个主题一般会有多个消息的订阅者，当生产者发布消息到某个主题时，订阅了这个主题的消费者都可以接收到生成者写入的新消息。 Kafka集群为每个主题维护了分布式的分区（partition）日志文件，物理意义上可以把主题（topic）看作进行了分区的日志文件（partition log）。主题的每个分区都是一个有序的、不可变的记录序列，新的消息会不断追加到日志中。分区中的每条消息都会按照时间顺序分配到一个单调递增的顺序编号，叫做偏移量（offset），这个偏移量能够唯一地定位当前分区中的每一条消息。 每个Partition中的消息都是有序的，生产的消息被不断追加到Partition log上，其中的每一个消息都被赋予了一个唯一的offset值。 发布到Kafka主题的每条消息包括键值和时间戳。消息到达服务器端的指定分区后，都会分配到一个自增的偏移量。原始的消息内容和分配的偏移量以及其他一些元数据信息最后都会存储到分区日志文件中。消息的键也可以不用设置，这种情况下消息会均衡地分布到不同的分区。 分区的原因： 方便在集群中扩展，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群可以适应任意大小的数据。 可以提高并发，以partition为单位写 传统消息系统在服务端保持消息的顺序，如果有多个消费者消费同一个消息队列，服务端会以消费存储的顺序依次发送给消费者。但由于消息是异步发送给消费者的，消息到达消费者的顺序可能是无序的，这就意味着在并行消费时，传统消息系统无法很好地保证消息被顺序处理。虽然我们可以设置一个专用的消费者只消费一个队列，以此来解决消息顺序的问题，但是这就使得消费处理无法真正执行。 Kafka比传统消息系统有更强的顺序性保证，它使用主题的分区作为消息处理的并行单元。Kafka以分区作为最小的粒度，将每个分区分配给消费者组中不同的而且是唯一的消费者，并确保一个分区只属于一个消费者，即这个消费者就是这个分区的唯一读取线程。那么，只要分区的消息是有序的，消费者处理的消息顺序就有保证。每个主题有多个分区，不同的消费者处理不同的分区，所以Kafka不仅保证了消息的有序性，也做到了消费者的负载均衡。 副本同一个partition可能会有多个replication（对应 server.properties 配置中的default.replication.factor=N）。没有replication的情况下，一旦broker宕机，其上所有 patition 的数据都不可被消费，同时producer也不能再将数据存于其上的patition。引入replication之后，同一个partition可能会有多个replication，而这时需要在这些replication之间选出一个leader，producer和consumer只与这个leader交互，其它replication作为follower从leader 中复制数据。 写入流程 producer先从zookeeper的 “/brokers/…/state”节点找到该partition的leader producer将消息发送给该leader leader将消息写入本地log followers从leader pull消息，写入本地log后向leader发送ACK leader收到所有ISR中的replication的ACK后，增加HW（high watermark，最后commit 的offset）并向producer发送ACK Kafka消息保存 上图展示了kafka在Zookeeper保存数据的目录结构。 物理上把topic分成一个或多个patition（对应 server.properties 中的num.partitions配置），每个patition物理上对应一个文件夹（该文件夹存储该patition的所有消息和索引文件）。 无论消息是否被消费，kafka都会保留所有消息。有两种策略可以删除旧数据： 1）基于时间：log.retention.hours=168 2）基于大小：log.retention.bytes=1073741824 需要注意的是，因为Kafka读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除过期文件与提高 Kafka 性能无关。 Kafka消费消息由生产者发布到Kafka集群后，会被消费者消费。消息的消费模型有两种：推送模型（push）和拉取模型（pull）。 基于推送模型（push）的消息系统，由消息代理记录消费者的消费状态。消息代理在将消息推送到消费者后，标记这条消息为已消费，但这种方式无法很好地保证消息被处理。比如，消息代理把消息发送出去后，当消费进程挂掉或者由于网络原因没有收到这条消息时，就有可能造成消息丢失（因为消息代理已经把这条消息标记为已消费了，但实际上这条消息并没有被实际处理）。如果要保证消息被处理，消息代理发送完消息后，要设置状态为“已发送”，只有收到消费者的确认请求后才更新为“已消费”，这就需要消息代理中记录所有的消费状态，这种做法显然是不可取的。 Kafka采用拉取模型，由消费者自己记录消费状态，每个消费者互相独立地顺序读取每个分区的消息。如下图所示，有两个消费者（不同消费者组）拉取同一个主题的消息，消费者A的消费进度是3，消费者B的消费进度是6。消费者拉取的最大上限通过最高水位（watermark）控制，生产者最新写入的消息如果还没有达到备份数量，对消费者是不可见的。这种由消费者控制偏移量的优点是：消费者可以按照任意的顺序消费消息。比如，消费者可以重置到旧的偏移量，重新处理之前已经消费过的消息；或者直接跳到最近的位置，从当前的时刻开始消费。 ​ 在一些消息系统中，消息代理会在消息被消费之后立即删除消息。如果有不同类型的消费者订阅同一个主题，消息代理可能需要冗余地存储同一消息；或者等所有消费者都消费完才删除，这就需要消息代理跟踪每个消费者的消费状态，这种设计很大程度上限制了消息系统的整体吞吐量和处理延迟。Kafka的做法是生产者发布的所有消息会一致保存在Kafka集群中，不管消息有没有被消费。用户可以通过设置保留时间来清理过期的数据，比如，设置保留策略为两天。那么，在消息发布之后，它可以被不同的消费者消费，在两天之后，过期的消息就会自动清理掉。 push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息。 对于Kafka而言，pull模式更合适，它可简化broker的设计，consumer可自主控制消费消息的速率，同时consumer可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。 pull模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直等待数据到达。为了避免这种情况，我们在我们的拉请求中有参数，允许消费者请求在等待数据到达的“长轮询”中进行阻塞（并且可选地等待到给定的字节数，以确保大的传输大小）。 Kafka API高级API 优点 高级API 写起来简单 不需要自行去管理offset，系统通过zookeeper自行管理。 不需要管理分区，副本等情况，.系统自动管理。 消费者断线会自动根据上一次记录在zookeeper中的offset去接着获取数据（默认设置1分钟更新一下zookeeper中存的offset） 可以使用group来区分对同一个topic 的不同程序访问分离开来（不同的group记录不同的offset，这样不同程序读取同一个topic才不会因为offset互相影响） 缺点 不能自行控制offset（对于某些特殊需求来说） 不能细化控制如分区、副本、zk等 低级API 优点 能够让开发者自己控制offset，想从哪里读取就从哪里读取。 自行控制连接分区，对分区自定义进行负载均衡 对zookeeper的依赖性降低（如：offset不一定非要靠zk存储，自行存储offset即可，比如存在文件或者内存中） 缺点 太过复杂，需要自行控制offset，连接哪个分区，找到分区leader 等。 Kafka 拦截器Producer拦截器(interceptor)是在Kafka 0.10版本被引入的，主要用于实现clients端的定制化控制逻辑。 对于producer而言，interceptor使得用户在消息发送前以及producer回调逻辑前有机会对消息做一些定制化需求，比如修改消息等。同时，producer允许用户指定多个interceptor按序作用于同一条消息从而形成一个拦截链(interceptor chain)。Intercetpor的实现接口是org.apache.kafka.clients.producer.ProducerInterceptor，其定义的方法包括： （1）configure(configs) 获取配置信息和初始化数据时调用。 （2）onSend(ProducerRecord)： 该方法封装进KafkaProducer.send方法中，即它运行在用户主线程中。Producer确保在消息被序列化以及计算分区前调用该方法。用户可以在该方法中对消息做任何操作，但最好保证不要修改消息所属的topic和分区，否则会影响目标分区的计算 （3）onAcknowledgement(RecordMetadata, Exception)： 该方法会在消息被应答或消息发送失败时调用，并且通常都是在producer回调逻辑触发之前。onAcknowledgement运行在producer的IO线程中，因此不要在该方法中放入很重的逻辑，否则会拖慢producer的消息发送效率 （4）close： 关闭interceptor，主要用于执行一些资源清理工作 如前所述，interceptor可能被运行在多个线程中，因此在具体实现时用户需要自行确保线程安全。另外倘若指定了多个interceptor，则producer将按照指定顺序调用它们，并仅仅是捕获每个interceptor可能抛出的异常记录到错误日志中而非在向上传递。这在使用过程中要特别留意。 Kafka StreamKafka Streams。Apache Kafka开源项目的一个组成部分。是一个功能强大，易于使用的库。用于在Kafka上构建高可分布式、拓展性，容错的应用程序。 Kafka Streams特点 功能强大 高扩展性，弹性，容错 轻量级 无需专门的集群 一个库，而不是框架 完全集成 100%的Kafka 0.10.0版本兼容 易于集成到现有的应用程序 实时性 毫秒级延迟 并非微批处理 窗口允许乱序数据 允许迟到数据 为什么要有Kafka Stream当前已经有非常多的流式处理系统，最知名且应用最多的开源流式处理系统有Spark Streaming和Apache Storm。Apache Storm发展多年，应用广泛，提供记录级别的处理能力，当前也支持SQL on Stream。而Spark Streaming基于Apache Spark，可以非常方便与图计算，SQL处理等集成，功能强大，对于熟悉其它Spark应用开发的用户而言使用门槛低。另外，目前主流的Hadoop发行版，如Cloudera和Hortonworks，都集成了Apache Storm和Apache Spark，使得部署更容易。 既然Apache Spark与Apache Storm拥用如此多的优势，那为何还需要Kafka Stream呢？主要有如下原因。 第一，Spark和Storm都是流式处理框架，而Kafka Stream提供的是一个基于Kafka的流式处理类库。框架要求开发者按照特定的方式去开发逻辑部分，供框架调用。开发者很难了解框架的具体运行方式，从而使得调试成本高，并且使用受限。而Kafka Stream作为流式处理类库，直接提供具体的类给开发者调用，整个应用的运行方式主要由开发者控制，方便使用和调试。 第二，虽然Cloudera与Hortonworks方便了Storm和Spark的部署，但是这些框架的部署仍然相对复杂。而Kafka Stream作为类库，可以非常方便的嵌入应用程序中，它对应用的打包和部署基本没有任何要求。 第三，就流式处理系统而言，基本都支持Kafka作为数据源。例如Storm具有专门的kafka-spout，而Spark也提供专门的spark-streaming-kafka模块。事实上，Kafka基本上是主流的流式处理系统的标准数据源。换言之，大部分流式系统中都已部署了Kafka，此时使用Kafka Stream的成本非常低。 第四，使用Storm或Spark Streaming时，需要为框架本身的进程预留资源，如Storm的supervisor和Spark on YARN的node manager。即使对于应用实例而言，框架本身也会占用部分资源，如Spark Streaming需要为shuffle和storage预留内存。但是Kafka作为类库不占用系统资源。 第五，由于Kafka本身提供数据持久化，因此Kafka Stream提供滚动部署和滚动升级以及重新计算的能力。 第六，由于Kafka Consumer Rebalance机制，Kafka Stream可以在线动态调整并行度。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL]]></title>
    <url>%2F2019%2F01%2F12%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2FMySQL%2F</url>
    <content type="text"><![CDATA[如果是仅仅简单使用MySQL那是很简单的一件事，而且现在有众多图形画界面(MySQL Front、Navicat、DataGrip等)更方便操作MySQL。但是MySQL的内容远不止于此。建议读者尝试用MySQL自带的Client来操作MySQL，也建议把MySQL安装目录的bin目录添加到环境变量，方便在控制台中使用。 MySQL引擎MySQL的存储引擎是插件式的，我们既可以使用MySQL自带的引擎也可以开发属于自己的引擎，简单介绍一下一些内置的引擎。MySQL引擎是基于表的，一个引擎对应一张表。如图我们可以通过图形化界面也可以使用show engines命令显示我们当前的可以使用的引擎。下文中的特性一般基于的是InnoDB引擎 InnoDB引擎从5.5.8版本开始InnoDB是MySQL的默认引擎，也是我们重点要介绍的。InnoDB支持事务，行锁，外键，四种隔离级别(下文中有具体介绍)以及插入缓存，二次写，自适应哈希索引，预读等高性能和高可用功能。每张表的存储是按主键顺序存放，如果没有主键，会自动生成一个6字节的ROWID，作为主键。 MyISAM引擎不支持事务，使用表锁，支持全文索引，缓冲池只缓存索引文件不缓冲数据文件，MyISAM存储引擎表由MYD(存放数据文件)和MYI(存放索引文件)组成，最大支持256TB单表，4GB索引缓冲区 Memory顾名思义，表的数据是存放在内存中的，默认使用哈希索引，一旦数据库重启崩溃数据丢失，适用于临时存储例如查询的中间结果集，因为表在内存中所以速度会很快。但一旦结果集大于其存储容量MySQL会转换成MyISAM引擎表存放到磁盘，又因为MyISAM不缓存数据文件，没有缓存则查询起来会有性能损失。 Archive只支持Insert和Select操作，支持索引，使用zlib算法对数据行压缩存储。适用于存储归档数据，例如日志信息。使用行锁实现高并发插入操作但本身不是事务安全的。 Federated不存储表，指向远程MySQL数据库服务器上的表。只支持MySQL，不支持异构数据库表。 MySQL文件参数文件查看当MySQL实例启动时，会预先读取一个参数文件，用来初始化某些参数，文件可以在MySQL数据文件夹下的找到名字为my.ini，里面有一些参数我们可以手动修改。也可以查看information_schema库下面的GLOBAL_VARIABLES进行查看相关参数,比如查看InnoDB缓存相关的参数 123select * fromGLOBAL_VARIABLESwhere VARIABLE_NAME like &apos;innodb_buffer%&apos;\g 上述语句有点长可以直接执行 1show variables like 'innodb_buffer%'\g 可能有读者执行时会遇到The ‘INFORMATION_SCHEMA.GLOBAL_VARIABLES’ feature is disabled; see the documentation for ‘show_compatibility_56’错误。那么请执行SET GLOBAL show_compatibility_56 = ON;完成后再执行上面的sql语句即可查看。 当然最简单的方法是直接查看my.ini 文件 {% asset_img 3.jpg %} 修改修改命令如下： 123SET global|session 参数名=值SET read_buffer_size=524288 MySQL参数分为两类：动态参数，静态参数 动态参数可以通过上述命令修改，静态参数在实例生命周期内不能修改。 日志文件日志文件分为一般日志，错误日志，慢查询日志，二进制日志 {% asset_img 4.jpg %} 日志文件可以在数据文件夹下的Data文件夹下找到，博主只开启了一份慢查询日志，和错误日志，没有开启一般日志，以及二进制日志。 错误日志帮助我们找到一些错误信息 慢查询日志帮助我们优化数据库语句 重点介绍一下 二进制日志二进制日志记录了对数据库的所有操作，但是不包括select和show等操作，其他操作即使没有修改数据库例如update也可能写入日志里。二进制日志可以用来，恢复，复制数据库，也可以查看是否有对数据库的攻击。 关于二进制日志的参数 max_binlog_size 单个二进制文件最大值 binlog_cache_size 未提交事务二进制日志会写入缓存 sync_binlog 每写缓冲多少次同步到磁盘 binlog-do-db 指定写入日志的数据库 binlog-ignore-db 忽略写入日志的数据库 log-slave-update 如果此台机器是slave机器将会取得Master的二进制日志更新自己日志 binlog_format 二进制格式 对二进制文件的操作： 查看是否开启 show variables like &quot;%log_bin%&quot; 与bin-log有关的操作 mysql&gt; flush logs； 日志刷新，此时就会多出一个最新的bin-log日志 mysql&gt; show master status ; 查看最后一个bin日志 mysql&gt; reset master; 清空所有bin-log日志 mysql&gt; mysqlbinlog --no-defaults mysql-00001.bin 查看bin-log日志的内容 mysql备份和bin-log日志 备份数据： mysqldump -uroot -pPASSWORD test -l -F &gt;&#39;/tmp/test.sql&#39; -F即flush logs，可以重新生成新的日志文件，包括log-bin日志；-l 为读锁，读锁时所有数据不允许写 恢复数据： mysql -uroot -pPASSWORD test -v -f &lt;/tmp/test.sql -v查看导入的详细信息；-f当遇到错误时skip，继续执行下面的语句。 mysql&gt; mysqlbinlog --no-defaults mysql-00001.bin | mysql -uUSERNAME -pPASSWORD databasename 用bin-log日志恢复数据库 套接字文件仅限于Unix系统，Unix连接MySQL可以采用Unix域套接字方式，这种方式需要一个套接字文件。 PID文件MySQL启动时会将自己的进程ID写入一个文件,在数据文件夹下的Data文件夹下的扩展名为.pid的文件，可由pid_file参数控制 frm文件MySQL是插件式的存储引擎的体系结构关系，MySQL数据的存储是根据表进行的，每个表都有对应的文件，因此用.frm来记录表的结构定义 表空间文件（InnoDB）InnoDB采用将存储的数据按照表空间进行存放的设计，默认配置下初始大小10M，有个名为ibdata1的文件，就是默认表空间文件。 重做日志文件（InnoDB）ib_logfile0和ib_logfile1文件记录了对于InnoDB存储引擎的事务日志。如果数据库由于主机断电导致实例失败，InnoDB会使用重做日志文件恢复到掉电前的时刻。至少有一个重做日志文件组，每个组至少2个文件，每个文件大小一致，参用循环方式写入。 MySQL索引定义：索引（Index）是帮助MySQL高效获取数据的数据结构。 可以理解为“排好序的快速查找的数据结构”。在数据之外，数据库系统还维护着满足特定查找算法的数据结构，这些数据结构以某种方式引用(指向)数据，这样就可以在这些数据 结构上实现高级查找算法。这种数据结构就是索引。 一般来说索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储在磁盘上 我们平时说的索引，如果没有特别指明，都是指B树(多路搜索树，并不一定+是二叉树)结构组织的索引，其中，聚集索引，次要索引，复合索引，前缀索引，唯一索引默认都是使用B+树索引，统称索引。当然，除了B+树这种类型的索引之外，还有哈希索引等。 MySQL索引结构（InnoDB） B+树索引 哈希索引 全文索引 InnoDB存储引擎会根据表的使用情况自动生成哈希索引，不能人为干预是否在一张表中生成哈希索引。传统说索引一般指B+树索引（B树详情参见本博客相关文章）。我们需要明确B+树索引并不是给定指定的键值来找到具体的行，B+树只能找到查找数据行所在的页。B+树索引在数据库有一个特点是高扇出性，因此在数据库中，B+树的高度一般在2~4层，因此查找某一键值的行记录时最多需要2~4次IO。B+树索引可以分为聚集索引和辅助索引，其内部都是B+树结构，不同点在于叶子节点存放的是否是一整行的信息。 聚集索引聚集索引按照每张表的主键构造一棵B+树，叶子节点中存放的为整张表的行记录数据，也将聚集索引的叶子结点称为数据页。每个数据页通过一个双向链表来进行链接。由于实际的数据页只能按照一棵B+树进行排序，因此每张表只能拥有一个聚集索引。多数情况下，查询优化器倾向于采用聚集索引。因为聚集索引能够在B+树索引的叶子节点上直接找到数据。由于定义了数据的逻辑顺序，因此能特别快速地访问针对范围值的查询。 辅助索引叶子节点不包含行记录的全部数据。叶子节点除了包含键值之外还包含一个书签，指向相应数据行的聚集索引。每张表上可以有多个辅助索引，通过辅助索引查找数据，会遍历辅助索引并通过叶级别的指针获得指向主键索引的主键，然后通过主键索引来找到一个完整的行记录。 优点： 提高数据检索的效率，降低数据库的IO成本。 通过索引列对数据进行排序，降低数据排序的成本，降低了CPU的消耗。 缺点： 实际上索引也是一张表，保存了主键与索引字段，并指向实体表的记录，所以索引也是要占空间的 虽然索引大大提高了查询速度却会降低更新表的速度(UPDATE DELETE INSERT)，因为更新表时，MySQL不仅要保存数据，还要保存一下索引文件每次更新添加了索引列的字段，都会调整因为更新所带来的剑指变化后的索引信息。 索引知识提高效率的一个因素，如果MySQL有大量的表，就需要花时间研究建立最优秀的索引，或优化查询语句 MySQL索引分类 单值索引：一个索引只包含单个列，一个表可以有多个单列索引 唯一索引：一个索引的值必须唯一，但允许有控制 复合索引：一个索引包含多个列 MySQL索引语法 创建：CREATE [UNIQUE] INDEX indexName ON mytable(columnname(length)) ALTER mytable ADD [UNIQUE] INDEX [indexName] ON (columnname(length)) 删除：DROP INDEX [indexName] ON mytable 查看：SHOW INDEX FROM table_name\G 产看结果会显示一些列的信息 Table 表名 Non_unique 非唯一的索引 Key_name 索引的名字 Seq_in_index 索引中该列的位置 Column_name 索引列的名称 Collaction 列以什么方式存储在索引中 Cardinality 索引中唯一值的数目的估计值 Sub_part 是否是列的部分被索引，如果索引整个列该字段为NULL Packed 关键字如何被压缩 Null 索引的列中是否含有Null值 Index_type 索引的类型，显示BTREE实际是B+树索引 Comment 注释 有四种方式来添加数据表的索引： ALTER TABLE tbl_name ADD PRIMARY KEY (column_list): 该语句添加一个主键，这意味着索引值必须是唯一的，且不能为NULL。 ALTER TABLE tbl_name ADD UNIQUE index_name (column_list): 这条语句创建索引的值必须是唯一的（除了NULL外，NULL可能会出现多次）。 ALTER TABLE tbl_name ADD INDEX index_name (column_list): 添加普通索引，索引值可出现多次。 ALTER TABLE tbl_name ADD FULLTEXT index_name (column_list): 该语句指定了索引为 FULLTEXT ，用于全文索引。 什么时候建立索引 主键自动建立唯一索引 频繁作为查询条件的字段应该创建索引 查询中与其他表关联的字段，外键关系建立索引 高并发下倾向创建组合索引 查询中排序的字段 查询中统计或者分组字段 上文中查询索引的字段中有一个重要的值Cardinality，它显示的是索引中不重复记录数量的预估值。用户可以参考这个值决定是否去建立索引。如果某个字段选取范围很广，几乎没有重复值例如姓名，则比较适合建立索引。Cardinality统计信息更新发生在UPDATE和DELETE阶段但是不能每次操作都统计一次Cardinality，所以内置策略是当表中1/16数据发生变化或者变化次数（stat_modified_counter）超过20亿次时。采样计数采用随机抽取B+树的8个叶子节点，统计数量然后把看看整B+树所有叶子节点可以分为几个8，然后乘以刚才采样的统计数量。 什么时候不要索引 表记录太少 经常增删改的表 数据重复且平均的表字段 MySQL事务事务的概念mysql 的默认引擎innodb支持事务 TCL(Transanction Control Language) 事务控制语言 事务：一个或一组sql语句组成一个执行单元，但这个执行单元要么全部执行，要么全部不执行,如果某条执行失败或者出现错误，将进行回滚。 在关系型数据库中，事务有4个特性ACID：A原子性，C一致性，I独立性，D持久性（详见文Redis） Mysql使用事务事物的创建 隐式事务：事物没有明显的开启和结束的标记；比如insert、 update、delete语句 显式事务：有明显的开启和结束的标记；前提：必须先设置自动提交的功能为禁用 123456show variables like &apos;autocommit&apos; #查看自动提交的状态set autocommit=0; #关闭自动提交，只对当前会话有效。每次打开都需要设置一次start transaction; #开启事务，可选，因为只要写了上面的语句就认为开启了事务。 #编写事务中的sql语句（select insert update delete）commit;#提交事务rollback;#事务回滚 事务的并发对于同时运行的多个事务, 当这些事务访问数据库中相同的数据时, 如果没有采取必要的隔离机制, 就会导致各种并发问题: 脏读: 对于两个事务 T1, T2, T1 读取了已经被 T2 更新但还没有被提交的字段.之后, 若 T2 回滚, T1读取的内容就是临时且无效的.（没提交回滚） 不可重复读: 对于两个事务T1, T2, T1 读取了一个字段, 然后 T2 更新了该字段.之后, T1再次读取同一个字段, 值就不同了.（更新） 幻读: 对于两个事务T1, T2, T1 从一个表中读取了一个字段, 然后 T2 在该表中插入了一些新的行. 之后, 如果 T1 再次读取同一个表, 就会多出几行.（插入） 数据库事务的隔离性: 数据库系统必须具有隔离并发运行各个事务的能力,使它们不会相互影响, 避免各种并发问题. 一个事务与其他事务隔离的程度称为隔离级别. 数据库规定了多种事务隔离级别, 不同隔离级别对应不同的干扰程度, 隔离级别越高, 数据一致性就越好, 但并发性越弱. mysql支持四种事务的隔离级别： 隔离级别 描述 脏读 不可重复读 幻读 READ UNCOMMITTED (读未提交数据) 允许事务读取未被其他事务提交的变更、脏读、不可重复读和幻读的问都户出现 有 有 有 READ COMMITTED(读已提交的数据) 只允许事务读取已经被其他事务提交的变更、可避免的脏读、但不可重复读和幻读的问题仍然可能出现 无 有 有 REPEATABLE READ(可重复读) 确保事务可以多次从一个字段中读取相同的值，在这个事务持续期间，禁止其他事务对这个字段更新，可以避免脏读和不可重复读，但幻读的问题仍然存在 无 无 有 SERIALIZABLE(串行化) 确保事务可以从一个表中读取相同的行，在这个事务持续期间，禁止其他事务对该表执行插入、更新和删除操作，所有并发问题都可以避免，但性能十分低下。 无 无 无 Mysql默认的事务隔离级别为:REPEATABLE READ Mysql设置隔离级别 每启动一个 mysql 程序, 就会获得一个单独的数据库连接. 每个数据库连接都有一个全局变量 @@tx_isolation, 表示当前的事务隔离级别. 查看当前的隔离级别: SELECT @@tx_isolation; 设置当前 mySQL 连接的隔离级别: set transaction isolation level read committed; 设置数据库系统的全局的隔离级别: set global transaction isolation level read committed; MySQL锁机制在数据库中，除传统的计算机资源(如CPU、RAM、I/O等)的争用以外，数据也是一中供许多用户分享的资源。如何保证数据并发访问的一致性、有效性是所有数据库必须解决的一个问题，锁冲突也是影像数据库并发访问性能的一个重要因素。从这个角度来说，锁对数据库而言显得更加重要，也更加复杂。 Lock和LatchLock和Latch都可称为锁但是本身含义截然不同 Lock Latch 对象 事务 线程 保护 数据库内容 内存数据结构 持续时间 整个事务过程 临界资源 模式 行锁，表锁，意向锁 对写锁，互斥量 死锁 通过waits-for graph、Time out等机制进行死锁检测与处理 无死锁检测与处理机制。仅通过应用程序加锁的顺序保证无死锁的情况发生。 存在于 Lock Manager 的哈希表中 每个数据结构的对象中 使用show engine innodb mutex;命令可以查看latch信息 使用show engine innodb status;查看lock信息，也可以在information_schema下面的INNODB_TRX，INNODE_LOCKS，INNODB_LOCK_WAITS来观察锁的信息。 锁的分类 从对数据操作的类型(读/写)分： 读锁(共享锁)：针对同一份数据，多个读操作可以同时进行而不会相互影响 写锁(排它锁)：当前写操作没有完成前，它会阻断其他写锁和读锁 对数据操作的粒度分： 表锁 页锁 行锁 表锁 特点： 偏向MyISAM 存储引擎，开销小，加锁快，无死锁；锁粒度大，发生所冲突的概率最高，并发度最低。 MyISAM在执行查询语句前，会自动给涉及的所有表加锁，在执行增删改操作前，会自动给涉及的表加写锁。 两种模式： 表共享读锁(Table Read Lock) 表独占写锁(Table Write Lock) 锁类型 兼容 读锁 写锁 读锁 是 是 否 写锁 是 否 否 总之就是读锁会阻塞写，但是不会阻塞读，而写锁则会把读和写都阻塞。 页锁开销和加锁时间界于表锁和行锁之间，会出现死锁；锁定粒度界于表锁和行锁之间，并发一般。 行锁 特点： 偏向InnoDB存储引擎，开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高 InnoDB与MyISAM的最大不同有两点：一是支持事务(Transanction),二是采用了行级锁 锁定一行： select xxx...for update; varchar类型必须加单引号，否则会产生自动类型转换会使索引失效，行锁会转换成表锁，造成其他线程阻塞。 InnoDB存储引擎由于实现了行级锁定，虽然在锁定机制的实现方面所带来的性能损耗可能比标记锁定会要更高一些，但是在整体并发处理能力方面要远远优于MyISAM的表级锁定的。当系统并发量较高的时候,InnoDB的整体性能和MyISAM相比就会有比较明显的优势了。 但是InnoDB的行级锁定同样也有其脆弱的一面，当我们使用不当的时候，可能会让InnoDB的整体性能表现不仅不能比MyISAM高，甚至可能会更差。 间隙锁 定义： 当我们用范围条件而不是相等条件检索数据，并请求共享或排它锁时，InnoDB会给符合条件的已有数据记录的索引项加锁；对于键值在条件范围内但并不存在的记录，叫做”间隙（GAP）“， InnoDB也会对这个”间隙“加锁，这种锁机制就是所谓的间隙锁(Next-Key锁) 。 危害 因为Query执行过程中通过范围查找的话，他会锁定整个范围内所有的索引键值，即使这个键值并不存在。 间隙锁有一个比较致命的弱点，就是当预定一个范围键值之后，即使某些不存在的键值也会被无辜锁定，而造成在锁定的时候无法插入锁定键值范围内的任何数据。在某些场景下可能对性能造成很大危害。(比如查询键值为2到5的数据时候，即使不存在键值为2的数据，在其他线程对2进行插入操作时该线程会被阻塞) 优化建议 尽可能让所有数据检索都通过索引来完成，避免无索引行锁升级为表锁 合理设计索引，尽量缩小锁的范围 尽可能较少检索条件，避免间隙锁 尽量控制事务的大小，减少锁定资源量和时间长度 尽可能低级别事务隔离 MySQL备份与恢复如果服务器宕机或者磁盘损坏数据库的内容就会丢失，这是非常严重的，因此需要平时备份数据库，以防意外情况出现。 备份分类按照备份方法 热备：运行时备份 冷备：停机备份，只需要复制物理文件即可 温备：加全局读锁保证备份数据一致性 按照备份文件 逻辑备份：文本文件，记录的是SQL语句，时间较长 裸文件备份：复制物理文件 按照备份内容 完全备份：整个数据库完整备份 增量备份：官方没有这种方法，需要借助二进制日志完成 日志备份： 逻辑备份与恢复基本备份格式 mysqldump [arguments] &gt;file_name 备份所有数据库 mysqldump --all-databases &gt;dump.sql 备份指定数据库 mysqldump --databases db1 db2 db3 &gt;dump.sql 备份架构 mysqldump --single-transaction db1 &gt;dump.sql 恢复 mysql -uroot -p &lt;dump.sql 二进制日志备份恢复备份 flush logs会生成新的二进制日志文件，然后备份之前的二进制日志 恢复mysqlbinlog binlog.00001|mysql -uroot -p test 导入文件mysqlbinlog binlog.00001&gt;logbin.sql mysql -u root -p -e &quot;source logbin.sql &quot; MySQL主从复制 登录mysql数据库 mysql -u root -p 给服务器设置授权用户 GRANT ALL PRIVILEGES ON *.* TO &#39;user&#39; @ &#39;%&#39; IDENTIFIED BY &#39;password&#39; WITH GRANT OPTION; %位置规定可以从哪个ip地址访问mysql数据库，%时为所有ip ​ with admin option 用于系统权限授权，with grant option 用于对象授权。 ​ 给一个用户授予系统权限带上with admin option 时，此用户可把此系统权限授予其他用户或角色，但收回这个用户的系统权限时，这个用户已经授予其他用户或角色的此系统权限不会因传播无效，如授予A系统权限create session with admin option,然后A又把create session权限授予B,但管理员收回A的create session权限时，B依然拥有create session的权限，但管理员可以显式收回B create session的权限，即直接revoke create session from B. ​ 而with grant option用于对象授权时，被授予的用户也可把此对象权限授予其他用户或角色，不同的是但管理员收回用with grant option授权的用户对象权限时，权限会因传播而失效，如：grant select on 表名 to A with grant option;，A用户把此权限授予B，但管理员收回A的权限时，B的权限也会失效，但管理员不可以直接收回B的SELECT ON TABLE 权限。 执行授权语句报错（ora-01031，ora-01929）时，可参考一下。 Mysql主从复制 mysql复制的优点 如果主服务器出现问题，可以快速切换到从服务器提供服务 可以在从服务器执行查询操作，降低主服务器的访问压力 可以在从服务器上执行备份，以避免备份期间影响主服务器的服务 注意：一般只有更新不频繁的数据或者对实时性要求不高的数据可以通过从服务器查询，实时性要求高的仍需从主数据库查询 主服务器配置 给从服务器设置授权用户 GRANT ALL SLAVE ON *.* TO &#39;user&#39;@192.168.1.1 IDENTIFIED BY &#39;password&#39;或者 GRANT ALL REPLICATION SLAVE ON *.* TO &#39;user&#39;@192.168.1.1 IDENTIFIED BY &#39;password&#39; 修改主服务器的配置文件my.cnf，开启binlog，并设置server-id的值 log-bin=mysql-bin server-id=1 在主服务器设置读锁定有效，确保没有数据库操作，以便获得一个一致性的快照（选） flush tables with read lock; 查看主服务器上当前的二进制日志名和偏移量值 show master status 目前主数据库服务器已经停止了更新操作，生成主数据库的备份，备份方式有两种： cp全部的数据 mysqldump 主数据库备份完毕后，主数据库可以恢复写操作，剩下的操作只需在从服务器上去执行 unlock tables; 把主数据库的一致性备份复制到从数据库上，放到相应目录 从服务器配置 修改以下配置 server-id,不能与主数据库相同，必须唯一。 master-host= master-user= master-password= master-port= log-bin=mysql-bin 重启mysqld服务 service mysqld restart 或者pkill mysqld 查看主从复制进程列表 show processlist \G 或者 show slave status\G \G用于行列反转 从数据库常用命令 start slave 启动复制线程 stop slave 停止 show slave status 查看从数据库的状态 show master logs 查看主数据库bin-log日志 change master to 动态改变到主服务器的配置]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程与锁(以java语言为例)]]></title>
    <url>%2F2019%2F01%2F10%2Fjava%2F%E7%BA%BF%E7%A8%8B%E4%B8%8E%E9%94%81(%E4%BB%A5java%E8%AF%AD%E8%A8%80%E4%B8%BA%E4%BE%8B)%2F</url>
    <content type="text"><![CDATA[几乎所有的编程语言都对线程和锁进行支持，但是像我们这种初级程序员不精通多线程编程就极其容易出现错误，本文算是对多线程的入门，以及简单介绍Java.util.concurrent包来实现多线程一些具体场景 Thread 和 synchronized这是最原始的并发的支持通过几个例子介绍一下 创建一个简单的线程12345678910public static void main(String[] args) throws InterruptedException &#123; Thread myThread = new Thread() &#123; public void run() &#123; System.out.println("Hello from new thread"); &#125; &#125;; myThread.start(); System.out.println("Hello from main thread"); &#125;&#125; Java中创建线程的方法主要就是两种，创建Thread对象，和实现Runnable接口，Thread也是Runnable接口的实现类。上述例子我们使用Thread类实现了两个线程（一个是main线程一个是我们定义的线程）。 Thread 常用方法 currentThread(): 返回当前线程 run(): 执行处理逻辑 start(): 启动线程 join(): 等待相应的线程结束。若线程A调用线程B的join方法，那么线程A的运行会被暂停，直到线程B运行结束。 yield(): 使当前线程主动放弃对处理器的占用，这可能导致当前线程被暂停。—-这个方法是不可靠的。 使用一把锁来计数123456789101112131415161718192021222324252627public class Counting &#123; public static void main(String[] args) throws InterruptedException &#123; class Counter &#123;//内部类用于计数 private int count = 0; public synchronized void increment() &#123; ++count; &#125; public int getCount() &#123; return count; &#125; &#125; final Counter counter = new Counter();//并发的资源 class CountingThread extends Thread &#123;//操纵技术的线程 public void run() &#123; for(int x = 0; x &lt; 100; ++x) counter.increment(); &#125; &#125; CountingThread t1 = new CountingThread(); CountingThread t2 = new CountingThread(); t1.start(); t2.start(); t1.join(); t2.join(); System.out.println(counter.getCount()); &#125;&#125; 上述代码我们用synchronized关键字修饰了一个增加数量的方法，我们创建了两个线程来并发调用这个增加的方法,每一个线程+100，最后我们可以得到结果是200。如果读者吧synchronized关键字去掉再运行代码，很少有机会得到正确的结果200，大部分会是一个小于200的数。因为++count会在系统内部拆成读-改-写三个阶段，即读取count，count+1，结果写回count，假设t1线程读取了count但还没有+1写回，t2也读取了count值。这样两个线程进行完+1写回操作后，虽然两个线程各执行了一遍，但其实只加了1。 synchronized不仅可以修饰在方法上也可以单独对一个变量加锁（见下文哲学家进餐问题） Thread和synchronized带来的问题当我们开启多线程的时候，程序员对线程调用顺序是不可控制的，这是由cpu自由进行调度的，因此会带来一堆问题，例如A线程修改 boolean value=true（默认为false）,线程B value==true? “true” : “false”。 那么问题来了如果A执行B的结果就是true，如果B先执行则结果就是false。 还有一个更让人头疼的问题是指令重排序（见博客JVM篇详细介绍），指令重排序可能发生在编译环节，JVM动态优化环节，硬件调用缓解。为什么要保留指令重排序设计呢，指令重排序确实会带来性能的提升，在单线程环境下也不会影响结果，但一旦到了多线程问题就浮现出来了。 哲学家进餐问题这是一个经典多线程问题，5个哲学家在圆形餐桌围成一圈每两个人之间有一根筷子，哲学家在思考，当哲学家想用餐时需要把左右手的筷子都拿到才能就餐。 /1.jpg) 定义筷子对象 12345class Chopstick &#123; private int id; public Chopstick(int id) &#123; this.id = id; &#125; public int getId() &#123; return id; &#125;&#125; 定义哲学家 1234567891011121314151617181920212223242526class Philosopher extends Thread &#123; private Chopstick left, right; private Random random; private int thinkCount; public Philosopher(Chopstick left, Chopstick right) &#123; this.left = left; this.right = right; random = new Random(); &#125; public void run() &#123; try &#123; while(true) &#123; ++thinkCount; if (thinkCount % 10 == 0) System.out.println("Philosopher " + this + " has thought " + thinkCount + " times"); Thread.sleep(random.nextInt(1000)); // 模拟思考 synchronized(left) &#123; // 拿起左筷子 synchronized(right) &#123; // 拿起右筷子 Thread.sleep(random.nextInt(1000)); // 模拟就餐 &#125; &#125; &#125; &#125; catch(InterruptedException e) &#123;&#125; &#125;&#125; 并发执行 12345678910111213141516public class DiningPhilosophers &#123; public static void main(String[] args) throws InterruptedException &#123; Philosopher[] philosophers = new Philosopher[5]; Chopstick[] chopsticks = new Chopstick[5]; for (int i = 0; i &lt; 5; ++i) chopsticks[i] = new Chopstick(i);//定义筷子 for (int i = 0; i &lt; 5; ++i) &#123; philosophers[i] = new Philosopher(chopsticks[i], chopsticks[(i + 1) % 5]);//定义哲学家分配左右筷子 philosophers[i].start();//并发执行 &#125; for (int i = 0; i &lt; 5; ++i) philosophers[i].join(); &#125;&#125; 请读者聚焦于哲学家类的run()方法，具体细节不用深究只需要看synchronized方法块就可以，我们就是按照我们想的拿起锁定左筷子，然后拿起锁定右筷子，开始就餐。但是会有这种情况，就是1个线程（哲学家）只执行了拿起左筷子操作，然后切换到另一个线程他也只执行了拿起左筷子，也被切到另外一个线程….最后5个线程都只执行了拿起左筷子操作，于是没有右筷子了程序就挂死在这了。这就是死锁，相互等待资源。只要程序中可能出现死锁就一定会出现，只是运行时间长短问题。 123456789101112131415161718192021222324252627282930class Philosopher extends Thread &#123; private Chopstick first, second; private Random random; private int thinkCount; public Philosopher(Chopstick left, Chopstick right) &#123; if(left.getId() &lt; right.getId()) &#123; first = left; second = right; &#125; else &#123; first = right; second = left; &#125; random = new Random(); &#125; public void run() &#123; try &#123; while(true) &#123; ++thinkCount; if (thinkCount % 10 == 0) System.out.println("Philosopher " + this + " has thought " + thinkCount + " times"); Thread.sleep(random.nextInt(1000)); synchronized(first) &#123; synchronized(second) &#123; Thread.sleep(random.nextInt(1000)); &#125; &#125; &#125; &#125; catch(InterruptedException e) &#123;&#125; &#125;&#125; 我们对筷子进行编号，哲学家每次先拿小筷子然后在拿大筷子，也就是最后一个哲学家他的左手是5号筷子右手边是1号筷子，其他哲学家都是左手边小号筷子右手边大筷子，这样就不会出现都拿左手筷子的情况了。 其实破坏死锁的核心就是破坏资源等待环。如果程序中出现了死锁我们只能终止程序运行来结束这种状况。试想如果大型服务出现死锁要重启服务器，而且不第一时间修复仍然可能再现。 Java.util.ConcurrentJava中原生提供了关于并发的支持就是就只有Thread类和synchronized来获取锁（下文中锁的分类），但是这远远不够，幸亏JDK1.5之后引入了Concurrent包。 ReentrantLock()上文中我们提到如果死锁必须终止程序才能终止死锁，ReentrantLock()则为我们提供了如果长时间获取不到资源可以终止线程，我们也可以自定义超时时间 123456789101112131415161718192021222324252627282930313233class Philosopher extends Thread &#123; private ReentrantLock leftChopstick, rightChopstick; private Random random; private int thinkCount; public Philosopher(ReentrantLock leftChopstick, ReentrantLock rightChopstick) &#123; this.leftChopstick = leftChopstick; this.rightChopstick = rightChopstick; random = new Random(); &#125; public void run() &#123; try &#123; while(true) &#123; ++thinkCount; if (thinkCount % 10 == 0) System.out.println("Philosopher " + this + " has thought " + thinkCount + " times"); Thread.sleep(random.nextInt(1000)); // 思考 leftChopstick.lock(); try &#123; if (rightChopstick.tryLock(1000, TimeUnit.MILLISECONDS)) &#123; // 获取右筷子 try &#123; Thread.sleep(random.nextInt(1000)); // 吃 &#125; finally &#123; rightChopstick.unlock(); &#125; &#125; else &#123; // 获取不到筷子放弃然后重新去思考 System.out.println("Philosopher " + this + " timed out"); &#125; &#125; finally &#123; leftChopstick.unlock(); &#125;//释放左筷子 &#125; &#125; catch(InterruptedException e) &#123;&#125; &#125;&#125; 我们还是对最原始的哲学家就餐问题使用，此时筷子资源变成了ReentrantLock，按照原来分析这个程序原本会在某一天死锁，但是ReentrantLock长时间获取不到右筷子也可以释放资源，虽然死锁有了解决的余地，但是程序中还是会出现的死锁的可能。 ReentrantLock().newCondition()12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class Philosopher extends Thread &#123; private boolean eating; private Philosopher left; private Philosopher right; private ReentrantLock table; private Condition condition; private Random random; private int thinkCount; public Philosopher(ReentrantLock table) &#123; eating = false; this.table = table; condition = table.newCondition(); random = new Random(); &#125; public void setLeft(Philosopher left) &#123; this.left = left; &#125; public void setRight(Philosopher right) &#123; this.right = right; &#125; public void run() &#123; try &#123; while (true) &#123; think(); eat(); &#125; &#125; catch (InterruptedException e) &#123;&#125; &#125; private void think() throws InterruptedException &#123; table.lock(); try &#123; eating = false; left.condition.signal(); right.condition.signal(); &#125; finally &#123; table.unlock(); &#125; ++thinkCount; if (thinkCount % 10 == 0) System.out.println("Philosopher " + this + " has thought " + thinkCount + " times"); Thread.sleep(1000); &#125; private void eat() throws InterruptedException &#123; table.lock(); try &#123; while (left.eating || right.eating) condition.await(); eating = true; &#125; finally &#123; table.unlock(); &#125; Thread.sleep(1000); &#125;&#125; 1234567891011121314151617public class DiningPhilosophers &#123; public static void main(String[] args) throws InterruptedException &#123; Philosopher[] philosophers = new Philosopher[5]; ReentrantLock table = new ReentrantLock(); for (int i = 0; i &lt; 5; ++i) philosophers[i] = new Philosopher(table); for (int i = 0; i &lt; 5; ++i) &#123; philosophers[i].setLeft(philosophers[(i + 4) % 5]); philosophers[i].setRight(philosophers[(i + 1) % 5]); philosophers[i].start(); &#125; for (int i = 0; i &lt; 5; ++i) philosophers[i].join(); &#125;&#125; 此时我们取消了左右筷子，而是这是左右哲学家，只要左右哲学家没有就餐哲学家就能就餐，而且锁定对象为整个餐桌，当哲学家思考时他会把调用左右哲学家condition.signal()告诉左右哲学家我没在使用筷子，你们可以用，而当哲学家用餐时会看看左右哲学家是否在吃，如果没有则自己就餐，如果隔壁哲学家在吃，则会调用await()方法阻塞自己，直到隔壁的哲学家调用了signal()方法释放资源。 AtomicInteger123456789101112131415161718192021public class Counting &#123; public static void main(String[] args) throws InterruptedException &#123; final AtomicInteger counter = new AtomicInteger(); class CountingThread extends Thread &#123; public void run() &#123; for(int x = 0; x &lt; 100; ++x) counter.incrementAndGet(); &#125; &#125; CountingThread t1 = new CountingThread(); CountingThread t2 = new CountingThread(); t1.start(); t2.start(); t1.join(); t2.join(); System.out.println(counter.get()); &#125;&#125; 上文中我们使用synchronized来保证两个线程计数会得到正确的结果，原因在于++count操作不是原子（原子是不可分的），他会拆成三个操作，现在我们换成AtomicInteger，他是int的一个封装，可以实现多线程进行运算操作的得到正确的结果。 ExecutorService123456789101112131415161718192021222324252627282930313233public class EchoServer &#123; public static void main(String[] args) throws IOException &#123; class ConnectionHandler implements Runnable &#123; InputStream in; OutputStream out; ConnectionHandler(Socket socket) throws IOException &#123; in = socket.getInputStream(); out = socket.getOutputStream(); &#125; public void run() &#123; try &#123; int n; byte[] buffer = new byte[1024]; while((n = in.read(buffer)) != -1) &#123; out.write(buffer, 0, n); out.flush(); &#125; &#125; catch (IOException e) &#123;&#125; &#125; &#125; ServerSocket server = new ServerSocket(4567); int threadPoolSize = Runtime.getRuntime().availableProcessors() * 2; ExecutorService executor = Executors.newFixedThreadPool(threadPoolSize); while (true) &#123; Socket socket = server.accept(); executor.execute(new ConnectionHandler(socket)); &#125; &#125;&#125; 我们是用Socket开启了一个端口4567，来监听客户端发送来的数据，但是如果每次有客户端连接就要建立一个线程，在实际生产中服务器很有可能会挂掉，我们使用ExecutorService来创建线程池，线程池的大小是可用处理器数量的2倍，我们只需要把线程类扔到线程池去，让线程池去管理线程的创建使用。对于IO密集型任务，我们可以把线程池设得稍微大一些，让CPU可以处理更多IO建立的请求然后。对于CPU密集型任务，我们可以让线程池小一些，让CPU能更专注CPU任务的计算。 ArrayBlockingQueue 实现生产消费12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public class ProductorConsumerDemo &#123; public static void main(String[] args) &#123; ArrayBlockingQueue&lt;Apple&gt; queue=new ArrayBlockingQueue&lt;Apple&gt;(5); Productor productor=new Productor(queue); Consumer consumer=new Consumer(queue); productor.start(); consumer.start(); &#125;&#125;class Consumer extends Thread&#123; private BlockingQueue &lt;Apple&gt; queue=null; public Consumer(BlockingQueue&lt;Apple&gt; queue) &#123; this.queue = queue; &#125; @Override public void run() &#123; for (int i=0;i&lt;20;i++)&#123; Apple take = null; try &#123; System.out.println("消费中...."); take = queue.take(); System.out.println("消费完毕 "+take.toString()); Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;class Productor extends Thread&#123; private BlockingQueue &lt;Apple&gt; queue=null; public Productor(BlockingQueue&lt;Apple&gt; queue) &#123; super(); this.queue = queue; &#125; @Override public void run() &#123; for (int i=0;i&lt;20;i++)&#123; try &#123; System.out.println("生产中..."); queue.put(new Apple(i+ Calendar.getInstance().get(Calendar.SECOND))); System.out.println("生产完毕"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;// 苹果类class Apple &#123; private int id; public Apple(int id) &#123; this.id = id; &#125; @Override public String toString() &#123; return "苹果:" + (id + 1); &#125;&#125; 我们使用ArrayBlockingQueue来实现经典的生产—消费模型，首先用ArrayBlockingQueue定义一个装苹果的“篮子”，然后定义生产者往篮子里装苹果，消费者从篮子里取苹果，为了方便辨识我给苹果用时间参数标记id。篮子只能装下5个苹果，生产者要生产20个苹果，当篮子为空时调用take方法会阻塞直到篮子里有苹果，当篮子满调用put方法也会阻塞直到篮子有空间为止。程序输出会出现生产消费相互交替，证明确实在并发执行。 ConcurrentLinkedQueue上文中ArrayBlockingQueue提供了阻塞队列而ConcurrentLinkedQueue提供了非阻塞队列，操作不需要等待，容量无限（篮子无限放），也就说生产者不用担心篮子的容量限制而阻塞自己，但是我们并不会使用它，因为生产者消费速度并不会保持相同，假设如果生产者过快，把10000个苹果都装到篮子，此时消费者只消费了其中一小部分，这么大的对象是很占内存空间的，甚至直接超过内存容量，因此我们使用阻塞队列来控制生产者消费者速度不会差距太大。 java中的锁对象头每个对象都有一个对象头。HotSpot 虚拟机的对象头包括两部分信息：Mark Word（标记字段）和 Klass Pointer（类型指针）。 如果对象是数组类型，则虚拟机用3个Word（字宽）存储对象头，如果对象是非数组类型，则用2字宽存储对象头。在32位虚拟机中，一字宽等于四字节，即32bit。 Mark Word主要用于表示对象当前的状态。它用来描述对象的HashCode、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等。这部分数据的长度在32位和64位的VM(暂不考虑开启压缩指针)中分别为32bit和64bit,存储格式如下： 状态 标志位 存储内容 未锁定 01 对象哈希码、对象分代年龄 轻量级锁定 00 指向锁记录的指针 膨胀(重量级锁定) 10 执行重量级锁定的指针 GC标记 11 空(不需要记录信息) 可偏向 01 偏向线程ID、偏向时间戳、对象分代年龄 锁的分类Java SE 1.6中锁一共有4种状态，级别从低到高依次是：无锁状态，偏向锁状态，轻量级锁状态，重量级锁状态。 锁可以升级但不能降级，这种策略是为了提高获得锁和释放锁的效率。 锁 优点 缺点 适用场景 偏向锁 加锁解锁不需要额外的消耗，和执行非同步方法比仅存在纳秒级的差距 如果线程减存在锁竞争，会带来额外的锁撤销消耗 适用于只有一个线程访问同步块的场景 轻量级锁 竞争的线程不会阻塞，提高了程序的响应速度 如果始终得不到锁竞争的线程，使用自旋会消耗CPU 追求响应时间，同步块执行速度非常快 重量级锁 线程竞争不使用自旋，不会消耗CPU 线程阻塞，响应时间慢 追求吞吐量，同步快执行速度较长 偏向锁大部分情况是没有竞争的，所以可以通过偏向来提高性能。所谓的偏向，就是偏心，即锁会偏向于当前已经占有锁的线程，将对象头Mark的标记设置为偏向，并将线程ID写入对象头Mark，只要没有竞争，获得偏向锁的线程，在将来进入同步块，不需要做同步，当其他线程请求相同的锁时，偏向模式结束，在竞争激烈的场合，偏向锁会增加系统负担。 -XX:+UseBiasedLocking默认启用 轻量级锁普通的锁处理性能不够理想，轻量级锁是一种快速的锁定方法。 如果对象没有被锁定，将对象头的Mark指针保存到锁对象中，将对象头设置为指向锁的指针（在线程栈空间中） 如果轻量级锁失败，表示存在竞争，升级为重量级锁（常规锁） 在没有锁竞争的前提下，减少传统锁使用OS互斥量产生的性能损耗 在竞争激烈时，轻量级锁会多做很多额外操作，导致性能下降 自旋锁当竞争存在时，如果线程可以很快获得锁，那么可以不在OS层挂起线程，让线程做几个空操作（自旋） JDK1.6中-XX:+UseSpinning开启 JDK1.7中，去掉此参数，改为内置实现 如果同步块很长，自旋失败，会降低系统性能 如果同步块很短，自旋成功，节省线程挂起切换时间，提升系统性能 小总结已上三种锁不是Java语言层面的锁优化方法 内置于JVM中的获取锁的优化方法和获取锁的步骤 偏向锁可用会先尝试偏向锁 轻量级锁可用会先尝试轻量级锁 以上都失败，尝试自旋锁 再失败，尝试普通锁，使用OS互斥量在操作系统层挂起 锁优化方法：减少锁的持有时间，减小锁的粒度(将大对象，拆成小对象，大大增加并行度，降低锁竞争) 锁分离根据功能进行锁分离，例如将读写分离。读多写少的情况，可以提高性能，读写分离思想可以延伸，只要操作互不影响，锁就可以分离 锁粗化通常情况下，为了保证多线程间的有效并发，会要求每个线程持有锁的时间尽量短，即在使用完公共资源后，应该立即释放锁。只有这样，等待在这个锁上的其他线程才能尽早的获得资源执行任务。但是，凡事都有一个度，如果对同一个锁不停的进行请求、同步和释放，其本身也会消耗系统宝贵的资源，反而不利于性能的优化。 锁消除在即时编译器时，如果发现不可能被共享的对象，则可以消除这些对象的锁操作。 锁不是由程序员引入的，JDK自带的一些库，可能内置锁，栈上对象，不会被全局访问的，没有必要加锁 无锁锁是悲观的操作，无锁是乐观的操作 无锁的一种实现方式 CAS(Compare And Swap) 非阻塞的同步 CAS(V,E,N) 在应用层面判断多线程的干扰，如果有干扰，则通知线程重试 CAS算法的过程是这样：它包含3个参数CAS(V,E,N)。V表示要更新的变量，E表示预期值，N表示新值。仅当V值等于E值时，才会将V的值设为N，如果V值和E值不同，则说明已经有其他线程做了更新，则当前线程什么都不做。最后，CAS返回当前V的真实值。CAS操作是抱着乐观的态度进行的，它总是认为自己可以成功完成操作。当多个线程同时使用CAS操作一个变量时，只有一个会胜出，并成功更新，其余均会失败。失败的线程不会被挂起，仅是被告知失败，并且允许再次尝试，当然也允许失败的线程放弃操作。基于这样的原理，CAS操作即时没有锁，也可以发现其他线程对当前线程的干扰，并进行恰当的处理。 java.util.Concurrent实例业务需求1(Semaphore)：假如现在有20个人去售票厅买票，但窗口只有两个，同时能够有两个人买票，当有一人买票离开窗口后，剩余游艺人可以占用窗口买票 拆解：20个人就是20个线程，2个窗口是资源 实际含义：怎样控制同一时间的并发数 1234567891011121314151617181920212223242526272829303132333435363738394041public class SemaphoreDemo &#123; public static void main(String... args)&#123; SemaphoreDemo sd=new SemaphoreDemo(); sd.execute(); &#125; private void execute()&#123; //定义窗口个数 final Semaphore semaphore=new Semaphore(2); //线程池 ExecutorService threadPool= Executors.newCachedThreadPool(); //模拟20个用户 for(int i=0;i&lt;20;i++)&#123; threadPool.execute(new SemaphoreRunnable(semaphore,i+1)); &#125; threadPool.shutdown(); &#125; //执行任务类，获取信号量和释放信号量 class SemaphoreRunnable implements Runnable&#123; private Semaphore semaphore;//信号量 private int user;//记录第几个用户 public SemaphoreRunnable(Semaphore semaphore,int user)&#123; this.semaphore=semaphore; this.user=user; &#125; @Override public void run() &#123; //获取信号量许可 try &#123; semaphore.acquire(); System.out.println("用户【"+user+"】进入窗口准备买票.."); Thread.sleep((long) (Math.random()*10000)); System.out.println("用户【"+user+"】买票完成即将离开.."); Thread.sleep((long) (Math.random()*10000)); System.out.println("用户【"+user+"】离开售票窗口.."); semaphore.release(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 业务需求2(CyclicBarrier)：公司组织周末聚餐，首先各自从家到聚餐地点，全部到齐之后，才开始一起吃东西（同步点），假如人员没有到齐，到的人只能等待在那里（阻塞）。 CyclicBarrier可循环的障碍物 应用： 多线程计算之后，最后合并结果 123456789101112131415161718192021222324252627282930313233public class CyclicBarrierDemo &#123; public static void main(String... args)&#123; //三人聚餐 final CyclicBarrier cb=new CyclicBarrier(3); //线程池 ExecutorService threadPool= Executors.newCachedThreadPool(); //模拟三个用户 for(int i=0;i&lt;3;i++)&#123; final int user=i+1; Runnable r=new Runnable() &#123; @Override public void run() &#123; try &#123; Thread.sleep((long) (Math.random()*10000)); System.out.println(user+"到达聚餐，当前已有"+(cb.getNumberWaiting()+1)+"人到达"); //阻塞 cb.await(); System.out.println("人员全部到齐，开始吃饭"); Thread.sleep((long) (Math.random()*10000)); System.out.println(user+"吃完饭了，准备回家"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;; threadPool.execute(r); &#125; threadPool.shutdown(); &#125;&#125; 另外一种构造函数 12345678910111213141516171819202122232425262728293031323334353637383940414243public class CyclicBarrierDemo &#123; public static void main(String... args)&#123; //三人聚餐 final CyclicBarrier cb=new CyclicBarrier(3, new Runnable() &#123; @Override public void run() &#123; System.out.println("人员全部到齐了，各自拍照留念"); try &#123; Thread.sleep((long) (Math.random()*10000)); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); //线程池 ExecutorService threadPool= Executors.newCachedThreadPool(); //模拟三个用户 for(int i=0;i&lt;3;i++)&#123; final int user=i+1; Runnable r=new Runnable() &#123; @Override public void run() &#123; try &#123; Thread.sleep((long) (Math.random()*10000)); System.out.println(user+"到达聚餐，当前已有"+(cb.getNumberWaiting()+1)+"人到达"); //阻塞 cb.await(); System.out.println("拍照结束，开始吃饭"); Thread.sleep((long) (Math.random()*10000)); System.out.println(user+"吃完饭了，准备回家"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;; threadPool.execute(r); &#125; threadPool.shutdown(); &#125;&#125; 业务需求3(Exchanger)：绑架案，目的是为了用钱来赎人凶手绑架了小乔，要大乔拿1000万赎人，凶手和大乔同时到达约定地点，完后一手交钱一手交人。 分析：两个线程在同一个点(阻塞点)，交换数据 Exchanger两个线程进行数据交换。 应用：用于两个线程交换数据，校对工作 123456789101112131415161718192021222324252627282930313233public class ExchangerDemo &#123; public static void main(String... args)&#123; //交换器，交换String类型数据 Exchanger &lt;String&gt; ec=new Exchanger&lt;String&gt;(); //线程池 ExecutorService threadPool= Executors.newCachedThreadPool(); //凶手 threadPool.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; String returnStr=ec.exchange("小乔"); System.out.println("绑架者用小乔交换回："+returnStr); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); //大乔 threadPool.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; String returnStr=ec.exchange("1000万"); System.out.println("大乔用1000万交换回："+returnStr); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); threadPool.shutdown(); &#125;&#125; 业务场景4(CountDownLatch):CountDownLatch倒计时计数器 有一个任务，他需要等待其他某几个任务执行完毕之后才能执行 123456789101112131415161718192021222324252627282930313233343536public class CountDownLatchDemo &#123; public static void main(String... args) throws InterruptedException &#123; final CountDownLatch latch=new CountDownLatch(2);//等待两个任务执行完才开始执行任务 //任务1 new Thread()&#123; public void run()&#123; try &#123; System.out.println("任务1正在执行任务。。。"); Thread.sleep((long) (Math.random()*10000)); System.out.println("任务1执行完毕。。。"); latch.countDown(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;.start(); //任务2 new Thread()&#123; public void run()&#123; try &#123; System.out.println("任务2正在执行任务。。。"); Thread.sleep((long) (Math.random()*10000)); System.out.println("任务2执行完毕。。。"); latch.countDown(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;.start(); //主线程 System.out.println("等待其他两个任务执行完毕，主线程才开始执行任务"+Thread.currentThread().getName()); latch.await();//阻塞点 System.out.println("其他两个任务执行完毕，主线程执行任务："+Thread.currentThread().getName()); &#125;&#125; 参考本文中部分实例摘自于《七周七并发模型》推荐阅读 如果想对Java并发有更多的理解请参考《Java并发编程实战》（Java Concurrency in Practice）]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>线程</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM]]></title>
    <url>%2F2019%2F01%2F06%2Fjava%2FJVM%2F</url>
    <content type="text"><![CDATA[JAVA语言和JVM相对独立，JVM主要定义二进制class文件和JVM指令集等 Java语言规范 定义：语法，变量，类型，文法 JVM规范 定义：Class文件类型，运行时数据，帧栈，虚拟机的启动，虚拟机的指令集 本文介绍JVM基本概念，Java内存模型，GC，以及类的加载相关机制。 JVM基本概念JVM启动流程 JVM基本结构 PC寄存器： 每个线程拥有一个PC寄存器 在线程创建时 创建 指向下一条指令的地址 执行本地方法时，PC的值为undefined 方法区 保存装载的类信息：类名，访问修饰符，常量池，字段描述，方法信息，方法字节码 通常和永久区(Perm)关联在一起 JDK6时，String信息置于方法区，JDK7时，已经移动到堆中 Java堆 和程序开发密切相关 应用系统对象都保存在Java堆中 所有线程共享Java堆 对分代GC来说，堆也是分代的 GC的主要工作区间 Java栈 线程私有 栈由一系列帧组成（因此Java栈也叫做帧栈） 帧保存一个方法的局部变量、操作数栈、常量池指针 每一次方法调用创建一个帧，并压栈 Java栈 – 栈上分配 小对象（一般几十个bytes），在没有逃逸的情况下，可以直接分配在栈上 直接分配在栈上，可以自动回收，减轻GC压力 大对象或者逃逸对象无法栈上分配 JVM的进化参考http://openjdk.java.net/jeps/122 JVM内存溢出Java堆溢出java堆用于存储对象实例，只要不断创建对象，并且保证GC Roots到对象直接有可达的路径来避免垃圾回收机制清除这些对象，那么在对象数量达到最大堆的容量限制后就会产生内存异常。 通过参数-XX :+HeapDumpOnOutOfMemoryError可以让虚拟机在出现内存异常时Dump出当前的堆内存转存储快照以便事后进行分析。 虚拟机栈和本地方法栈溢出由于在HotSpot虚拟机中并不区分虚拟机栈和本地方法栈，因此，对于HotSpot来说，虽然-Xoss参数（设置本地方法栈的大小）存在，但实际上十五喜爱的，占容量只由-Xss设定。关于虚拟机栈和本地方法栈，在Java虚拟机规范中描述了两种异常： 如果线程请求的栈深度大于虚拟机所允许的最大深度，将抛出StackOverflowError异常 如果虚拟机在扩展栈时无法申请到足够的内存空间，则将抛出OutOfMemoryError异常。 如果使用虚拟机默认参数，栈深度在大多数情况下（因为每个方法压入栈的帧大小并不是一样的，所以说只能说大多数情况下）达到1000~2000完全没有问题。但是如果简历过多的线程导致的内存溢出，在不能减少线程数或者更换64位虚拟机的情况下，就只能通过减少最大堆和减少栈的容量来换取更多的线程。 方法区和运行时常量池溢出本机直接内存溢出JVM参数配置感觉调优参数命名很混乱 先说VM选项， 三种： - : 标准VM选项，VM规范的选项 -X: 非标准VM选项，不保证所有VM支持 -XX: 高级选项，高级特性，但属于不稳定的选项 参见 Java HotSpot VM Options 题主提到的参数前缀为X，显然属于第二类 再说这几个参数，其语义分别是： -Xmx: 堆的最大内存数，等同于-XX:MaxHeapSize -Xms: 堆的初始化初始化大小 -Xmn: 堆中新生代初始及最大大小，如果需要进一步细化，初始化大小用-XX:NewSize，最大大小用-XX:MaxNewSize -Xss: 线程栈大小，等同于-XX:ThreadStackSize 命名应该非简称，助记的话： memory maximum, memory startup, memory nursery/new, stack size. 以上摘自知乎 Trace跟踪参数 -verbose:gc -XX:+printGC -XX:+PrintGCDetails –打印GC详细信息 -XX:+PrintGCTimeStamps –打印CG发生的时间戳 -Xloggc:log/gc.log –指定GC log的位置，以文件输出 -XX:+TraceClassLoading –监控类的加载 -XX:+PrintHeapAtGC –每次一次GC后，都打印堆信息 -XX:+PrintClassHistogram –按下Ctrl+Break后，打印类的信息 -Xmx –Xms –指定最大堆和最小堆。例：-Xmx20m -Xms5m；Java会尽可能维持在最小堆 12345678910111213System.out.print("Xmx=");//最大空间System.out.println(Runtime.getRuntime().maxMemory()/1024.0/1024+"M");System.out.print("free mem=");//可用空间System.out.println(Runtime.getRuntime().freeMemory()/1024.0/1024+"M");System.out.print("total mem=");//总的空间System.out.println(Runtime.getRuntime().totalMemory()/1024.0/1024+"M"); 堆的分配参数 (官方推荐新生代占堆的3/8，幸存代占新生代的1/10) -Xmn –设置新生代大小 -XX:NewRatio –新生代（eden+2*Survivor区）和老年代（不包含永久区）的比值 例：–4 表示 新生代:老年代=1:4，即年轻代占堆的1/5 -XX:SurvivorRatio –设置两个Survivor区和eden的比 例：–8表示 两个Survivor :eden=2:8，即一个Survivor占年轻代的1/10 -XX:+HeapDumpOnOutOfMemoryError –OOM时导出堆到文件 -XX:+HeapDumpPath –导出OOM的路径 -XX:OnOutOfMemoryError –在OOM时，执行一个脚本 例如： –”-XX:OnOutOfMemoryError=D:/tools/jdk1.7_40/bin/printstack.bat %p“ （D:/tools/jdk1.7_40/bin/jstack -F %1 &gt;D:/a.txt） – 当程序OOM时，在D:/a.txt中将会生成线程的dump，可以在OOM时，发送邮件，甚至是重启程序 永久区分配参数 -XX:PermSize –设置永久区的初始空间 -XX:MaxPermSize -最大空间、 他们表示，一个系统可以容纳多少个类型 如果堆空间没有用完也抛出了OOM，有可能是永久区导致的 栈的分配 -Xss 栈通常只有几百K，决定了函数调用的深度，每个线程都有独立的栈空间，局部变量、参数 分配在栈上（栈帧） Java内存模型处理器处理数据要从内存进行读取数据，而处理器与内存之间是存在速度差异的，因此现在计算机采用加入缓存的手段来缓解之间的速度差异。处理器内存可以通过缓存中读取写入数据，进行数据传送，但这样同时带来一个新问题，就是缓存数据和内存数据不一致。 在多处理器中，每个处理器有各自的缓存，但是只有一个内存，那么同步缓存数据到内存时就会出现差异，于是出现一些协议规定。内存模型就是在特定的协议下，对特定内存或高速缓存进行读写访问过程的抽象。不同的架构的物理机器有不同的协议，Java虚拟机有自己的内存模型。 除了加入缓存，计算机还会对指令进行重排序，执行语句顺序不一定，但是保证结果相同。 主内存&amp;工作内存所有的变量都存储在主内存，每个线程有自己的工作内存，工作内存的使用的变量是主内存的拷贝副本，各工作内存不共享变量，需通过主内存进行交互。 内存间的交互Java定义了8中操作来完成主内存与工作内存的数据交互，每一个操作都是原子性的 lock unlock read 读取主内存变量 load 把read读取的变量放入工作内存 use 把工作内存中的变量值传给执行引擎 assign 在工作内存中赋值 store 工作内存传输到主内存 write 写回主内存变量 volatile型变量你会再很多JDK原生代码中看到这个变量，这个是java虚拟机提供的最轻量级的同步机制。当一个变量被volatile修饰之后它将具有以下两种特性： 保证此变量对所有线程可见性 我们上面提到每个线程有自己的工作内存，因此线程修改变量后没有写回主内存会出现数据不一致的情况，其他线程仍然会读取主内存老的值，使用volatile可以立即写回主内存。但并不能说volatile变量是线程安全的。 例如i++操作，这个操作分为取i，i增加1,写回i。也就是说这个操作不是原子的（可以拆分成三个步骤），在写回i之前，其他线程仍然会读取原来的i 禁止指令重排序 假设线程A执行了一系列操作之后设置flag变量为true，另外一个线程B等待flag变量为true之后休眠程序，我们单看线程A，flag什么时候执行对A都没有影响，计算机也是这么认为的，于是对指令进行了重排序，导致flag提前被设为true，程序提前被休眠。volatile可以禁止指令重排序。 对于64位数据long 和 double虚拟机允许没有被volatile修饰的变量拆分成两次32位操作，所以有可能会读到“半个变量”的情况，但现在商用虚拟机中几乎都把long double当成一次64位的原子操作，也不需要我们担心这种半个变量的情况，也就不需要volatile声明。 对于可见性我们还可以使用synchronized和final关键字来实现 volatile和synchronized区别 volatile synchronized 仅能使用在变量 可以使用在变量，方法，类 仅能保证可见性，并不能保证原子性 可以保证可见性和原子性 不会线程阻塞 可能 变量不会被编译器优化 变量可以被编译器优化 GCGC(Garbage Collection)也就是垃圾回收，程序运行中会产生一些无用的对象，如果不进行回收，将会浪费宝贵的资源空间，所以这些无用的对象，必须在无用的时候一段时间内被回收。 在java中GC的工作是由一个回收线程不断地进行扫描，然后进行回收。Java中，GC的对象是堆空间和永久区 GC回收算法引用计数算法 使用者：COM，ActionScript3，Python 实现原理： 引用计数器的实现很简单，对于一个对象A，只要有任何一个对象引用了A，则A的引用计数器就加1，当引用失效时，引用计数器就减1。只要对象A的引用计数器的值为0，则对象A就不可能再被使用。 问题：引用和去引用伴随加法和减法，影响性能；很难处理循环引用； 标记-清除算法 标记-清除算法是现代垃圾回收算法的思想基础。 实现原理： 标记-清除算法将垃圾回收分为两个阶段：标记阶段和清除阶段。一种可行的实现是，在标记阶段，首先通过根节点，标记所有从根节点开始的可达对象。因此，未被标记的对象就是未被引用的垃圾对象。然后，在清除阶段，清除所有未被标记的对象。 问题：标记和清除的效率都不高，标记清除之后，会产生大量不连续的内存碎片，当需要为大对象分配空间时，无法找到足够的连续空间而提前触发另一次垃圾收集动作。 标记-压缩（标记-整理）算法 实现原理： 标记-压缩算法适合用于存活对象较多的场合，如老年代。它在标记-清除算法的基础上做了一些优化。和标记-清除算法一样，标记-压缩算法也首先需要从根节点开始，对所有可达对象做一次标记。但之后，它并不简单的清理未标记的对象，而是将所有的存活对象压缩到内存的一端。之后，清理边界外所有的空间。 复制算法 实现原理： 将原有的内存空间分为两块，每次只使用其中一块，在垃圾回收时，将正在使用的内存中的存活对象复制到未使用的内存块中，之后，清除正在使用的内存块中的所有对象，交换两个内存的角色，完成垃圾回收。不适用于存活对象较多的场合 如老年代 问题：空间浪费 整合标记清理思想 改进： 根据IBM的研究新生代（Eden）的对象98%都是“朝生夕死”的，所以我们没有必要将分配区域划分成1:1，而是采用如下一块大的（Eden）+两块小的（Survivor）区域。每次使用一块Eden区和一块小的Survivor区，当GC时： 我们将大对象移到担保空间（一般就是老年代空间） 将每次GC都没被回收的对象（存在计数器来进行标记）也移动到老年代 然后将剩下存活的对象复制到另外一个Survivor的区域 然后清除掉Eden和原来Survivor区域 HotSpot虚拟机默认Eden和Survivor为8:1，也就是说每次整个新生代有90%的空间可以利用 ​ 分代思想依据对象的存活周期进行分类，短命对象归为新生代，长命对象归为老年代。 根据不同代的特点，选取合适的收集算法 少量对象存活，适合复制算法 大量对象存活，适合标记清理或者标记压缩 Java中的引用JDK1.2之后，Java对于引用的概念进行了扩充，将引用分为以下四个等级，强弱关系依次递减。 强引用：在程序代码中普遍存在，类似Object obj=new Object()这类的引用只要强引用存在垃圾收集器永远不会回收被引用的对象 软引用：一些还有用但是并非必需的对象。对于软引用关联着的对象，在系统将要发生内存溢出异常之前，将会把这些对象列进回收范围之中进行第二次回收。如果这次回收还没有足够的内存才会抛出内存溢出异常。JDK1.2提供SoftReference类来实现软引用。 弱引用：也是用来描述非必需对象的，但是它的强度比软引用更弱一些，被弱引用关联的对象只能生存到下一次垃圾收集发生之前。当垃圾收集器工作时，无论当前内存是否足够，都会回收掉只被弱引用关联的对象。在JDK1.2之后，提供了WeakReference类来实现弱引用。 虚引用：也成幽灵/幻影引用，他是最弱的一种引用关系。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来去的一个对象实例。为一个对象设置虚引用关联的唯一目的就是能在这个对象被收集器回收时收到一个系统通知。在JDK1.2之后提供了PhantomReference类来实现虚引用。 ​ —摘自《深入理解Java虚拟机》 可触及性 可触及的 从根节点可以触及到这个对象 可复活的 一旦所有引用被释放，就是可复活状态 因为在finalize()中可能复活该对象 不可触及的 在finalize()后，可能会进入不可触及状态 不可触及的对象不可能复活 可以回收 即使一个对象是不可触及的，也并非是真正宣告该对象的“死亡”，一个对象被宣布死亡至少要进行两次标记： 如果进行可达性分析之后没有根节点进行相连，那么将进行第一次标记，并进入依次筛选，看是否有必要执行finalize()方法 如果没必要执行finalize()（对象没有覆盖finalize方法，或者该方法已经被调用过），将会被等待回收，否则如果成功重新连接到引用链上，那么对象将会重新复活 系统只会调用一次finalize方法 12345678910111213141516171819202122232425262728293031323334353637383940public class CanReliveObj &#123; public static CanReliveObj obj; @Override protected void finalize() throws Throwable &#123; super.finalize(); System.out.println("CanReliveObj finalize called"); obj = this; &#125; @Override public String toString() &#123; return "I am CanReliveObj"; &#125; public static void main(String[] args) throws InterruptedException &#123; obj = new CanReliveObj(); obj = null; //可复活 System.gc(); Thread.sleep(1000); if (obj == null) &#123; System.out.println("obj 是 null"); &#125; else &#123; System.out.println("obj 可用"); &#125; System.out.println("第二次gc"); obj = null; //不可复活 System.gc(); Thread.sleep(1000); if (obj == null) &#123; System.out.println("obj 是 null"); &#125; else &#123; System.out.println("obj 可用"); &#125; &#125;&#125;/*CanReliveObj finalize calledobj 可用第二次gcobj 是 null*/ 注意问题：避免使用finalize()，操作不慎可能导致错误。优先级低，何时被调用， 不确定，何时发生GC不确定。可以使用try-catch-finally来替代它 Stop-The-WorldJava中一种全局暂停的现象。所有Java代码停止，native代码可以执行，但不能和JVM交互。多半由于GC引起，还有一些其他原因例如：Dump线程，死锁检查，堆Dump。 GC时为什么会有全局停顿？ 类比在聚会时打扫房间，聚会时很乱，又有新的垃圾产生，房间永远打扫不干净，只有让大家停止活动了，才能将房间打扫干净。 危害 长时间服务停止，没有响应 遇到HA系统，可能引起主备切换，严重危害生产环境。 GC收集器串行收集器Serial最古老，最稳定，效率高，可能会产生较长的停顿，只会使用一个CPU或一条线程去完成工作，工作的时候必须暂停其他工作线程 参数使用：-XX:+UseSerialGC 新生代、老年代使用串行回收 新生代复制算法 老年代标记压缩 并行收集器ParNewSerial的多线程版本 使用参数： -XX:+UseParNewGC 新生代并行，老年代串行 复制算法 多线程，需要多核支持 -XX:ParallelGCThreads 限制线程数量 并行收集器Parallel类似ParNew，更加关注吞吐量 新生代复制算法 老年代 标记-压缩 使用参数： -XX:+UseParallelGC 使用Parallel收集器+ 老年代串行 -XX:+UseParallelOldGC 使用Parallel收集器+ 老年代并行 两个调优参数： XX:MaxGCPauseMills 最大停顿时间，单位毫秒 GC尽力保证回收时间不超过设定值 XX:GCTimeRatio 0-100的取值范围 垃圾收集时间占总时间的比 默认99%，即最大允许1%的时间去做GC 这两个参数是矛盾的。因为停顿时间和吞吐量不可能同时调优 CMS收集器全称Concurrent Mark Sweep并发标记清除，使用标记-清除算法，分为四个步骤： 初始标记（根可以直接关联到的对象，速度快） 并发标记（主要标记过程，标记全部对象） 重新标记（由于并发标记时，用户线程依然运行，因此在正式清理前，再做修正） 并发清除（基于标记结果，直接清理对象） 其中耗时最长的并发标记与并发清除操作可以和用户线程一起并发执行。所以GC操作会占用用户线程导致吞吐量降低。 使用参数： -XX:+UseConcMarkSweepGC 1-XX:ParallelCMSThreads（设定CMS的线程数量） 缺点： 会影响系统整体吞吐量和性能 清理不彻底，无法处理浮动垃圾。因为在清理阶段，用户线程还在运行，会产生新的垃圾，无法清理 因为是标记清除算法，会产生大量的碎片，会找不到足够大的连续空间 -XX:+UseCMSCompactAtFullCollection 默认开启，用于在要进行FullGC时开启碎片合并 -XX:CMSFullGCsBeforeCompaction 设置执行执行多少次不压缩的FullGC后跟着一次压缩 G1收集器从JDK9开始，G1开始作为默认收集器使用，下面基于《深入理解JVM》文章简单介绍 G1之前的收集器都是针对整个新生代和老年代去使用，G1收集器将整个Java堆分成多个大小相等的独立区域（Region），虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔离，他们都是一部分Region（不需要连续）的集合。 G1跟踪各个Region里面的垃圾堆积的价值大小，在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的Region(也是Garbage-First名称的由来)。这种使用Region划分内存空间以及有优先级的区域回收方式，保证了G1收集器在有限的时间内可以获取尽可能高的收集效率。 特点： 并发与并行：G1能充分利用多CPU，使用多个CPU（CPU或者CPU核心）来缩短Stop-The-World停顿的时间，部分其他收集器原本需要停顿Java线程执行的GC动作，G1收集器仍然可以通过并发的方式让Java程序继续执行 分代收集：分代的概念在G1中仍然得以保留。虽然G1可以不需要其他收集器配合就能独立管理整个GC堆，但他能够采用不同的方式去处理新创建的对象和已经存活了一段时间、熬过多次GC的旧对象以获取更好的收集效果 空间整合：整体来看G1基于“标记-整理”算法实现的收集器，从局部（两个Region之间）上来看是基于“复制”算法实现的，但两种算法都不会产生内部碎片。 可预测停顿：G1除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒，这几乎已经是实时Java的垃圾收集器的特征了。 运作步骤 初始标记 并发标记 最终标记 筛选回收 ClassLoad类的加载Class类位于java.lang包中用于保存类的定义或者结构，保存在堆中。虚拟机把描述类的数据从Class文件加载到内存，并对数据校验，转换解析，初始化，最终形成可以被虚拟机直接使用的Java类型，这就是虚拟机的类加载机制。 Class Files -&gt; ClassLoader-&gt;运行时数据区-&gt;执行引擎，本地接口-&gt;本地方法库 加载，链接（验证，准备，解析），初始化，使用，卸载。 类的生命周期加载装载类的第一个阶段，取得类的二进制流，转换为方法区的数据结构，在Java堆中生成对应的java.lang.Class对象。注意java虚拟机根本没有指定二进制流要从.class文件中获取，也就是说你甚至可以从其他类型文件.zip.jar.war甚至网络中获取。 链接-验证目的：保证Class流的格式是正确的 文件格式的验证 是否以0xCAFEBABE开头（这个称为魔树，为了标识这是个.class文件可以理解成写在文件中的扩展名） 版本号是否合理 常量池是否有不被支持的常量 元数据验证 是否有父类 继承了final类？ 非抽象类实现了所有的抽象方法 字节码验证 (很复杂) 运行检查 栈数据类型和操作码数据参数吻合 跳转指令指定到合理的位置 符号引用验证 常量池中描述类是否存在 访问的方法或字段是否存在且有足够的权限 链接-准备 分配内存，并为类设置初始值 （方法区中），这次分配只分配类变量，不包括实例变量 链接-解析 符号引用替换为直接引用 •public static int v=1;在准备阶段中，v会被设置为0 •在初始化的&lt;clinit&gt;中才会被设置为1 •public static final int v=1; •对于static final类型，在准备阶段就会被赋上正确的值 初始化java虚拟机规范没有对进行强制约束，但是5种情况下必须初始化 遇到 new 、getstatic、putstatic、invokestatic这四条字节码指令时 使用java.lang.reflect包的方法对类进行反射调用 初始化一个类但其父类没有初始化 包含main()方法的类 使用动态语言支持 12345678910111213141516public class SuperClass &#123; static &#123; System.out.println("superclass init"); &#125; public static int value=123;&#125;class SubClass extends SuperClass&#123; static &#123; System.out.println("subclass init"); &#125;&#125;public class Main &#123; public static void main(String[] args) &#123; System.out.println(SubClass.value); &#125;&#125; 上述代码只会只会输出superclass init 和123，而subclass并不会初始化 1234567891011public class SuperClass &#123; static &#123; System.out.println("superclass init"); &#125; public static final String value="123";&#125;public class Main &#123; public static void main(String[] args) &#123; System.out.println(SuperClass.value); &#125;&#125; 注意此时静态变量 变成了static final String修饰的，上述代码只会输出123，这是因为java源码中引用了常量，通过编译优化已经讲常量“123”存储到Main的常量池中，以后Main类调用常量value都被转到自身的常量池的引用，也就是两个类没有关系。如果去掉final或者String运行代码都会导致SuperClass被初始化输出superclass init。 执行类构造器&lt;clinit&gt; 为静态变量静态方法块赋初值 子类的&lt;clinit&gt;调用前保证父类的&lt;clinit&gt;被调用 &lt;clinit&gt;是线程安全的 ClassLoad类ClassLoader是一个抽象类负责类装载过程中的加载阶段，他的实例将读入Java字节码将类装载到JVM中，可以进行定制，以满足不同字节码流获取方式（网络流，文件流等方式获取） 其中重要方法： 1public Class&lt;?&gt; loadClass(String name) throws ClassNotFoundException//载入并返回一个Class 1protected final Class&lt;?&gt; defineClass(byte[] b, int off, int len)//定义一个类，不公开调用 1protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException//loadClass回调该方法，自定义ClassLoader的推荐做法 1protected final Class&lt;?&gt; findLoadedClass(String name) //寻找已经加载的类 JVM预定义的三种类型类加载器： 启动（Bootstrap）类加载器：是用本地代码实现的类装入器，它负责将 &lt;Java_Runtime_Home&gt;/lib下面的类库加载到内存中（比如rt.jar）。由于引导类加载器涉及到虚拟机本地实现细节，开发者无法直接获取到启动类加载器的引用，所以不允许直接通过引用进行操作。 标准扩展（Extension）类加载器：是由 Sun 的 ExtClassLoader（sun.misc.Launcher$ExtClassLoader）实现的。它负责将&lt; Java_Runtime_Home &gt;/lib/ext或者由系统变量 java.ext.dir指定位置中的类库加载到内存中。开发者可以直接使用标准扩展类加载器。 应用程序（Application）类加载器：是由 Sun 的 AppClassLoader（sun.misc.Launcher$AppClassLoader）实现的。它负责将系统类路径（CLASSPATH）中指定的类库加载到内存中。开发者可以直接使用系统类加载器。由于这个类加载器是ClassLoader中的getSystemClassLoader()方法的返回值，因此一般称为系统（System）加载器。 除了以上列举的三种类加载器，还有一种比较特殊的类型 — 线程上下文类加载器。 双亲委派模型{% asset_img 3-1.png%} 双亲委派机制描述某个特定的类加载器在接到加载类的请求时，首先将加载任务委托给父类加载器，依次递归，如果父类加载器可以完成类加载任务，就成功返回；只有父类加载器无法完成此加载任务时，才自己去加载。 为什么使用：避免类的重复加载，防止内存中出现多份同样的字节码 比如两个类A和类B都要加载System类：如果不用委托而是自己加载自己的，那么类A就会加载一份System字节码，然后类B又会加载一份System字节码，这样内存中就出现了两份System字节码。如果使用委托机制，会递归的向父类查找 Java虚拟机的第一个类加载器是Bootstrap，这个加载器很特殊，它不是Java类，因此它不需要被别人加载，它嵌套在Java虚拟机内核里面，也就是JVM启动的时候Bootstrap就已经启动，它是用C++写的二进制代码（不是字节码），它可以去加载别的类。 这也是我们在测试时为什么发现System.class.getClassLoader()结果为null的原因，这并不表示System这个类没有类加载器，而是它的加载器比较特殊，是BootstrapClassLoader，由于它不是Java类，因此获得它的引用肯定返回null。 能不能自己写个类叫java.lang.System？ 答案：通常不可以，但可以采取另类方法达到这个需求。解释：为了不让我们写System类，类加载采用委托机制，这样可以保证爸爸们优先，爸爸们能找到的类，儿子就没有机会加载。而System类是Bootstrap加载器加载的，就算自己重写，也总是使用Java系统提供的System，自己写的System类根本没有机会得到加载。 但是，我们可以自己定义一个类加载器来达到这个目的，为了避免双亲委托机制，这个类加载器也必须是特殊的。由于系统自带的三个类加载器都加载特定目录下的类，如果我们自己的类加载器放在一个特殊的目录，那么系统的加载器就无法加载，也就是最终还是由我们自己的加载器加载。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[neo4j安装入门]]></title>
    <url>%2F2019%2F01%2F05%2F%E5%AE%89%E8%A3%85%E7%BB%8F%E9%AA%8C%2Fneo4j%E5%AE%89%E8%A3%85%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[Neo4j作为一个java写的图数据库，很方便用来做社交网络，下面介绍一下Neo4j安装过程 安装Neo4j不仅提供了windows的安装的zip包，还提供了桌面版的管理窗口，是很方便的。桌面管理窗口作为辅助手段，重要的是我们要启动Neo4j的服务程序 下载Neo4j，win版压缩包，名字格式为neo4j-community-版本号 解压后，为了方便使用添加其中bin目录环境变量 1234bin目录：用于存储Neo4j的可执行程序；conf目录：用于控制Neo4j启动的配置文件；data目录：用于存储核心数据库文件；plugins目录：用于存储Neo4j的插件； 打开命令行窗口输入 1neo4j console 这样Neo4j就启动起来了，以后每次运行使用这个命令就可以启动Neo4j了 另外一种方法是，Neo4j注册为服务程序 1neo4j install-service 这样以后使用这个命令就可以启动服务了 1neo4j start 常用命令参数 1234567console：打开Neo4j的控制台。start：启动Neo4j。前提是使用install-service安装了服务stop：关闭Neo4j。restart：重启Neo4j。status：查看Neo4j运行状态。install-service：安装Neo4j在Windows系统上的服务。uninstall-service：卸载Neo4j在Windows系统上的服务。 使用 按照上述步骤开启Neo4j服务后就可以访问浏览器 http://localhost:7474/ 打开管理界面（这个界面和使用官方提供的桌面管理界面一样，其实桌面管理器也是小型个浏览器） 第一次打开会让你输入Connect URL，Username，Password默认的分别为bolt://localhost:7687，neo4j，neo4j 然后提示修改密码 基本环境就搭建好了]]></content>
      <categories>
        <category>安装</category>
      </categories>
      <tags>
        <tag>Neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java新特性]]></title>
    <url>%2F2018%2F12%2F25%2Fjava%2Fjava%E6%96%B0%E7%89%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[一文收集java8——12新特性 Java8 新特性Lambda表达式第一次知道这个表达式还是在Kotlin中了解的，去查的时候发现Java8已经被使用了。 Lambda 是一个匿名函数，我们可以把 Lambda表达式理解为是一段可以传递的代码（将代码像数据一样进行传递）。可以写出更简洁、更灵活的代码。作为一种更紧凑的代码风格，使Java的语言表达能力得到了提升。 匿名类到Lambda 1234567Runnable r1=new Runnable() &#123; @Override public void run() &#123; System.out.println("helloworld"); &#125;&#125;;Runnable r2=()-&gt; System.out.println("helloworld");//Lambda 12345678TreeSet&lt;String&gt; ts=new TreeSet&lt;&gt;(new Comparator&lt;String&gt;() &#123; @Override public int compare(String o1, String o2) &#123; return Integer.compare(o1.length(),o2.length()); &#125;&#125;);TreeSet&lt;String&gt; ts2=new TreeSet&lt;&gt;(((o1, o2) -&gt; Integer.compare(o1.length(),o2.length()))); 以上代码中两句其实是等价的可以看出来Lambda表达式可以缩短我们的代码量 Lambda 表达式在Java 语言中引入了一个新的语法元素和操作符。这个操作符为 “-&gt;” ， 该操作符被称为 Lambda 操作符或剪头操作符。它将 Lambda 分为两个部分： 左侧：指定了 Lambda 表达式需要的所有参数右侧：指定了 Lambda 体，即 Lambda 表达式要执行的功能。 Lambda 表达式语法 语法格式一：无参，无返回参数，Lambda体只需要一条语句 Runnable r2=()-&gt; System.out.println(&quot;helloworld&quot;); 语法格式二：Lambda需要一个参数 Consumer&lt;String&gt; fun = (args)-&gt;System.out.println(args) 语法格式三：Lambda只需要一个参数时，参数的小括号可以省略 Consumer&lt;String&gt; fun = args-&gt;System.out.println(args) 语法格式四：Lambda需要两个参数，并且有返回值 BinaryOperator&lt;Long&gt; bo = (x,y)-&gt;{System.out.println(&quot;实现函数接口方法！&quot;);return x+y;} 语法格式五：当Lambda体只有一条语句时，return与大括号可以省略 BinaryOperator&lt;Long&gt; bo =(x,y)-&gt;x+y; 语法格式六：数据类型可以省略，因为可以由编译器推断出——类型判断。 函数式接口只包含一个抽象方法的接口，称为函数式接口 你可以通过 Lambda 表达式来创建该接口的对象。（若 Lambda表达式抛出一个受检异常，那么该异常需要在目标接口的抽象方法上进行声明）。 我们可以在任意函数式接口上使用 @FunctionalInterface 注解，这样做可以检查它是否是一个函数式接口，同时 javadoc 也会包含一条声明，说明这个接口是一个函数式接口。 1234@FunctionalInterfacepublic interface MyNumber &#123; public double getValue();&#125; 函数式接口使用泛型 1234@FunctionalInterfacepublic interface MyFun&lt;T&gt; &#123; public T getValue(T t);&#125; 函数式接口作为参数传递Lambda表达式 1234public String toUpperString(MyFunc&lt;String&gt; mf,String str)&#123; return mf.getValue(str);&#125;String newStr=toUpperstring((str)-&gt;str.toUpperCase(),"abcdef"); 作为参数传递 Lambda 表达式：为了将 Lambda 表达式作为参数传递，接收Lambda 表达式的参数类型必须是与该 Lambda 表达式兼容的函数式接口的类型。 Java内置四大核心函数式接口 函数式接口 参数类型 返回类型 用途 Consumer消费型接口 T void 对类型为T的对象应用操作，包含方法：void accept(T t) Supplier供给型接口 无 T 返回类型为T的对象，包含方法：T get(); Function函数型接口 T R 对类型为T的对象应用操作，并返回结果。结果是R类型的对象。包含方法：R apply(T t); Predicate断定型接口 T boolean 确定类型为T的对象是否满足某约束，并返回boolean 值。包含方法boolean test(T t); 方法引用当要传递给Lambda体的操作，已经有实现的方法了，可以使用方法引用！（实现抽象方法的参数列表，必须与方法引用方法的参数列表保持一致！） 方法引用：使用操作符 “::” 将方法名和对象或类的名字分隔开来。如下三种主要使用情况： 对象::实例方法 类::静态方法 类::实例方法 例如： 12345678(x)-&gt;System.out.println(x);System.out::println//等同于BinaryOperator&lt;Double&gt;bo=(x,y)-&gt;Math.pow(x,y);BinaryOperator&lt;Double&gt; bo=Math::pow;compare((x,y)-&gt;x.equals(y),"abcded","abcdef");compare(String::equals,"abc","abc"); 构造器引用格式：ClassName::new 与函数式接口相结合，自动与函数式接口中方法兼容。可以把构造器引用赋值给定义的方法，与构造器参数列表要与接口中抽象方法的参数列表一致 12Function&lt;Integer,MyClass&gt; fun=(n)-&gt;new MyClass(n);Function&lt;Integer,MyClass&gt; fun=MyClass::new; 数组引用格式：type[] :: new 12Function&lt;Integer,Integer[]&gt; fun=(n)-&gt;new MyClass[n];Function&lt;Integer,Integer[]&gt; fun=Integer[]::new; Stream APIJava8中有两大最为重要的改变。第一个是 Lambda 表达式；另外一个则是 Stream API(java.util.stream.*)。Stream 是 Java8 中处理集合的关键抽象概念，它可以指定你希望对集合进行的操作，可以执行非常复杂的查找、过滤和映射数据等操作。使用Stream API 对集合数据进行操作，就类似于使用 SQL 执行的数据库查询。也可以使用 Stream API 来并行执行操作。简而言之，Stream API 提供了一种高效且易于使用的处理数据的方式。 流(Stream) 到底是什么呢？是数据渠道，用于操作数据源（集合、数组等）所生成的元素序列。“集合讲的是数据，流讲的是计算！” 注意：①Stream 自己不会存储元素。②Stream 不会改变源对象。相反，他们会返回一个持有结果的新Stream。③Stream 操作是延迟执行的。这意味着他们会等到需要结果的时候才执行。 Stream 的操作三个步骤 创建 Stream 一个数据源（如：集合、数组），获取一个流 中间操作 一个中间操作链，对数据源的数据进行处理 终止操作(终端操作) 一个终止操作，执行中间操作链，并产生结果 数组创建流Java8 中的 Arrays 的静态方法 stream() 可以获取数组流： static Stream stream(T[] array): 返回一个流 由值创建流可以使用静态方法 Stream.of(), 通过显示值创建一个流。它可以接收任意数量的参数。 public static Stream of(T… values) : 返回一个流 由函数创建流：创建无限流可以使用静态方法 Stream.iterate() 和Stream.generate(), 创建无限流。 迭代 public static&lt;T&gt; Stream&lt;T&gt; iterate(final T seed, final UnaryOperator&lt;T&gt; f) 生成 public static&lt;T&gt; Stream&lt;T&gt; generate(Supplier&lt;T&gt; s) : 多个中间操作可以连接起来形成一个流水线，除非流水线上触发终止操作，否则中间操作不会执行任何的处理！而在终止操作时一次性全部处理，称为“惰性求值”。 并行流与串行流并行流就是把一个内容分成多个数据块，并用不同的线程分别处理每个数据块的流。 Java 8 中将并行进行了优化，我们可以很容易的对数据进行并行操作。Stream API 可以声明性地通过 parallel() 与sequential() 在并行流与顺序流之间进行切换。 Fork/Join 框架​ Fork/Join 框架：就是在必要的情况下，将一个大任务，进行拆分(fork)成若干个小任务（拆到不可再拆时），再将一个个的小任务运算的结果进行 join 汇总。 Fork/Join 框架与传统线程池的区别： 采用 “工作窃取”模式（work-stealing）：当执行新的任务时它可以将其拆分分成更小的任务执行，并将小任务加到线程队列中，然后再从一个随机线程的队列中偷一个并把它放在自己的队列中。​ 相对于一般的线程池实现,fork/join框架的优势体现在对其中包含的任务的处理方式上.在一般的线程池中,如果一个线程正在执行的任务由于某些原因无法继续运行,那么该线程会处于等待状态.而在fork/join框架实现中,如果某个子问题由于等待另外一个子问题的完成而无法继续运行.那么处理该子问题的线程会主动寻找其他尚未运行的子问题来执行.这种方式减少了线程的等待时间,提高了性能。 接口中的默认方法与静态方法Java 8中允许接口中包含具有具体实现的方法，该方法称为“默认方法”，默认方法使用 default 关键字修饰。 123456interface Myfun&lt;T&gt;&#123; T func(int a); default String getName()&#123; return "hello java8!"; &#125;&#125; 接口默认方法的”类优先”原则： 若一个接口中定义了一个默认方法，而另外一个父类或接口中又定义了一个同名的方法时 选择父类中的方法。如果一个父类提供了具体的实现，那么接口中具有相同名称和参数的默认方法会被忽略。 接口冲突。如果一个父接口提供一个默认方法，而另一个接口也提供了一个具有相同名称和参数列表的方法（不管方法是否是默认方法），那么必须覆盖该方法来解决冲突 12345678910111213141516interface Myfun&#123; default String getName()&#123; return "hello java8!"; &#125;&#125;interface Named&#123; default String getName()&#123; return "hello cokid"; &#125;&#125;class MyClass implements Myfun,Named&#123; @Override public String getName() &#123; return Named.super.getName(); &#125;&#125; Java8 中，接口中允许添加静态方法。 123456789interface Named&#123; public Integer myFun(); default String getName()&#123; return "hello cokid"; &#125; static void show()&#123; System.out.println("Hello"); &#125;&#125; Optional类到目前为止，臭名昭著的空指针异常是导致Java应用程序失败的最常见原因。以前，为了解决空指针异常，Google公司著名的Guava项目引入了Optional类，Guava通过使用检查空值的方式来防止代码污染，它鼓励程序员写更干净的代码。受到Google Guava的启发，Optional类已经成为Java8类库的一部分。 Optional实际上是个容器：它可以保存类型T的值，或者仅仅保存null。Optional提供很多有用的方法，这样我们就不用显式进行空值检测。 Optional类的Javadoc描述如下：这是一个可以为null的容器对象。如果值存在则isPresent()方法会返回true，调用get()方法会返回该对象。 常用方法：Optional.of(T t) : 创建一个 Optional 实例Optional.empty() : 创建一个空的 Optional 实例Optional.ofNullable(T t):若 t 不为 null,创建 Optional 实例,否则创建空实例isPresent() : 判断是否包含值orElse(T t) : 如果调用对象包含值，返回该值，否则返回torElseGet(Supplier s) :如果调用对象包含值，返回该值，否则返回 s 获取的值map(Function f): 如果有值对其处理，并返回处理后的Optional，否则返回 Optional.empty()flatMap(Function mapper):与 map 类似，要求返回值必须是Optional 重复注解与类型注解Java 8对注解处理提供了两点改进：可重复的注解及可用于类型的注解。 集合的底层源码实现详见Java常用API源码分析 新时间API如果我们可以跟别人说：“我们在1502643933071见面，别晚了！”那么就再简单不过了。但是我们希望时间与昼夜和四季有关，于是事情就变复杂了。JDK 1.0中包含了一个java.util.Date类，但是它的大多数方法已经在JDK 1.1引入Calendar类之后被弃用了。而Calendar并不比Date好多少。它们面临的问题是： 可变性：像日期和时间这样的类应该是不可变的。 偏移性：Date中的年份是从1900开始的，而月份都从0开始。 格式化：格式化只对Date有用，Calendar则不行。 此外，它们也不是线程安全的；不能处理闰秒等。 总结：对日期和时间的操作一直是Java程序员最痛苦的地方之一。 第三次引入的API是成功的，并且java8中引入的java.time API 已经纠正了过去的缺陷，将来很长一段时间内它都会为我们服务。 Java 8 吸收了 Joda-Time 的精华，以一个新的开始为Java 创建优秀的API。新的 java.time 中包含了所有关于本地日期（LocalDate）、本地时间（LocalTime）、本地日期时间（LocalDateTime）、时区（ZonedDateTime）和持续时间（Duration）的类。历史悠久的Date 类新增了 toInstant() 方法，用于把 Date 转换成新的表示形式。这些新增的本地化时间日期 API 大大简化了日期时间和本地化的管理。 java.time – 包含值对象的基础包 java.time.chrono – 提供对不同的日历系统的访问 java.time.format – 格式化和解析时间和日期 java.time.temporal – 包括底层框架和扩展特性 java.time.zone – 包含时区支持的类 LocalDate、LocalTime、LocalDateTime 类是其中较重要的几个类，它们的实例是不可变的对象，分别表示使用 ISO-8601日历系统的日期、时间、日期和时间。它们提供了简单的本地日期或时间，并不包含当前的时间信息，也不包含与时区相关的信息。 在处理时间和日期的时候，我们通常会想到年,月,日,时,分,秒。然而，这只是时间的一个模型，是面向人类的。第二种通用模型是面向机器的，或者说是连续的。在此模型中，时间线中的一个点表示为一个很大的数，这有利于计算机处理。在UNIX中，这个数从1970年开始，以秒为的单位；同样的，在Java中，也是从1970年开始，但以毫秒为单位。 java.time包通过值类型Instant提供机器视图，不提供处理人类意义上的时间单位。Instant表示时间线上的一点，而不需要任何上下文信息，例如，时区。概念上讲，它只是简单的表示自1970年1月1日0时0分0秒（UTC）开始的秒数。因为java.time包是基于纳秒计算的，所以Instant的精度可以达到纳秒级。(1ns = 10-9 s) 1秒 =1000毫秒 =10^6微秒=10^9纳秒 Java9 新特性java 9 提供了超过 150 项新功能特性，包括备受期待的模块化系统、可交互的 REPL 工具：jshell，JDK 编译工具，Java 公共 API 和私有代码，以及安全增强、扩展提升、性能管理改善等。可以说 Java 9 是一个庞大的系统工程，完全做了一个整体改变。 参考：http://openjdk.java.net/projects/jdk9/ 具体来讲： 模块化系统 jShell 命令 多版本兼容 jar 包 接口的私有方法 钻石操作符的使用升级 语法改进：try 语句 下划线使用限制 String 存储结构变更 便利的集合特性：of() 增强的 Stream API 多分辨率图像 API 全新的 HTTP 客户端 API Deprecated 的相关 API 智能 Java 编译工具 统一的 JVM 日志系统 javadoc 的 HTML 5 支持 Javascript 引擎升级：Nashorn java 的动态编译器 从Java 9 这个版本开始，Java 的计划发布周期是 6 个月，下一个 Java 的主版本将于 2018 年 3 月发布，命名为 Java 18.3，紧接着再过六个月将发布 Java 18.9。这意味着java的更新从传统的以特性驱动的发布周期，转变为以时间驱动的（6 个月为周期）发布模式，并逐步的将 Oracle JDK 原商业特性进行开源。针对企业客户的需求，Oracle 将以三年为周期发布长期支持版本 JEP(JDK Enhancement Proposals)：jdk 改进提案，每当需要有新的设想时候，JEP 可以在 JCP(java community Process)之前或者同时提出非正式的规范(specification)，被正式认可的 JEP 正式写进 JDK 的发展路线图并分配版本号。 JSR(Java Specification Requests)：java 规范提案，新特性的规范出现在这一阶段，是指向 JCP(Java Community Process)提出新增一个标准化技术规范的正式请求。请求可以来自于小组/项目、JEP、JCP成员或者 java 社区(community)成员的提案，每个 java 版本都由相应的 JSR 支持。 目录结构JDK 8 的目录结构 {% asset_img 1.png %} bin 目录 包含命令行开发和调试工具，如 javac，jar 和 javadoc。 include 目录 包含在编译本地代码时使用的 C/C++头文件 lib 目录 包含 JDK 工具的几个 JAR 和其他类型的文件。 它有一个 tools.jar 文件，其中包含 javac 编译器的 Java 类 jre/bin 目录 包含基本命令，如 java 命令。 在 Windows 平台上，它包含系统的运行时动态链接库（DLL）。 jre/lib 目录 包含用户可编辑的配置文件，如.properties 和.policy文件。包含几个 JAR。 rt.jar 文件包含运行时的 Java类和资源文件。 JDK 9 的目录结构 {% asset_img 2.png %} bin 目录 包含所有命令。 在 Windows 平台上，它继续包含系统的运行时动态链接库。 conf 目录 包含用户可编辑的配置文件，例如以前位于 jre\lib 目录中的.properties 和.policy 文件 include 目录 包含要在以前编译本地代码时使用的 C/C++头文件。它只存在于 JDK 中 jmods 目录 包含 JMOD 格式的平台模块。 创建自定义运行时映像时需要它。 它只存在于 JDK 中 legal 目录 包含法律声明 lib 目录 包含非 Windows 平台上的动态链接本地库。 其子目录和文件不应由开发人员直接编辑或使用 模块化系统: Jigsaw -&gt; Modularity谈到 Java 9 大家往往第一个想到的就是 Jigsaw 项目。众所周知，Java 已经发展超过 20 年（95 年最初发布），Java 和相关生态在不断丰富的同时也越来越暴露出一些问题： Java 运行环境的膨胀和臃肿。每次JVM启动的时候，至少会有30～60MB的内存加载，主要原因是JVM需要加载rt.jar，不管其中的类是否被classloader加载，第一步整个jar都会被JVM加载到内存当中去（而模块化可以根据模块的需要加载程序运行需要的class） 当代码库越来越大，创建复杂，盘根错节的“意大利面条式代码”的几率呈指数级的增长。不同版本的类库交叉依赖导致让人头疼的问题，这些都阻碍了 Java 开发和运行效率的提升。 很难真正地对代码进行封装, 而系统并没有对不同部分（也就是 JAR 文件）之间的依赖关系有个明确的概念。每一个公共类都可以被类路径之下任何其它的公共类所访问到，这样就会导致无意中使用了并不想被公开访问的 API。 类路径本身也存在问题: 你怎么知晓所有需要的 JAR 都已经有了, 或者是不是会有重复的项呢? 同时，由于兼容性等各方面的掣肘，对 Java 进行大刀阔斧的革新越来越困难，Jigsaw 从 Java 7 阶段就开始筹备，Java 8 阶段进行了大量工作，终于在 Java 9 里落地，一种千呼万唤始出来的意味。 Jigsaw项目（后期更名为Modularity）的工作量和难度大大超出了初始规划。JSR 376 Java 平台模块化系统（JPMS, Java PlatformModule System）作为 Jigsaw 项目的核心, 其主体部分被分解成6 个 JEP(JDK Enhancement Proposals)。 作为java 9 平台最大的一个特性，随着 Java 平台模块化系统的落地，开发人员无需再为不断膨胀的 Java 平台苦恼，例如，您可以使用 jlink 工具，根据需要定制运行时环境。这对于拥有大量镜像的容器应用场景或复杂依赖关系的大型应用等，都具有非常重要的意义。 本质上讲，模块(module)的概念，其实就是package外再裹一层，也就是说，用模块来管理各个package，通过声明某个package暴露，不声明默认就是隐藏。因此，模块化使得代码组织上更安全，因为它可以指定哪些部分可以暴露，哪些部分隐藏。 设计理念模块独立、化繁为简：模块化（以 Java 平台模块系统的形式）将 JDK 分成一组模块，可以在编译时，运行时或者构建时进行组合。 实现目标 主要目的在于减少内存的开销 只须必要模块，而非全部jdk模块，可简化各种类库和大型应用的开发和维护 改进 Java SE 平台，使其可以适应不同大小的计算设备 改进其安全性，可维护性，提高性能 使用举例模块将由通常的类和新的模块声明文件（module-info.java）组成。该文件是位于 java 代码结构的顶层，该模块描述符明确地定义了我们的模块需要什么依赖关系，以及哪些模块被外部使用。在 exports 子句中未提及的所有包默认情况下将封装在模块中，不能在外部使用。 java 9demo 模块中的 ModuleTest 类使用如下： 1234567891011121314151617import com.cokid.bean.Person;import org.junit.Test;import java.util.logging.Logger;public class ModuleTest &#123;private static final Logger LOGGER =Logger.getLogger("java9test");public static void main(String[] args) &#123; Person p = new Person("马云",40); System.out.println(p); LOGGER.info("aaaaaa"); &#125; @Test public void test1()&#123; System.out.println("hello"); &#125;&#125; 对应在 java 9demo 模块的 src 下创建 module-info.java 文件： requires：指明对其它模块的依赖。 12345module java9demo &#123; requires java9test; requires java.logging; requires junit;&#125; 在 java9test 模块的指定包下提供类 Person： 1234567891011121314151617181920212223242526272829public class Person &#123; private String name; private int age; public Person() &#123; &#125; public Person(String name, int age) &#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public String toString() &#123; return "Person&#123;" + "name='" + name + '\'' + ", age=" + age + '&#125;'; &#125;&#125; 要想在 java9demo 模块中调用 java9test 模块下包中的结构，需要在java9test 的 module-info.java 中声明： 123module java9test &#123; exports com.cokid.bean;&#125; exports：控制着哪些包可以被其它模块访问到。所有不被导出的包默认都被封装在模块里面。 JShell 命令像 Python 和 Scala 之类的语言早就有交互式编程环境 REPL (read -evaluate - print - loop)了，以交互式的方式对语句和表达式进行求值。开发者只需要输入一些代码，就可以在编译前获得对程序的反馈。而之前的 Java 版本要想执行代码，必须创建文件、声明类、提供测试方法方可实现。 设计理念即写即得、快速运行 实现目标 Java 9 中终于拥有了 REPL 工具：jShell。利用 jShell 在没有创建类的情况下直接声明变量，计算表达式，执行语句。即开发时可以在命令行里直接运行 java 的代码，而无需创建 Java 文件，无需跟人解释”public static void main(String[] args)”这句废话。 jShell 也可以从文件中加载语句或者将语句保存到文件中。 jShell 也可以是 tab 键进行自动补全和自动添加分号。 使用/help intro 帮助 /import 查看已经导入的包 Tab键 自动补全 /list 有效代码片段 /var 创建的变量 /methods 创建的方法 /edit 调用外部编辑器 /open 调用文件直接写方法 /exit 退出 多版本兼容 jar 包当一个新版本的 Java 出现的时候，你的库用户要花费数年时间才会切换到这个新的版本。这就意味着库得去向后兼容你想要支持的最老的 Java 版本（许多情况下就是 Java 6 或者 Java7）。这实际上意味着未来的很长一段时间，你都不能在库中运用 Java 9 所提供的新特性。幸运的是，多版本兼容 jar 功能能让你创建仅在特定版本的 Java 环境中运行库程序选择使用的 class 版本。 语法改进：接口的私有方法Java 8 中规定接口中的方法除了抽象方法之外，还可以定义静态方法和默认的方法。一定程度上，扩展了接口的功能，此时的接口更像是一个抽象类。 在 Java 9 中，接口更加的灵活和强大，连方法的访问权限修饰符都可以声明为 private 的了，此时方法将不会成为你对外暴露的 API的一部分。 123456789101112131415161718interface MyInterface &#123; void normalInterfaceMethod(); default void methodDefault1() &#123; init(); &#125;public default void methodDefault2() &#123; init();&#125;private void init() &#123; System.out.println("默认方法中的通用操作"); &#125;&#125;class MyInterfaceImpl implements MyInterface&#123; @Override public void normalInterfaceMethod() &#123; System.out.println("实现接口的方法"); &#125;&#125; 钻石操作符我们将能够与匿名实现类共同使用钻石操作符（diamond operator）在 java 8 中如下的操作是会报错的： 编译报错信息：’&lt;&gt;’ cannot be used with anonymous classes 1234567private List&lt;String&gt; flattenStrings(List&lt;String&gt;... lists) &#123; Set&lt;String&gt; set = new HashSet&lt;&gt;()&#123;&#125;; for(List&lt;String&gt; list : lists) &#123; set.addAll(list); &#125; return new ArrayList&lt;&gt;(set);&#125; 语法改进：try 语句在 java 8 之前，我们习惯于这样处理资源的关闭： 12345678910111213141516InputStreamReader reader = null;try&#123; reader = new InputStreamReader(System.in); //流的操作 reader.read();&#125;catch (IOException e)&#123; e.printStackTrace();&#125;finally&#123; if(reader != null)&#123; try &#123; reader.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; java 8 中，可以实现资源的自动关闭，但是要求执行后必须关闭的所有资源必须在 try 子句中初始化，否则编译不通过。如下例所示： 12345try(InputStreamReader reader = newInputStreamReader(System.in))&#123;&#125;catch (IOException e)&#123; e.printStackTrace();&#125; java 9 中，用资源语句编写 try 将更容易，我们可以在 try 子句中使用已经初始化过的资源，此时的资源是 final 的： 12345678InputStreamReader reader = new InputStreamReader(System.in);OutputStreamWriter writer = new OutputStreamWriter(System.out);try(reader;writer)&#123; //reader 是 final 的，不可再被赋值// reader = null;&#125;catch (IOException e)&#123; e.printStackTrace();&#125; 下划线使用的限制在 java 8 中，标识符可以独立使用“_”来命名： 12String _ = "hello";System.out.println(_); 但是，在 java 9 中规定“_”不再可以单独命名标识符了，如果使用，会报错。但本人在Java11环境中测试仍然可以使用 String 存储结构变更String 再也不用 char[] 来存储啦，改成了 byte[] 加上编码标记，节约了一些空间。 集合工厂方法：快速创建只读集合要创建一个只读、不可改变的集合，必须构造和分配它，然后添加元素，最后包装成一个不可修改的集合。 比如： 123456List namesList = new ArrayList &lt;&gt;();namesList.add("Joe");namesList.add("Bob");namesList.add("Bill");namesList = Collections.unmodifiableList(namesList);System.out.println(namesList); 缺点：我们一下写了五行。即：它不能表达为单个表达式。当然，我们也可以稍微简单点处理： 1234567891011List&lt;String&gt; list = Collections.unmodifiableList(Arrays.asList("a", "b", "c"));Set&lt;String&gt; set = Collections.unmodifiableSet(new HashSet&lt;&gt;(Arrays.asList("a", "b", "c")));Map&lt;String,Integer&gt; map = Collections.unmodifiableMap(newHashMap&lt;String,Integer&gt;()&#123; &#123; put("a", 1); put("b", 2); put("c", 3); &#125;&#125;);map.forEach((k,v) -&gt; System.out.println(k + ":" + v)); Java 9 因此引入了方便的方法，这使得类似的事情更容易表达。List firsnamesList = List.of(“Joe”,”Bob”,”Bill”);调用集合中静态方法 of()，可以将不同数量的参数传输到此工厂方法中。此功能可用于 Set 和 List，也可用于 Map 的类似形式。此时得到的集合，是不可变的：在创建后，继续添加元素到这些集合会导致“UnsupportedOperationException” 。由于 Java 8 中接口方法的实现，可以直接在 List，Set 和 Map 的接口内定义这些方法，便于调用。 增强的 Stream API​ Java 的 Steam API 是java标准库最好的改进之一，让开发者能够快速运算，从而能够有效的利用数据并行计算。Java 8 提供的 Steam能够利用多核架构实现声明式的数据处理。​ 在 Java 9 中，Stream API 变得更好，Stream 接口中添加了 4 个新的方法：dropWhile, takeWhile, ofNullable，还有个 iterate 方法的新重载方法，可以让你提供一个 Predicate (判断条件)来指定什么时候结束迭代。 ​ 除了对 Stream 本身的扩展，Optional 和 Stream 之间的结合也得到了改进。现在可以通过 Optional 的新方法 stream() 将一个Optional 对象转换为一个(可能是空的) Stream 对象。 takeWhile()的使用： 用于从 Stream 中获取一部分数据，接收一个 Predicate 来进行选择。在有序的Stream 中，takeWhile 返回从开头开始的尽量多的元素。 12345List&lt;Integer&gt; list = Arrays.asList(45,43,76,87,42,77,90,73,67,88);list.stream().takeWhile(x -&gt; x &lt; 50).forEach(System.out::println);System.out.println();list = Arrays.asList(1,2,3,4,5,6,7,8);list.stream().takeWhile(x -&gt; x &lt; 5).forEach(System.out::println); dropWhile()的使用： dropWhile 的行为与 takeWhile 相反，返回剩余的元素。 12345List list = Arrays.asList(45,43,76,87,42,77,90,73,67,88);list.stream().dropWhile(x -&gt; x &lt; 50).forEach(System.out::println);System.out.println();list = Arrays.asList(1,2,3,4,5,6,7,8);list.stream().dropWhile(x -&gt; x &lt; 5).forEach(System.out::println); ofNullable()的使用： Java 8 中 Stream 不能完全为 null，否则会报空指针异常。而 Java 9 中的ofNullable 方法允许我们创建一个单元素 Stream，可以包含一个非空元素，也可以创建一个空 Stream。 12345678910111213141516//报 NullPointerException//Stream&lt;Object&gt; stream1 = Stream.of(null);//System.out.println(stream1.count());//不报异常，允许通过Stream&lt;String&gt; stringStream = Stream.of("AA", "BB", null);System.out.println(stringStream.count());//3//不报异常，允许通过List&lt;String&gt; list = new ArrayList&lt;&gt;();list.add("AA");list.add(null);System.out.println(list.stream().count());//2//ofNullable()：允许值为 nullStream&lt;Object&gt; stream1 = Stream.ofNullable(null);System.out.println(stream1.count());//0Stream&lt;String&gt; stream = Stream.ofNullable("hello world");System.out.println(stream.count());//1 iterator()重载的使用： 1234//原来的控制终止方式：Stream.iterate(1,i -&gt; i + 1).limit(10).forEach(System.out::println);//现在的终止方式：Stream.iterate(1,i -&gt; i &lt; 100,i -&gt; i + 1).forEach(System.out::println); Optional 类中 stream()的使用： 1234567List list = new ArrayList&lt;&gt;();list.add("Tom");list.add("Jerry");list.add("Tim");Optional&lt;List&lt;String&gt;&gt; optional =Optional.ofNullable(list);Stream&lt;List&lt;String&gt;&gt; stream = optional.stream();stream.flatMap(x -&gt;x.stream()).forEach(System.out::println); 全新的 HTTP 客户端 APIHTTP，用于传输网页的协议，早在 1997 年就被采用在目前的 1.1版本中。直到 2015 年，HTTP2 才成为标准。 HTTP/1.1和HTTP/2的主要区别是如何在客户端和服务器之间构建和传输数据。HTTP/1.1 依赖于请求/响应周期。 HTTP/2 允许服务器“push”数据：它可以发送比客户端请求更多的数据。 这使得它可以优先处理并发送对于首先加载网页至关重要的数据。 Java 9 中有新的方式来处理 HTTP 调用。它提供了一个新的 HTTP客户端（ HttpClient ）， 它 将 替代仅适用于 blocking 模式的HttpURLConnection （HttpURLConnection是在HTTP 1.0的时代创建的，并使用了协议无关的方法），并提供对 WebSocket 和 HTTP/2 的支持。此外，HTTP 客户端还提供 API 来处理 HTTP/2 的特性，比如流和服务器推送等功能。 全新的 HTTP 客户端 API 可以从 jdk.incubator.httpclient 模块中获取。因为在默认情况下，这个模块是不能根据 classpath 获取的，需要使用 add modules 命令选项配置这个模块，将这个模块添加到 classpath中。 1234567HttpClient client = HttpClient.newHttpClient();HttpRequest req =HttpRequest.newBuilder(URI.create("http://www.atguigu.com")).GET().build();HttpResponse&lt;String&gt; response = client.send(req,HttpResponse.BodyHandler.asString());System.out.println(response.statusCode());System.out.println(response.version().name());System.out.println(response.body()); Deprecated 的相关 APIJava 9 废弃或者移除了几个不常用的功能。其中最主要的是Applet API，现在是标记为废弃的。随着对安全要求的提高，主流浏览器已经取消对 Java 浏览器插件的支持。HTML5 的出现也进一步加速了它的消亡。开发者现在可以使用像 Java Web Start 这样的技术来代替 Applet，它可以实现从浏览器启动应用程序或者安装应用程序。同时，appletviewer 工具也被标记为废弃。 智能 Java 编译工具智能 java 编译工具( sjavac )的第一个阶段始于 JEP139 这个项目，用于在多核处理器情况下提升 JDK 的编译速度。如今，这个项目已经进入第二阶段，即 JEP199，其目的是改进 Java 编译工具，并取代目前 JDK 编译工具 javac，继而成为 Java 环境默认的通用的智能编译工具。JDK 9 还更新了 javac 编译器以便能够将 java 9 代码编译运行在低版本 Java 中。 统一的 JVM 日志系统日志是解决问题的唯一有效途径：曾经很难知道导致 JVM 性能问题和导致 JVM 崩溃的根本原因。不同的 JVM 日志的碎片化和日志选项（例如：JVM 组件对于日志使用的是不同的机制和规则），这使得 JVM 难以进行调试。解决该问题最佳方法：对所有的 JVM 组件引入一个单一的系统，这些 JVM 组件支持细粒度的和易配置的 JVM 日志。 javadoc 的 HTML 5 支持jdk 8 ：生成的 java 帮助文档是在 HTML 4 中，而 HTML 4 已经是很久的标准了。jdk 9 ：javadoc 的输出，现在符合兼容 HTML 5 标准。 Javascript 引擎升级：NashornNashorn 项目在 JDK 9 中得到改进，它为 Java 提供轻量级的Javascript 运行时。Nashorn 项目跟随 Netscape 的 Rhino 项目，目的是为了在 Java 中实现一个高性能但轻量级的 Javascript 运行时。Nashorn 项目使得 Java 应用能够嵌入 Javascript。它在 JDK 8 中为Java 提供一个 Javascript 引擎。JDK 9 包含一个用来解析 Nashorn 的 ECMAScript 语法树的API。这个 API 使得 IDE 和服务端框架不需要依赖 Nashorn 项目的内部实现类，就能够分析 ECMAScript 代码。 java 的动态编译器Oracle 一直在努力提高 Java 启动和运行时性能，希望其能够在更广泛的场景达到或接近本地语言的性能。但是，直到今天，谈到Java，很多 C/C++ 开发者还是会不屑地评价为启动慢，吃内存。简单说，这主要是因为 Java 编译产生的类文件是 Java 虚拟机可以理解的二进制代码，而不是真正的可执行的本地代码，需要 Java虚拟机进行解释和编译，这带来了额外的开销。 JIT（Just-in-time）编译器可以在运行时将热点编译成本地代码，速度很快。但是 Java 项目现在变得很大很复杂，因此 JIT 编译器需要花费较长时间才能热身完，而且有些 Java 方法还没法编译，性能方面也会下降。AoT 编译就是为了解决这些问题而生的。在 JDK 9 中， AOT（JEP 295: Ahead-of-Time Compilation）作为实验特性被引入进来，开发者可以利用新的 jaotc 工具将重点代码转换成类似类库一样的文件。虽然仍处于试验阶段，但这个功能使得 Java应用在被虚拟机启动之前能够先将 Java 类编译为原生代码。此功能旨在改进小型和大型应用程序的启动时间，同时对峰值性能的影响很小。但是 Java 技术供应商 Excelsior 的营销总监 Dmitry Leskov 担心 AoT 编译技术不够成熟，希望 Oracle 能够等到 Java 10 时有个更稳定版本才发布。 另外 JVMCI （JEP 243: Java-Level JVM Compiler Interface）等特性，对于整个编程语言的发展，可能都具有非常重要的意义，虽然未必引起了广泛关注。目前 Graal Core API 已经被集成进入 Java 9，虽然还只是初始一小步，但是完全用 Java 语言来实现的可靠的、高性能的动态编译器，似乎不再是遥不可及，这是 Java 虚拟机开发工程师的福音。与此同时，随着 Truffle 框架和 Substrate VM 的发展，已经让个别信心满满的工程师高呼“One VM to Rule Them All!”， 也许就在不远的将来 Ploygot 以一种另类的方式成为现实。 Java10 新特性关键新特性Java 10 的 12 项关键新特性： JEP 286: 局部变量的类型推断。该特性在社区讨论了很久并做了调查，可查看 JEP 286 调查结果 JEP 296: 将 JDK 的多个代码仓库合并到一个储存库中 JEP 304: 垃圾收集器接口。通过引入一个干净的垃圾收集器（GC）接口，改善不同垃圾收集器的源码隔离性。 JEP 307: 向 G1 引入并行 Full GC JEP 310: 应用类数据共享。为改善启动和占用空间，在现有的类数据共享（“CDS”）功能上再次拓展，以允许应用类放置在共享存档中 JEP 312: 线程局部管控。允许停止单个线程，而不是只能启用或停止所有线程 JEP 313: 移除 Native-Header Generation Tool (javah) JEP 314: 额外的 Unicode 语言标签扩展。包括：cu (货币类型)、fw (每周第一天为星期几)、rg (区域覆盖)、tz (时区) 等 JEP 316: 在备用内存设备上分配堆内存。允许 HotSpot 虚拟机在备用内存设备上分配 Java 对象堆 JEP 317: 基于 Java 的 JIT 编译器（试验版本） JEP 319: 根证书。开源 Java SE Root CA 程序中的根证书 JEP 322: 基于时间的版本发布模式。“Feature releases” 版本将包含新特性，“Update releases” 版本仅修复 Bug 我比较关心的是类型推断机制，以及G1的并行Full GC。G1的改动介绍比较少，我翻译一下吧（翻译的不太好） G1的改动 动机 G1垃圾收集器在jdk9的时候被作为默认收集器，之前的收集器都是可以并行收集，并行fullGC。为了最小化full GC对用户体验的影响，G1的fullGC 也应该被设计成并行处理。 描述 一开始G1收集器是为了避免全部收集而设计的，但是当当前的收集器无法快速回收内存时，就会发生影响性能的full GC。当前G1的fullGC采用的一个线程实现，使用“标记- 清除-紧凑”的算法。我们试图将这个算法并行化，然后使用和年轻带和混合收集器相同的线程。你可以通过-XX:ParallelGCThreads这个选项来控制线程的数量，但是这样同样会引起年轻代和混合收集器的线程变化。 风险和猜想 这项工作被假定在基础的G1设计没有阻止并行的full GC G1使用region时很可能导致并行的full GC将会比一个线程的fullGC更耗费空间 链接http://openjdk.java.net/projects/jdk/10/ 官方Feather介绍 https://www.oschina.net/translate/109-new-features-in-jdk-10 翻译109项新特性 Java11 新特性经过java9,10的迭代，java11作为长期版本新特性并不是很多，很多是对JDK9和10特性的修改正式化 以下内容根据官方文件翻译整理，并不全面，有些特性受博主知识能力限制并不能完全理解，也不一定会使用，只做简单介绍。 https://www.oracle.com/technetwork/java/javase/11-relnote-issues-5012449.html#NewFeature 重要的更改 不再支持Applet，web start application以及对浏览器部分的支持，这些api曾经在java9中被标记为过期 从java11之后不再提供单独的jre和server jre的下载，可以使用jlink创造一个小型的用户环境 移除JavaFX，可以单独作为一个模块下载 Java Mission Control 再也不会在带在Oracle JDK安装包里了，可以作为单独软件下载 windows更新包格式从tar.gz变为.zip，mac更新包格式从.app变为.dmg JDK11支持Unicode10之前JDK10只支持Unicode8 支持ChaCha20和Poly1305加密方法 HTTP ClientJDK9的时候引入 HTTP Client API，现在作为JDK11的标准库。之前放在jdk.incubator.http中现在移入到 Java.net.http Epsilon——No-Op垃圾收集器Epsilon垃圾收集器是一种No-Op垃圾收集器。也就是他只管内存的分配，而不做任何实际的垃圾回收工作，一旦java的堆空间被耗尽则jvm将会停止。 这对测试是相当有用的，我们可以用来对比其他GC的性价比。 他也可以方便用来测定内存大小，以及内存压力。 在极端的案例，他可以被使用在生命周期很短的作业中，这种作业的内存回收只会发生在JVM终止的时候，或者被使用在低垃圾回收的应用。 Epsilon是一种消极的GC，他不是提供一个手动管理内存的功能，也不是为了管理java heap的新的api，总的来说主要为了测试。因此System.gc()无效，因为它不回收内存 移除Java EE和CORBA模块java6中引入，java9标为过期，java11正式移除，JavaEE的更新迭代给JavaSE带来了许多困难。而且现在JavaEE可以在许多第三方网站获得比如Maven，所以没必要在JavaSE和JDK中包含它们。 java.util.collections123default &lt;T&gt; T[] toArray(IntFunction&lt;T[]&gt; generator) &#123; return toArray(generator.apply(0));&#125; 新增了一个toArray的方法 ZGC（实验性）ZGC是一个可扩展低延迟的垃圾收集器，它的目标是被设计成： 暂停时间不超过10ms 暂停时间不会随堆或者实时设置的大小而增长 可以处理从几百兆字节到几千兆字节大小的堆 ZGC的核心是并发垃圾收集器，这意味着在Java线程继续执行时，所有繁重的工作（标记，压缩，引用处理，字符串表清理等）都已完成。这极大地限制了垃圾收集对应用程序响应时间的负面影响。ZGC作为实验性功能包含在内。要启用它，因此需要将-XX：+ UnlockExperimentalVMOptions选项与-XX：+ UseZGC选项结合使用。ZGC的这个实验版具有以下限制： 它仅适用于Linux / x64。 不支持使用压缩的oops和/或压缩的类点。默认情况下禁用-XX：+UseCompressedOops和-XX：+UseCompressedClassPointers选项。启用它们将不起作用。 不支持类卸载。默认情况下禁用-XX：+ ClassUnloading和-XX：+ ClassUnloadingWithConcurrentMark选项。启用它们将不起作用。 不支持将ZGC与Graal结合使用。 Lambda参数可以接收本地变量当声明形参时可以使用var保留字，这会导致类型推断，如果一个lambda表达式用了var声明的形参，那么这个表达式的其他形参也必须是var声明的 直接运行单文件以前我们写完helloWorld.java文件，必须javac编译成helloWorld.class然后再java helloWorld运行。 现在可以直接java helloWorld.java运行文件,这并不是为了把java变成一门脚本语言，这只是为了在早期学习java写一些小型用例的时候提供便利 JDK10 中java命令有三种运行模式 运行一个class文件 运行一个含有main类的jar包 运行一个含有main类的模块(module) 现在新增一种：运行一个在源文件（.java）声明的类 Java12 新特性Java12于美国当地时间3月19正式发布，改动不是很多，主要是改动GC和switch表达式 低暂停时间的 GC（实验性功能）新增了一个名为 Shenandoah 的 GC 算法，通过与正在运行的 Java 线程同时进行 evacuation 工作来减少 GC 暂停时间。使用 Shenandoah 的暂停时间与堆大小无关，这意味着无论堆是 200MB 还是 200GB，都将具有相同的暂停时间。 微基准测试套件JDK 源码中新增了一套微基准测试套件，开发人员可通过它轻松运行已有的微基准测试并创建新的基准测试。 Switch 表达式（预览功能）对 switch 语句进行了扩展，使其不仅可以作为语句（statement），还可以作为表达式（expression），并且两种写法都可以使用传统的 switch 语法，或者使用简化的 “case L -&gt;” 模式匹配语法作用于不同范围并控制执行流。这些更改将简化日常编码工作，并为 switch 中的模式匹配（JEP 305）做好准备。 JVM 常量 API引入 API 来对关键类文件（key class-file）和运行时工件（run-time artifacts）的名义描述（nominal descriptions）进行建模，特别是可从常量池加载的常量。 在新的 java.lang.invoke.constant 包中定义了一系列基于值的符号引用（JVMS 5.1）类型，它们能够描述每种可加载常量。 符号引用以纯 nominal 形式描述可加载常量，与类加载或可访问性上下文区分开。有些类可以作为自己的符号引用（例如 String），而对于可链接常量，定义了一系列符号引用类型（ClassDesc、MethodTypeDesc、MethodHandleDesc 和 DynamicConstantDesc），它们包含描述这些常量的 nominal 信息。 只保留一个 AArch64 实现在保留 32 位 ARM 实现和 64 位 aarch64 实现的同时，删除与 arm64 实现相关的所有源码。 JDK 中存在两套 64 位 ARM 实现，主要存在于src/hotspot/cpu/arm和open/src/hotspot/cpu/aarch64 目录。两者都实现了 aarch64，现在将只保留后者，删除由 Oracle 提供的 arm64。这将使贡献者将他们的精力集中在单个 64 位 ARM 实现上，并消除维护两套实现所需的重复工作。 默认类数据共享归档文件针对 64 位平台，使用默认类列表增强 JDK 构建过程，以生成类数据共享（class data-sharing，CDS）归档。 可中止的 G1 混合 GC如果混合 GC 的 G1 存在超出暂停目标的可能性，则使其可中止。 G1 及时返回未使用的已分配内存增强 G1 GC，以便在空闲时自动将 Java 堆内存返回给操作系统。 为了实现向操作系统返回最大内存量的目标，G1 将在应用程序不活动期间定期执行或触发并发周期以确定整体 Java 堆使用情况。这将导致它自动将 Java 堆的未使用部分返回给操作系统。而在用户控制下，可以可选地执行完整的 GC，以使返回的内存量最大化。 版本变化带来的坑在 java 9中，应用程序和扩展类都不再是 java.net.URLClassLoader 的实例。Springboot如果是1.5.x的版本则不能再java9环境中启动，需要升级到2.0以上版本。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>新特性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构算法]]></title>
    <url>%2F2018%2F10%2F18%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[数据结构算法总结 顺序表12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include&lt;iostream&gt;using namespace std;# define maxSize 100template &lt;class DataType&gt;class SeqList&#123; private: DataType data[maxSize]; int length; public: SeqList(DataType a[],int n)&#123; if(n&gt;maxSize)throw "exception"; for(int i=0;i&lt;n;i++)&#123; data[i]=a[i]; &#125; length=n; &#125; bool Insert(int i,DataType x)&#123; if(i&lt;1||i&gt;length+1)return false; if(length&gt;=maxSize)return false; for(int j=length;j&gt;=i;j--) data[j]=data[j-1]; data[i-1]=x; length++; return true; &#125; DataType Delete(int i)&#123;//删除i位置上的元素 if(i&lt;1||i&gt;length+1)return false; DataType x=data[i-1]; for(int j=i;j&lt;length;j++) data[j-1]=data[j]; length--; return true; &#125; int Local(DataType x)&#123;//查找X for(int i=0;i&lt;length;i++)&#123; if(data[i]==x)return i+1; &#125; return 0; &#125; DataType Get(int i)&#123; if(i&lt;1||i&gt;length)throw "exception"; else return data[i-1]; &#125;&#125;;int main()&#123; &#125; 单链表123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101#include&lt;iostream&gt;using namespace std;# define maxSize 100 template &lt;class DT&gt;struct Node&#123; DT data; Node&lt;DT&gt; *next;&#125;;template &lt;class DT&gt;class LinkList&#123; private: Node&lt;DT&gt; *first; public: LinkList()&#123; first=new Node&lt;DT&gt;; first-&gt;next=NULL; &#125; LinkList_Head(DT a[],int n)&#123; first=new Node&lt;DT&gt;; first-&gt;next=NULL; for(int i=0;i&lt;n;i++)&#123; Node&lt;DT&gt; *s=new Node&lt;DT&gt;; s-&gt;data=a[i]; s-&gt;next=first-&gt;next; first-&gt;next=s; &#125; &#125; LinkList_Tail(DT a[],int n)&#123; first=new Node&lt;DT&gt;; Node&lt;DT&gt; *r=first; Node&lt;DT&gt; *s=new Node&lt;DT&gt;; for(int i=0;i&lt;n;i++)&#123; s-&gt;data=a[i]; r-&gt;next=s; r=s; &#125; r-&gt;next=NULL; &#125; int Length()&#123; Node&lt;DT&gt; *p=first-&gt;next; int count=0; while(p!=NULL)&#123; p=p-&gt;next; count++; &#125; return count; &#125; DT Get(int i)&#123; Node&lt;DT&gt; *p=first-&gt;next; int count=1; while(p!=NULL&amp;&amp;count&lt;i)&#123; p=p-&gt;next; count++; &#125; return p-&gt;data; &#125; int Locate(DT x)&#123; Node&lt;DT&gt; *p=first-&gt;next; int count=1; while(p!=NULL)&#123; if(p-&gt;data==x)return count; p=p-&gt;next; count++; &#125; return 0; &#125; void Insert(int i,DT x)&#123; Node&lt;DT&gt; *p=first; int count=0; while(p!=NULL&amp;&amp;count&lt;i-1)&#123; p=p-&gt;next; count++; &#125; Node&lt;DT&gt; *s=new Node&lt;DT&gt;; s-&gt;data=x; s-&gt;next=p-&gt;next; p-&gt;next=s; &#125; DT Delete(int i)&#123; Node&lt;DT&gt; *p=first; count=0; while(p!=NULL&amp;&amp;count&lt;i-1)&#123; p=p-&gt;next; count++; &#125; if(p==NULL||p-&gt;next==NULL)&#123; throw "位置"; &#125;else&#123; Node&lt;DT&gt; *q=p-&gt;next; DT x=q-&gt;data; p-&gt;next=q-&gt;next; delete q; return x; &#125; &#125; &#125;; int main()&#123; cout&lt;&lt;1&lt;&lt;endl;&#125; 顺序栈12345678910111213141516171819202122232425262728#include&lt;iostream&gt;using namespace std;# define StackSize 100 template &lt;class DT&gt;class SeqStack&#123; private: DT data[StackSize]; int top; public: SeqStack()&#123;top=-1;&#125; void Push(DT x)&#123; if(top==StackSize-1)throw "溢出"; data[++top]=x; &#125; DT Pop()&#123; if(top==-1)throw "溢出"; DT x=data[top--]; return x; &#125; DT GetTop()&#123; if(top!=-1) return data[top]; &#125;&#125;;int main()&#123; &#125; 链栈1234567891011121314151617181920212223242526272829303132333435#include&lt;iostream&gt;using namespace std;# define maxSize 100 template &lt;class DT&gt;struct Node&#123; DT data; Node&lt;DT&gt; *next;&#125;;template &lt;class DT&gt;class LinkStack&#123; private: Node&lt;DT&gt; *top; public: LinkStack()&#123;top=NULL;&#125; void Push(DT x)&#123; Node&lt;DT&gt; *s=new Node&lt;DT&gt;; s-&gt;data=x; s-&gt;next=top; top=s; &#125; DT Pop()&#123; if(top==NULL)throw "下溢"; DT x=top-&gt;data; Node&lt;DT&gt; *p=top; top=top-&gt;next; delete p; return x; &#125; DT GetTop()&#123; if(top!=NULL) return top-&gt;data; &#125;&#125;;int main()&#123; &#125; 顺序共享栈1234567891011121314151617181920212223242526272829303132#include&lt;iostream&gt;using namespace std;# define StackSize 100 template &lt;class DT&gt;class BothStack&#123; private: DT data[StackSize]; int top1; int top2; public: SeqStack()&#123;top=-1;&#125; void Push(int i,DT x)&#123; if(top1==top2-1)throw "溢出"; if(i==1)data[++top1]=x; if(i==2)data[--top2]=x; &#125; DT Pop(int i)&#123; if(i==1)&#123; if(top1==-1)throw "下溢"; return data[top1--]; &#125; if(i==2)&#123; if(top2==StackSize)throw "下溢"; return data[top2++]; &#125; &#125; &#125;;int main()&#123; &#125; 循环队列1234567891011121314151617181920212223242526272829303132333435#include &lt;iostream&gt;using namespace std;const int QueueSize=100;template &lt;class DT&gt;class CirQueue&#123; private: DT data[QueueSize]; int front,rear; public: CirQueue()&#123; front=rear=QueueSize-1; &#125; void EnQueue(DT x)&#123; if((rear+1)%QueueSize==front)throw "上溢"; rear=(rear+1)%QueueSize; data[rear]=x; &#125; DT DeQueue()&#123; if(rear=front)throw "下溢"; front=(front+1)%QueueSize; return data[front]; &#125; DT GetQueue()&#123; if(rear=front)throw "下溢"; int i=(front+1)%QueueSize; return data[i]; &#125; int Empty()&#123; front==rear?return 1:retuen 0; &#125;&#125;; int main()&#123; &#125; 链队列1234567891011121314151617181920212223242526272829303132333435363738394041#include &lt;iostream&gt;using namespace std;template &lt;class DT&gt;struct Node&#123; DT data; Node *next;&#125;;template &lt;class DT&gt;class LinkQueue&#123; private: Node&lt;DT&gt; *front,*rear; public: LinkQueue()&#123; Node&lt;DT&gt; *s=new Node&lt;DT&gt;; s-&gt;next=NULL; front=rear=s; &#125; void EnQueue(DT x)&#123; Node&lt;DT&gt; *s=new Node&lt;DT&gt;; s-&gt;data=x; s-&gt;next=NULL; rear-&gt;next=s; rear=s; &#125; DT DeQueue()&#123; if(rear==front) throw "下溢"; Node&lt;DT&gt; *p=front-&gt;next; DT x=p-&gt;data; front-&gt;next=p-&gt;next; if(p-&gt;next==NULL)rear=front; delete p; return x; &#125; int Empty()&#123; if(front==rear)return 1; return 0; &#125;&#125;;int main()&#123; &#125; KMP123456789101112131415161718192021222324252627282930313233343536373839404142434445#include&lt;iostream&gt;#include&lt;String&gt;using namespace std;void getnext(string T,int next[])&#123;//T[0]，next[0] 不用 int i=1,j=0; next[1]=0; while(i&lt;=T[0])&#123; if(j==0||T[i]==T[j])&#123; i++; j++; next[i]=j; &#125;else&#123; j=next[j]; &#125; &#125;&#125;int KMP(string S,string T,int next[])&#123;//s为主串，T为模式串，每个数组的0位置不用，用来存数组大小 int i=1,j=1; while(i&lt;=S[0]&amp;&amp;j&lt;=T[0])&#123; if(j==0||S[i]==T[j])&#123; i++; j++; &#125;else&#123; j=next[j]; &#125; &#125; if(j&gt;T[0])return i-T[0]; else return 0; &#125;int main()&#123; char cnt; string S="abcabaaabaabcac"; cnt=15; S=cnt+S; string T="abaabcac";// char T[10]=&#123;8,'a','b','a','a','b','c','a','c'&#125;; cnt=8; T=cnt+T; int next[9];next[0]=-1; getnext(T,next); cout&lt;&lt;KMP(S,T,next);&#125; 图邻接矩阵广搜深搜1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include &lt;iostream&gt;#include &lt;queue&gt;using namespace std;const int MaxSize=10;int visited[MaxSize]=&#123;0&#125;;template &lt;class DT&gt;class MGraph&#123; private: DT vertex[MaxSize];//存放顶点 int arc[MaxSize][MaxSize];//存放边 int vertexNum,arcNum; public: MGraph(DT a[],int n,int e)&#123;//n个顶点，e条边 vertexNum=n;arcNum=e; for(int i=0;i&lt;vertexNum;i++)vertex[i]=a[i]; for(int i=0;i&lt;vertexNum;i++) for(int j=0;j&lt;vertexNum;j++) arc[i][j]=0; for(int k=0;k&lt;arcNum;k++)&#123;int i,j; cin&gt;&gt;i&gt;&gt;j; arc[i][j]=arc[j][i]=1; &#125; &#125; void DFSTraverse(int v)&#123;//初始遍历结点 cout&lt;&lt;vertex[v]; visited[v]=1; for(int i=0;i&lt;vertexNum;i++) if(arc[v][i]==1&amp;&amp;visited[i]==0) DFSTraverse(i); &#125; void BFSTraverse(int v)&#123; queue&lt;int&gt; q; int front,rear;front=rear=-1; cout&lt;&lt;vertex[v];visited[v]=1; q.push(v); while(q.empty()!=true)&#123; v=q.pop(); for(int i=0;i&lt;vertexNum;i++)&#123; if(arc[v][i]==1&amp;&amp;visited[i]==0) cout&lt;&lt;vertex[i]; visited[i]=1; q.push(i); &#125; &#125; &#125;&#125;; int main()&#123; &#125; 图邻接表广搜深搜1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768#include &lt;iostream&gt;#include &lt;queue&gt;using namespace std;const int MaxSize=10;int visited[MaxSize]=&#123;0&#125;;struct Node&#123;//邻接点 int data; Node *next; &#125;;template &lt;class DT&gt;struct VertexNode&#123;//顶点表结点 DT vertex; Node *firstedge;&#125;;template&lt;class DT&gt;class ALGraph&#123; private: VertexNode&lt;DT&gt; adjlist[MaxSize]; int vertexNum,arcNum; public:e ALGraph(DT a[],int n,int e)&#123;//n顶点，e条边 vertexNum=n;arcNum=e; for(int i=0;i&lt;vertexNum;i++)&#123; adjlist[i].vertex=a[i]; adjlist[i].firstedge=NULL; &#125; for(int k=0;k&lt;arcNum;k++)&#123; int i,j; cin&gt;&gt;i&gt;&gt;j; Node *s=new Node; s-&gt;data=j; s-&gt;next=adjlist[i].firstedge;//头插 adjlist[i].fistedge=s; &#125; &#125; void DFSTraverse(int v)&#123; cout&lt;&lt;adjlist[v].vertex;visited[v]=1; Node *p=adjlist[v].firstedge; while(p!=NULL)&#123; int i=p-&gt;data; if(visited[i]==0)DFSTraverse(i); p=p-&gt;next; &#125; &#125; void BFSTraverse(int v)&#123; queue&lt;int&gt; Q; cout&lt;&lt;adjlist[v].vertex; visited[v]=1; Q.push(v); while(Q.empty()!=true)&#123; v=Q.pop(); Node *p=adjlist[v].firstedge; while(p!=NULL)&#123; int i=p-&gt;data; if(visited[i]==0)&#123; cout&lt;&lt;adjlist[i].vertex; visited[i]=1; Q.push(i); &#125; p=p-&gt;next; &#125; &#125; &#125; &#125;; int main()&#123; &#125; 树B树&amp;B+树http://www.cokid.cc/2018/03/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%A0%91%E7%9A%84%E9%81%8D%E5%8E%86/#more 二叉排序树123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#include &lt;iostream&gt;using namespace std;struct Node&#123; int data; Node *lchild; Node *rchild; &#125;;void InsertBST(Node *root,Node *s)&#123; if(root==NULL)root=s; else if(s-&gt;data&lt;root-&gt;data)InsertBST(root-&gt;lchild,s); else InsertBST(root-&gt;rchild,s); &#125;Node *BiSortTree(int a[],int n)&#123; Node *root=NULL; for(int i=0;i&lt;n;i++)&#123; Node *s=new Node; s-&gt;data=a[i]; s-&gt;lchild=s-&gt;rchild=NULL;//构造好要插入的节点，执行插入操作 InsertBST(root,s); &#125;&#125;//此删除假设的是删除节点是父节点的左孩子情况void DeleteBST(Node *p,Node *f)&#123;//待删结点p，双亲结点f if(p-&gt;lchild==NULL&amp;&amp;p-&gt;rchild==NULL)&#123; f-&gt;lchild=NULL; delete p; &#125;else if(p-&gt;rchild==NULL)&#123;//p只有左子树 f-&gt;lchild==p-&gt;rchild; delete p; &#125;else if(p-&gt;lchild==NULL)&#123;//p只有右子树 f-&gt;lchild=p-&gt;rchild; delete p; &#125;else&#123; Node *par=p; Node *s=p-&gt;lchild; while(s-&gt;lchild!=NULL)&#123; par=s; s=s-&gt;lchild; &#125; p-&gt;data=s-&gt;data; if(par==p)par-&gt;rchild=s-&gt;rchild; else par-&gt;lchild=s-&gt;rchild; delete s; &#125;&#125;int main()&#123; &#125; 后序遍历-非递归二叉树的后序遍历的非递归算法与二叉树的先序和中序遍历的非递归算法相比稍微复杂一点。 大致思路是：如果当前结点左右子树均为空，则可以访问当前结点，或者左右子树不均为空，但是前一个访问的结点是当前结点的左孩子或者右孩子，则也可以访问当前结点，如果前面两种情况均不满足（即，当前结点的左右孩子不均为空，并且前一个访问的结点不是当前结点左右孩子中的任意一个），则若当前结点的右孩子不为空，将右孩子入栈，若当前结点的左孩子不为空，将左孩子入栈。 1234567891011121314151617181920212223public static void postOrder(TreeNode root) &#123; if(root==null) return; Stack&lt;TreeNode&gt; stack = new Stack&lt;TreeNode&gt;(); TreeNode current = null; TreeNode pre = null; stack.push(root); while(!stack.isEmpty()) &#123; current = stack.peek(); if((current.left==null &amp;&amp; current.right==null) || (pre!=null &amp;&amp; (pre == current.left || pre == current.right))) &#123; System.out.println(current.val); stack.pop(); pre = current; &#125; else &#123; if(current.right!=null) &#123; stack.push(current.right); &#125; if(current.left != null) &#123; stack.push(current.left); &#125; &#125; &#125;&#125; 前序遍历-非递归1234567891011121314void PreOrder(Node root)&#123; if(root==null)return; Stack s=new Stack(); while(true)&#123; while(root!=null)&#123; System.out.println(root.getData()); s.push(root); root=root.getLeft(); &#125; if(s.isEmpty)break; root=s.pop; root=root.getRight(); &#125;&#125; 中序遍历-非递归123456789101112131415void PreOrder(Node root)&#123; if(root==null)return; Stack s=new Stack(); while(true)&#123; while(root!=null)&#123; s.push(root); root=root.getLeft(); &#125; if(s.isEmpty)break; root=s.pop; System.out.println(root.getData()); root=root.getRight(); &#125;&#125; 层序遍历123456789front=rear=-1;if(root==null)return;Q[++rear]=root;while(front!=rear)&#123; q=Q[++front]; cout&lt;&lt;q-&gt;data; if(q-&gt;left!=null)Q[++rear]=q-&gt;left; if(q-&gt;right!=null)Q[++rear]=q-&gt;right;&#125; 排序插入排序优点：实现简单，数据量少时效率高 如果输入序列已经预排序，时间复杂度为O(n+d),d是反转的次数。 算法运行效率优于选择排序冒泡排序即使是最坏的情况三个算法时间复杂度均为O($n^2 $) 能在接收序列的同时进行排序 123456789void InsertSort(int r[],int n)&#123; for(int i=2;i&lt;n;i++)&#123; r[0]=r[i];//r[0]用于暂存要移动的元素 for(int j=i-1;r[0]&lt;r[j];j--)&#123;//往前找插入位置 r[j+1]=r[j];//元素后移 &#125; r[j+1]=r[0];//插入 &#125;&#125; 冒泡排序按照改进的算法，对于一个已经有序的数组，算法完成第一次外层循环后就会返回。实际上只发生了 N - 1次比较，所以最好的情况下，该算法复杂度是O(n)。 123456789101112131415void BubbleSort()&#123; int [] array=&#123;1,5,3,2,6,7,9,13,54,20&#125;; for(int i=0;i&lt;array.length-1;i++)&#123; //每一轮比较的次数为N-1-i; for(int j=0;j&lt;array.length-1-i;j++)&#123; //比较相邻的两个数,小靠前 if(array[j]&gt;array[j+1])&#123; //两个数做交换.通过设置临时变量 int temp=array[j]; array[j]=array[j+1]; array[j+1]=temp; &#125; &#125; &#125;&#125; 123456789101112131415161718void BubbleSortImproved()&#123; int [] array=&#123;1,5,3,2,6,7,9,13,54,20&#125;; boolean swapped=true; for(int i=0;i&lt;array.length-1&amp;&amp;swapped;i++)&#123; swapped=false; //每一轮比较的次数为N-1-i; for(int j=0;j&lt;array.length-1-i;j++)&#123; //比较相邻的两个数,小靠前 if(array[j]&gt;array[j+1])&#123; //两个数做交换.通过设置临时变量 int temp=array[j]; array[j]=array[j+1]; array[j+1]=temp; swapped=true; &#125; &#125; &#125;&#125; 选择排序优点：容易实现，原地排序不需要额外的存储空间 缺点：扩展性差 12345678910111213141516171819void SelectSort()&#123; int [] array=&#123;1,5,3,2,6,7,9,13,54,20&#125;; int min=0;//保存最元素值的下标 for(int i=0;i&lt;array.length-1;i++)&#123; min=i; //查找最小数在数组中的下标 for(int j=i+1;j&lt;array.length;j++)&#123; if(array[min]&gt;array[j])&#123; min=j;//保存最小数的下标 &#125; &#125; //如果第i个最小的数位置不在i上,则进行交换 if(i!=min)&#123; int temp=array[i]; array[i]=array[min]; array[min]=temp; &#125; &#125;&#125; 希尔排序希尔排序是基于直接插入排序的，直接插入排序在元素较少和元素基本有序时效率是不错的，但随着元素增多和有序性破坏，效率会下降的很明显。希尔排序通过分组实现跳跃式移动，保证待排序序列基本有序后再排序，就能提高效率。 1234567891011void ShellSort(int r[],int n)&#123; for(int d=n/2;d&gt;1;d=d/2)&#123; for(int i=d+1;i&lt;n;i++)&#123; r[0]=r[i];//把元素存起来 for(int j=i-d;j&gt;0&amp;&amp;r[0]&lt;r[j];j=j-d)&#123;//往前以步长d查找元素 r[j+d]=r[j];//元素后移 &#125; r[j+d]=r[0];//插入 &#125; &#125;&#125; 快速排序快速排序的思想是分割，是分治算法技术的一个实例。确保一个元素左边的元素都小于这个元素，右边的元素都大于这个元素，然后对这两部分分别继续进行分割，从而达到排序的效果。 123456789101112131415161718192021222324252627void Quicksort(int[] a,int low,int high)&#123; int temp; if(low&lt;high)&#123; temp = partition(a,low,high); Quicksort(a,low,temp-1); Quicksort(a,temp+1,high); &#125;&#125;int partition(int[] a,int low,int high)&#123; int i=low; int j=high; while(i&lt;j)&#123; while(i&lt;j&amp;&amp;a[i]&lt;=r[j]) j--;//右侧扫描 if(i&lt;j)&#123;swap(a,i,j);i++;&#125;//小记录置前 while(i&lt;j&amp;&amp;a[i]&lt;=r[j]) i++;//左侧扫描 if(i&lt;j)&#123;swap(a,i,j);j--&#125;//大记录置后 &#125; return low;&#125;void swap(int[] a,int low,int high)&#123; if(low&lt;high)&#123; int temp=a[low]; a[low]=a[high]; a[high]=a[low]; &#125;&#125; 堆排序基于比较的排序，属于选择排序，优点是最坏的情况下O($ n \log n $) 基本思想：首先将待排序的记录序列构造成一个堆，此时，选出了堆中所有记录的最大者，然后将它从堆中移走，并将剩余的记录再调整成堆，这样又找出了次小的记录，以此类推，直到堆中只有一个记录。 1234567891011121314151617181920212223 void HeapSort(int[] a,int n)&#123; for(int i=n/2; i&gt;=1; i--)&#123; heapAdjust(a,i,n);//从最后一个有子节点的节点开始依次往前调整对应节点来生成大顶堆 &#125; for(int i=1; i&lt;n; i++)&#123; swap(a,1,n-i-1);//交换堆顶元素与未排序堆最后一个元素 heapAdjust(a,1,n-i);//根据调整节点重新生成大顶堆 &#125;&#125; void heapAdjust(int r[], int k, int m )&#123; //要筛选结点的编号为k，堆中最后一个结点的编号为m int i=k; int j=2*i;//到达下一层的左孩子 while (j&lt;=m)&#123; //筛选还没有进行到叶子 if (j&lt;m &amp;&amp; r[j]&lt;r[j+1]) j++; //左右孩子中取较大者 if (r[i]&gt;r[j]) break; else &#123; swap(r,i,j); i=j; j=2*i; &#125; &#125; &#125; 归并排序归并排序的主要操作是归并，其主要思想是：将若干有序序列逐步归并，最终得到一个有序序列。 123456789101112131415161718192021222324252627282930313233343536373839int[] sort(int[] o,int m,int n)&#123; int mid; int[] result = new int[o.length]; if(o.length == 1|| m==n)&#123; result[0] = o[0]; &#125;else&#123; mid = (m+n)/2; int[] temp1 = new int[mid-m+1]; int[] temp2 = new int[o.length-mid+m-1]; System.arraycopy(o,0,temp1,0,mid-m+1); System.arraycopy(o,mid-m+1,temp2,0,o.length-mid+m-1); int[] result1 = sort(temp1,m,mid); int[] result2 = sort(temp2,mid+1,n); result = Merge(result1,result2); &#125; return result;&#125;int[] Merge(int[] i,int[] j)&#123; int m=0,n=0,k=0; int[] result = new int[i.length+j.length]; for(; m&lt;i.length&amp;&amp;n&lt;j.length; k++)&#123; if(i[m]&lt;j[n])&#123; result[k] = i[m++]; &#125;else&#123; result[k] = j[n++]; &#125; &#125; if(m&lt;i.length)&#123; while(m&lt;i.length)&#123; result[k++] = i[m++]; &#125; &#125; if(n&lt;j.length)&#123; while(n&lt;j.length)&#123; result[k++] = j[n++]; &#125; &#125; return result;&#125; 线性排序-计数排序计数排序的基本思想是对于给定的输入序列中的每一个元素x，确定该序列中值小于x的元素的个数（此处并非比较各元素的大小，而是通过对元素值的计数和计数值的累加来确定）。一旦有了这个信息，就可以将x直接存放到最终的输出序列的正确位置上。例如，如果输入序列中只有17个元素的值小于x的值，则x可以直接存放在输出序列的第18个位置上。 牺牲空间换取时间，当K=O(n)时计数排序速度快，否则复杂度将会更高 123456789101112131415161718192021222324int[] CountSort(int[]a)&#123; int b[] = new int[a.length]; int max = a[0],min = a[0]; for(int i:a)&#123; if(i&gt;max)&#123; max=i; &#125; if(i&lt;min)&#123; min=i; &#125; &#125; int k=max-min+1;//这里k的大小是要排序的数组中，元素大小的极值差+1 int c[]=new int[k]; for(int i=0;i&lt;a.length;++i)&#123;//O(n) c[a[i]-min]+=1;//优化过的地方，减小了数组c的大小 &#125; for(int i=1;i&lt;c.length;++i)&#123;//O(k) c[i]=c[i]+c[i-1]; &#125; for(int i=a.length-1;i&gt;=0;--i)&#123;//O(n) b[--c[a[i]-min]]=a[i];//按存取的方式取出c的元素 &#125; return b; &#125; 线性排序-桶排序与计数排序类似，桶排序也对输入加以限制来提高算法性能。换言之。如果输入的序列来自固定集合，则桶排序的效率较高。例如假设所有输入元素是在【0，k-1】上的整数集合，这就表示k是输入序列中最远距离元素的数目。桶排序采用K个计数器，第i个计数器记录第i个的元素出现次数。 1234567891011static int BUCKET=10;void BucketSort(int A[],int array_size)&#123; int[] bucket=new int[BUCKET]; for(int i=0;i&lt;BUCKET;i++)bucket[i]=0; for(int i=0;i&lt;array_size;i++)++bucket[A[i]]; for(int i=0,j=0;j&lt;BUCKET;j++)&#123; for(int k=bucket[j];k&gt;0;--k)&#123; A[i++]=j; &#125; &#125;&#125; 线性排序-基数排序1）取每个元素最低有效位 2）基于最低有效位对序列中的元素进行排序，并保持具有相同位的元素的原有次序（稳定排序） 3）对下一个最低有效位重复该过程 基数排序速度取决于内部基本操作。如果输入值具有的位数长度不等。还需要对附加位进行排序，这是基数排序最慢的部分之一，也是最难进行效率优化的部分之一。 算法灵活性不如其他排序算法，对于每一种不同类型数据，基数排序都需要重写，难以编写一个处理所有数据的通用基数排序算法。 12345678910111213141516171819202122232425void RadixSort(int[] number, int d)&#123; //d表示最大的数有多少位 int k = 0; int n = 1; int m = 1; //控制键值排序依据在哪一位 int[][]temp = new int[10][number.length]; //数组的第一维表示可能的余数0-9 int[]order = new int[10]; //数组orderp[i]用来表示该位是i的数的个数 while(m &lt;= d) &#123; for(int i = 0; i &lt; number.length; i++) &#123; int lsd = ((number[i] / n) % 10); temp[lsd][order[lsd]] = number[i]; order[lsd]++; &#125; for(int i = 0; i &lt; 10; i++) &#123; if(order[i] != 0) for(int j = 0; j &lt; order[i]; j++) &#123; number[k] = temp[i][j]; k++; &#125; order[i] = 0; &#125; n *= 10; k = 0; m++; &#125;&#125; 性能比较 排序算法 平均时间复杂度 最好情况 最坏情况 空间复杂度 稳定性 插入排序 O(n^2) O(n) O(n^2) O(1) 稳定 冒泡排序 O( n^2 ) O(n) O(n^2) O(1) 稳定 选择排序 O( n^2 ) O( n^2 ) O(n^2) O(1) 不稳定 希尔排序 O(nlog n)~O(n^2) O( n^{1.3} ) O( n^2 ) O(1) 不稳定 快速排序 O( nlog n ) O( nlog n ) O( n^2 ) O(log n)~O(n) 不稳定 堆排序 O( nlog n ) O( nlog n ) O( nlog n ) O(1) 不稳定 归并排序 O( nlog n ) O( nlog n ) O( nlog n ) O(n) 稳定 线性排序-计数排序 O(n+k) O(n+k) O(n+k) O(1) 稳定 线性排序-桶排序 O(n+k) O(n+k) O( n^2) O(n+k) 稳定 线性排序-基数排序 O(n*k) O(n*k) O(n*k) O(n+k) 稳定]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[外排序]]></title>
    <url>%2F2018%2F08%2F30%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E5%A4%96%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[外部排序作为考研大纲新添加的内容完全理解需要一点时间，考试出现概率小，工作面试也几乎不会涉及，但本着求知的态度，应该去学习 概述如果对大文件进行排序，内存肯定是一下装不了的，排序算法大都是针对内排序（在内存进行的排序），所以对大文件排序，需要从磁盘分几部分调入内存，每一部分排序后，再写回磁盘。 外部排序通常用归并排序方法。 外部排序需要不停的从磁盘读写数据，因此会消耗大量的时间： 总时间=内部（内存）排序时间+外存（磁盘）信息读写时间+内部归并时间 为了下文介绍方便，引入几个变量： S：归并趟数 n：参加归并的记录数 m：归并路数 r：初试归并段（因为磁盘调入内存是以”块“为单位的，所以r可以理解为一开始划分了几个块） 多路平衡归并一般情况比较次数S (n-1) (m-1)=\lceil \log_mr\rceil*(n-1)*(m-1)=\lceil \log_2r\rceil*(n-1)*(m-1)/\lceil \log_2m\rceil(换底公式) 进行m=5的5路归并，归并时需要从5个块中选取最小的那一个，所以把每个块的第一个（块内已经排序所以第一个最小）进行5-1次比较就可以得出最小的放到输出里面 对于每一个元素（总数为n）都要进行这样的比较所以有（n-1）*（m-1），因为只剩一个时不需要比较 $ \lceil \log_mr\rceil $ 解释需要进行几趟（每一趟都是全体元素的操作，只不过每一次元素属于不同的块而已） 引入败者树比较次数从上面那个用换换底公式的那个式子可以看出\lceil \log_2r\rceil*(n-1) 是不变的，而 (m-1)/\lceil \log_2m\rceil 是会随着m的增大而增大的，也就是说我们增大m本来是为了减少访问磁盘的次数，可是另一方面这会增加内部归并的时间（因为比较次数多了），为了让内部归并不受m增大的影响引入了败者树。 图为败者树，可以看作为一个完全二叉树，其中蓝色部分为待合并的段，红色为段中第一个元素（肯定是所在段中最小的），圆形节点用来记录“失败者”(数字大的为失败者，数字小的胜利)，过程如下： 从每个段读入一个元素 b3与b4比：6&lt;12，因此b4失败，ls[4]记录4 b1与b2比：9&lt;20，因此b2失败，ls[3]记录2 然后b3与b0比较（因为第二步b3赢了，虽然结点记录的是b4的号码，但是要用胜者b3和b0比，我也不知道为什么，科学家的思想），6&lt;10，因此b0失败，ls[2]记录0 最后b3和b1比（和上面的一样，记败比胜），6&lt;9，因此b1失败，ls[1]记录1 最后胜者当然是b3了所以ls[0]记3 输出b3记录的6，然后块中下一个记录15移入b3，继续上述过程 m路归并树，深度为\lceil\log_2 m\rceil ,m个记录在树中需要\lceil\log_2 m\rceil比较得出最小的，所以总的比较次数为： 躺 记录数-1 选出最小的比较次数： S*(n-1)*\lceil \log_2 m \rceil =\lceil \log_mr\rceil*(n-1)*\lceil \log_2 m \rceil=(n-1)*\lceil \log_2 r \rceil注意既然这样可以消除m增大的影响，是不是可以随意增加m，当然不是： m增大，需要增加缓存区个数，导致每个缓存区容量减少，增加内外存交换次数 置换-选择排序（生成初始归并段）文字概念不好理解，直接举例子说 工作区容量为3 输出文件 FI 工作区 WA 输入文件FI 说明 - - 17 21 05 44 10 12 56 32 29 读入输入文件 - 17 21 05 44 10 12 56 32 29 读入三个数 05 17 21 44 10 12 56 32 29 选出最小的输出，然后再读入44填补位置 05 17 10 21 44 12 56 32 29 选出最小的但要比05大，输出，读入10 05 17 21 10 12 44 56 32 29 再选最小的但要比17大，输出，读入12 05 17 21 44 10 12 56 32 29 同上 05 17 21 44 56 10 12 32 29 同上 05 17 21 44 56# 10 12 32 29 发现没有比56大的，结束这个段 10 29 12 32 重复上述过程 10 12 29 32 10 12 29 29 32 10 12 29 32 - 10 12 29 32 # - 最佳归并树构造完初始归并段之后，接下来应该是进行合并了，合并的方案不同（谁和谁先合并）会影响读写磁盘次数，从而影响效率，因此我们选择记录数少的先合并，记录数多的后合并(很类似哈夫曼的思想) 假设我们构造完的归并段的长度为{9，30，12，18，3，17，2，6，24}进行三路归并可以有如下归并树 对没错，就是哈夫曼树 不过大多数情况段数并不是能满足每个节点都有m个子树，比如你进行3路合并，最后发现最后一步只剩了2个结点进行了合并，这就不满足我们构造一个完美的哈夫曼树条件。 因此我们引入虚段（可以理解成数据为0的结点），不够就补，补的个数有如下数量关系： 恰好构造：u=(n-1)%(m-1)==0 不能构造，补m-u-1个虚段]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>外排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[B树]]></title>
    <url>%2F2018%2F08%2F27%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2FB%E6%A0%91%2F</url>
    <content type="text"><![CDATA[总结B树的一些知识 首先明确一个概念，所谓的B-树就是B树，你可以把-理解成“杠”而不是“减”，第一次接触概念的时候找了一圈发现怎么有的材料只讲B树和B+树，有的材料只讲B-树和B+树，其实B-树==B树 B树定义B树是一种平衡的多路查找树 多路查找树：一个根节点可以有多个子树 平衡：左右子树差为1 一棵m阶的B树或者为空树，或者为满足下列特征的m叉树: 所有的叶子节点都出现在同一层，并且不带信息。叶子结点的双亲成为终端结点 树中每个结点最多有m棵子树 若根节点不是终端节点，则至少有两棵子树 除根结点之外的所有非终端结点至少有m/2(向上取整)棵子树 所有非终端节点都包含一下数据： （n,A0,K1,A1,K2,…..,Kn,An） 其中n为关键码的个数，A为指向子树根节点的指针，K为关键码。 $\lceil\frac{m}{2}\rceil-1$$\leq $n$\leq $ m-1 $K_i&lt;K_{i+1}$ $K_i&lt;$ A所指向子树中的所有结点的关键码$&lt; K_{i+1}$ B树的叶子节点可以看作外部节点（即查找失败）的结点，通常称为外结点。其实这些结点都是不存在的，指向这些节点的指针为空。所以B树的叶子可以不用画出来，因为叶子都在同一层，所以B树也是树高平衡的。另外，每个结点中关键码的个数为子树的个数减1 4阶B树 应用主要面向动态查找，通常用在文件系统中。一个结点的大小能够填满一个磁盘页，存储在B树中的指针实际上是包含其孩子结点的块号，每个节点一般允许100个或者更多的孩子。 对于包含n个关键字，高度为h，阶数为m的B树： $\log_m(n+1)\leq$h$\leq\log_{\lceil \frac{m}{2}\rceil}(\frac{n+1}{2})+1$ 查找查找步骤： 到达某个节点 在结点的有序表中查找（B树的每个结点上是多关键码的有序表） 查找成功返回，不成功，跳到子树查找 到达叶子则查找失败 在磁盘上找到某节点后，先将节点的信息读入内存，然后再查找等于K的关键码。因为磁盘上的查找比在内存中进行查找耗费时间多，所以在磁盘上进行查找的次数，即待查关键码所在结点在B树的层数，是决定B树查找效率的首要因素。 插入插入步骤：设m阶子树，n=m-1为关键码数目最大值 定位： 查找插入位置——结点p 如果p结点关键码个数小于n，则直接插入 否则执行分裂提升过程。 分裂提升： 将p”分裂“成两个设为p1,p2 把中间的关键码k“提升”到父节点 将k的左指针指向p1，右指针指向p2 如果执行“提升”过程父节点也溢出则父节点继续执行“分裂提升”过程 如果根节点也分裂了则加一层 3阶B树插入的例子 插入62 65插入溢出，分裂 65提升到父节点，然后分裂 插入30 提升28，溢出在分裂 提升28，分裂20，40 插入86 提升90，分裂 分裂至根节点，加一层 删除删除步骤： 找到关键码所在结点 判断如果结点中关键码的个数大于 $\lceil \frac{m}{2} \rceil -1$ ,则直接删除关键码 否则删除后不符合m阶B树要求，需要借结点以保证B树特性，情况分为两种： 兄弟够借： 查看相邻兄弟借点 兄弟借点记录多于 $\lceil \frac{m}{2} \rceil$ 则从兄弟借关键码上移到双亲结点 将双亲结点中关键码下移 兄弟不够借： 相邻兄弟都不够借 将被删结点合并到兄弟结点 因此双亲结点少了一个孩子结点 将双亲结点中关键码下移到合并节点 如果双亲结点下移后空了，就在执行合并兄弟结点的操作 如果合并一直上传到根节点，则B树减少一层 3阶B树删除的例子 3阶B树 删除90 删除50，向左兄弟借导致40下移，28上移 删除40，兄弟不够借，则删除40，将40结点与22结点合并，然后将28关键码下移 删除70，兄弟不够借，删除70，合并70和96结点，下移90， 合并20和空节点，60下移，导致根空，删除根节点，树高-1 B+树定义B+树是B树的一个变体，一棵m阶的B+树在结构上与m阶的B树相同，但在关键码的内部安排上有所不同： 含有m棵子树的结点有m个关键码。 关键码ki是它所对应子树的根节点中最大（或最小）关键码（所以每个节点只包含子节点关键字中的最大值以及指向子节点的指针） 所有的终端节点中包含了全部关键码信息，及指向关键码记录的指针 各终端节点按关键码的大小次序链在一起，形成单链表，并设置头指针。 与B树差异 B树 B+树 含有n个关键字的结点 含有n+1棵子树 含有n棵子树 关键字n范围 $\lceil\frac{m}{2}\rceil-1$$\leq $n$\leq $ m-1 $\lceil\frac{m}{2}\rceil$$\leq $n$\leq $ m 叶结点 包含关键字但不与非叶结点冲突 包含全部关键字信息，非叶结点只是索引，非叶结点出现过的关键字一定会在叶结点出现 操作插入，删除操作和B树基本相似 查找：如果是非叶结点上关键字值等于给定值是并不会终止，而是继续向下查找，直到叶结点上的该关键字为止，所以在B+树种查找，无论查找成功与否，每次查找都是一条从根节点到叶结点的路径]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>B树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSR安装]]></title>
    <url>%2F2018%2F05%2F15%2F%E5%AE%89%E8%A3%85%E7%BB%8F%E9%AA%8C%2FSSR%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[安装ssr 1wget -N --no-check-certificate https://makeai.cn/bash/ssrmu.sh &amp;&amp; chmod +x ssrmu.sh &amp;&amp; bash ssrmu.sh 切换内核（为了锐速） wget -N —no-check-certificate https://freed.ga/kernel/ruisu.sh &amp;&amp; bash ruisu.sh 安装锐速 wget -N —no-check-certificate https://github.com/91yun/serverspeeder/raw/master/serverspeeder.sh &amp;&amp; bash serverspeeder.sh I P : xx.xx.xx.xx 加密 : chacha20-ietf 协议 : auth_sha1_v4 混淆 : http_post]]></content>
      <categories>
        <category>安装</category>
      </categories>
      <tags>
        <tag>SSR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基础杂项]]></title>
    <url>%2F2018%2F04%2F03%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%2F%E6%9F%A5%E9%98%85%E5%B0%8F%E7%AC%94%E8%AE%B0%2F%E5%9F%BA%E7%A1%80%E6%9D%82%E9%A1%B9%2F</url>
    <content type="text"><![CDATA[其他一些需要查阅的杂项 进制转换编码方式计算机内部的数据是以二进制形式存在的 原码：直接转二进制，第一位是符号位0正1负 反码：正数的反码和原码一样，负数的反码保留符号位，其余按位取反 补码：正数的补码和原码反码一样，负数的补码=反码+1=原码取反+1 而二进制在计算机内部又有多种编码方式计算机内部对二进制进行运算用的是补码 负数的原码=负数的补码-1再取反=负数的补码取反再+1 Java中的位运算符 运算符 运算 用法 解释 &amp; 按位与运算 6&amp;3==2 只有对应两个二进制位均为1是结果为1否则为0 \ 按位或运算 6\ 3==7 一个为1结果为1 ^ 异或运算 6^3=3 相异结果为1 ~ 按位非运算 ~9=-10 参与运算求反，1变0，0变1 &lt;&lt; 左移 2&lt;&lt;3=16 往左移右边补0——0111-&gt;011100 >&gt; 右移 3&gt;&gt;1=1 往右移正数左边补0，负数补1 >&gt;&gt; 无符号右移 3&gt;&gt;&gt;1=1 往右移左边补0 与(&amp;)12345678910int num1=128;int num2=129;System.out.println(num1&amp;num2);//输出128/* 00000000 00000000 00000000 10000000(128的补码==原码) 00000000 00000000 00000000 10000001(129的补码==原码) ------------------------------------- 00000000 00000000 00000000 10000000(按位与运算结果==128)*/ 或(|)12345678910int num1=128;int num2=129;System.out.println(num1&amp;num2);//输出129/* 00000000 00000000 00000000 10000000(128的补码==原码) 00000000 00000000 00000000 10000001(129的补码==原码) ------------------------------------- 00000000 00000000 00000000 10000001(按位或运算结果==129(补码==反码))*/ 异或(^)12345678910int num1=128;int num2=129;System.out.println(num1^num2);//输出1/* 00000000 00000000 00000000 10000000(128的补码==原码) 00000000 00000000 00000000 10000001(129的补码==原码) ------------------------------------- 00000000 00000000 00000000 00000001(按位或运算结果==1)*/ 非(~)12345678int num1=128;System.out.println(~num1);//输出-129/* 00000000 00000000 00000000 10000000(128的补码==原码) 11111111 11111111 11111111 01111111(按位取反 -&gt; 负数的补码) 10000000 00000000 00000000 10000001(取反+1 -&gt; 转成原码结果-129)*/ 左移(&lt;&lt;)123456System.out.println(2&lt;&lt;3);//输出16即(2*2^3)/* 00000000 00000000 00000000 00000010(2的补码==原码) 00000000 00000000 00000000 00010000(补码==原码16)*/ 12345678System.out.println(-2&lt;&lt;3);//输出-16/* 10000000 00000000 00000000 00000010(原码) 11111111 11111111 11111111 11111110(补码) 11111111 11111111 11111111 11110000(左移--补码) 10000000 00000000 00000000 00010000(取反+1==-16)*/ 右移(&gt;&gt;)123456System.out.println(16&gt;&gt;3);//输出2/* 00000000 00000000 00000000 00010000(补码) 00000000 00000000 00000000 00000010(补码==原码==2)*/ 12345678System.out.println(-16&gt;&gt;3);//输出2/* 10000000 00000000 00000000 00010000(原码) 11111111 11111111 11111111 11110000(补码)--&gt; 11111111 11111111 11111111 11111110(补码右移三位) 10000000 00000000 00000000 00000010(原码==-2)*/ 无符号右移(&gt;&gt;&gt;)12345678System.out.println(-16&gt;&gt;&gt;3);//输出536870910==2^29-2/* 10000000 00000000 00000000 00010000(原码) 11111111 11111111 11111111 11110000(补码) 00011111 11111111 11111111 11111110(补码右移三位，变成正数了所以补码就是原码==2^29-2)*/ 日期转换JS12345var stringdate = 1402233166999;var f = new Date(stringdate);var year = f.getFullYear()+'年';var month = f.getMonth()+1+'月';var date = f.getDate()+'日'; Java1234567Date date=new Date();String strDate="2010-10-19 10:11:30.345";SimpleDateFormat s=new SimpleDateFormat("yyyy-MM-dd HH:mm:ss.SSS");//字符串转指定格式日期类型s.parse(strDate);//日期类型格式化成指定格式字符串s.format(date); 12//DateFormat.getDateTimeInstance会返回时间类型，不过默认是根据具体运行语言环境决定的DateFormat.getDateTimeInstance(DateFormat.FULL,DateFormat.FULL,new Locale("zh","CN")); 12345678910111213//Calendar类也能完成Date类的功能，已经渐渐取代Date类，Date类有些功能已经过时被标注下划线Calendar calendar=Calendar.getInstance(); System.out.println(calendar.get(Calendar.YEAR));System.out.println(calendar.get(Calendar.MONTH)+1);System.out.println(calendar.get(Calendar.DATE));System.out.println(calendar.get(Calendar.HOUR_OF_DAY));System.out.println(calendar.get(Calendar.MINUTE));System.out.println(calendar.get(Calendar.SECOND));calendar.set(Calendar.YEAR,2012);System.out.println(calendar);System.out.println(calendar.getTimeInMillis()); 图形变换 顺时针旋转 \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \\ \end{bmatrix} 逆时针旋转 \begin{bmatrix} \cos\theta & \sin\theta \\ -\sin\theta & \cos\theta \\ \end{bmatrix}]]></content>
      <categories>
        <category>查阅</category>
      </categories>
      <tags>
        <tag>进制转换</tag>
        <tag>日期转换</tag>
        <tag>图形变换</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络]]></title>
    <url>%2F2018%2F03%2F15%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%2F%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[响应代码对照 HTTP报文用于HTTP协议互换的信息被称为HTTP报文。客户端的HTTP报文叫做请求报文，服务器端的报文叫做响应报文。HTTP报文是由多行数据构成的字符串文本。 报文的结构 http请求报文与响应报文的结构几乎是一样的 请求行：包含请求的方法，请求的URI和HTTP版本 状态行：包含表明相应结果的状态码，原因短语和HTTP版本。 首部字段：包含表示请求和响应的各种条件和属性的各种首部。一般有四种首部：通用首部、请求首部、响应首部、和实体首部 其他：包含HTTP的RFC未定义的首部（Cookie） HTTP状态码 类别 原因短语 1XX Information信息性状态码 接收的请求正在处理 2XX Success 请求正常完毕 3XX Redirection重定向状态码 需要进行附加操作以完成请求 4XX Client Error 服务器无法处理请求 5XX Server Error 服务器处理请求错误 只要遵守状态码类别的定义，即使改变RFC2616中定义的状态码，或服务器端自行创建状态码都没问题。 仅记录在RFC2616上的状态码就有40种，若再加上WebDAV和附加HTTP状态码等扩展数量就达60余种。不过常用的大概只有14种。 200 OK 204 No Content 请求处理成功，但是没有资源可以返回 206 Partial Content 客户端进行了范围请求，响应中包含Content-Range指定范围实体内容 301 Moved Permanently 永久重定向，请求的资源已经分配了新的URI。如果保存为书签，应该按Location首部字段提示的URI重新保存 302 Found 临时性重定向，资源已经被分配到新的URI，希望本次能使用新的URI访问 303 See Other 由于请求对应的资源存在着另一个URI，应使用GET方法获取请求的资源 304 Not Modified 客户端发送能够附带条件的请求时，服务器端允许请求访问资源，但因发生资源请求未满足条件的情况后直接返回304 307 Temporary Redirect 临时重定向和302含义相同，尽管302禁止POST变成GET，但是实际使用时大家并不遵守 400 Bad Request 请求报文中存在语法错误，浏览器会像对待200一样对待400 401 Unauthorized 发送的请求需要有通过HTTP认证的认证信息（BASIC认证、DIGEST认证）的认证信息 403 Forbidden 对请求的资源访问被服务器拒绝了 404 Not Found服务器上没有请求的资源 500 Internal Server Error 服务器执行请求时发生了错误。也有可能是Web应用存在的bug或某些临时的故障 503 Service Unavailable 服务器暂时处于超负载或者正在进行停机维护，现在无法处理请求 报文首部首部字段是构成HTTP报文的要素之一，无论请求还是响应都会使用到，能起到传递额外信息的作用。 首部字段是由首部字段名和字段值构成的中间用冒号分隔 首部字段名：字段值 HTTP/1.1首部字段： 通用首部字段 首部字段名 说明 Cache-Control 控制缓存的行为 Connection 逐跳首部、连接的管理 Date 创建报文的日期时间 Pragma 报文指令 Trailer 报文末端的首部一览 Transfer-Encoding 制定报文主题的传输编码方式 Upgrade 升级为其他协议 Via 代理服务器的相关信息 Warning 错误通知 请求首部字段 首部字段名 说明 Accept 用户代理可处理的媒体类型 Accept-Charset 优先的字符集 Accept-Encoding 优先的内容编码 Accept-Language 优先的语言 Authorization Web认证信息 Expect 期待服务器的特定行为 From 用户的电子邮箱地址 Host 请求资源所在的服务器 if-Match 比较实体标记ETag if-Modified-Since 比较资源的更新时间 if-None-Match 比较实体标记（与if-Match相反） if-Range 资源未更新时发送实体Byte的范围请求 if-Unmodified-Since 比较资源的更新时间 Max-Forwords 最大传输逐跳数 Proxy-Authentization 代理服务器要求客户端的认证信息 Range 实体的字节范围请求 Referer 对请求中URI的原始获取方 TE 传输编码的优先级 User-Agent HTTP客户端程序的信息 响应首部字段 首部字段名 说明 是否接受字节范围请求 Age 推算资源创建经过时间 ETag 资源的匹配信息 Location 令客户端重定向至指定URI Proxy-Authenticate 代理服务器对客户端的认证信息 Retry-After 对再次发起请求的时机要求 Server HTTP服务器的安装信息 Vary 代理服务器缓存的管理信息 WWW-Authenticate 服务器对客户端的认证信息 实体首部字段 首部字段名 说明 Allow 资源可支持的HTTP方法 Content-Encoding 实体主体适用的编码方式 Content-Language 实体主体的自然语言 Content-Length 实体主体的大小，单位字节 Content-Location 替代对应资源的URI Content-MD5 实体主体的报文摘要 Content-Range 实体主体的位置范围 Content-Type 实体主体的媒体类型 Expires 实体主体过期的日期时间 Last-Modified 资源的最后修改日期时间 不仅限于RFC2616定义的47种首部字段，还有Cookie、Set-Cookie和Content-Disposition等在其他RFC中定义的首部字段，使用频率也很高。 TCP&amp;UDP UDP TCP 不要需要先建立连接 需要建立连接 尽最大努力交付，不可靠 可靠 面向报文 面向字节流 没有拥塞控制 有拥塞控制 支持一对多 只支持一对一 首部开销小 首部开销大 TCP可靠传输的实现：累积确认，超时重传 TCP拥塞控制：慢开始（指数），拥塞避免（线性），快重传，快恢复 如果超时：从1开始，门限值设为拥塞窗口的一半 如果三次ACK：门限值设为拥塞窗口一半，从拥塞窗口开始（快重传，快恢复） 三次握手&amp;四次挥手 客户端主动发送连接请求，SYN为同步位，初始seq自选，SYN报文段不能携带数据但要消耗一个序号 服务端，接收连接请求，初始seq自选 客户端确认，ACK报文可以携带数据，但如果不携带则不消耗序号 （最后确认是为了防止，第一次建立请求连接阻塞，第二次请求建立连接，传送完数据，第一次请求延时到达后，服务器又建立连接，但是客户端并没有数据要传送，浪费计算机资源） 客户端发送FIN终止传送，FIN不携带数据消耗一个序号 服务器同意断开连接，此时TCP连接处于半关闭状态，客户端到服务器方向的连接关闭了。客户端没有数据要发送了，但服务端仍可能有数据需要客户端接收 服务端没有数据要发送了，要断开连接，w表示又发送了一些数据， 客户端收到断开请求返回ACK （等待2MSL是为了让最后的ACK报文能够到达服务器，如果立刻关闭连接ACK丢失那么服务器不能正常关闭连接）]]></content>
      <categories>
        <category>技术总结</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[内存泄露]]></title>
    <url>%2F2018%2F03%2F09%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%2F%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93%2F%E5%86%85%E5%AD%98%E6%B3%84%E9%9C%B2%2F</url>
    <content type="text"><![CDATA[内存泄露就是指一个不再被程序使用的对象或变量一直被占据在内存中。Java 使用有向图的方式进行垃圾回收管理，可以消除引用循环的问题，例如有两个对象，相互引用，只要它们和根进程不可达的，那么GC也是可以回收它们的 1.java中内存泄露的发生场景，通俗地说，就是程序员可能创建了一个对象，以后一直不再使用这个对象，这个对象却一直被引用，即这个对象无用但是却无法被垃圾回收器回收的，这就是java中的内存泄露，一定要让程序将各种分支情况都完整执行到程序结束，然后看某个对象是否被使用过，如果没有，则才能判定这个对象属于内存泄露。 2.如果一个外部类的实例对象的方法返回了一个内部类的实例对象，这个内部类对象被长期引用了，即使那个外部类实例对象不再被使用，但由于内部类持久外部类的实例对象，这个外部类对象将不会被垃圾回收，这也会造成内存泄露。 3.当一个对象被存储进HashSet集合中以后，就不能修改这个对象中的那些参与计算哈希值的字段了，否则，对象修改后的哈希值与最初存储进HashSet集合中时的哈希值就不同了，在这种情况下，即使在contains方法使用该对象的当前引用作为的参数去HashSet集合中检索对象，也将返回找不到对象的结果，这也会导致无法从HashSet集合中单独删除当前对象，造成内存泄露。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>内存泄露</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中的IO]]></title>
    <url>%2F2018%2F02%2F11%2Fjava%2FJava%E4%B8%AD%E7%9A%84IO%2F</url>
    <content type="text"><![CDATA[Java NIO（New IO）是从Java 1.4版本开始引入的一个新的IO API，可以替代标准的Java IO API。NIO与原来的IO有同样的作用和目的，但是使用的方式完全不同，NIO支持面向缓冲区的、基于通道的IO操作。NIO将以更加高效的方式进行文件的读写操作。本文先介绍一些基本概念再讲Java中涉及IO的内容 同步，异步，阻塞 ，非阻塞首先我们搞清这几个概念，不要两两搭配，可能有的读者第一次接触的时候会想当然的认为，同步阻塞，同步非阻塞，异步阻塞，异步非阻塞，异步就是异步没有阻塞非阻塞之分只有同步中才会有。 阻塞：做某件事情，直到完成，除非超时，如果没有完成，继续等待 传统的 IO 流都是阻塞式的。也就是说，当一个线程调用 read() 或 write()时，该线程被阻塞，直到有一些数据被读取或写入，该线程在此期间不能执行其他任务。因此，在完成网络通信进行 IO 操作时，由于线程会阻塞，所以服务器端必须为每个客户端都提供一个独立的线程进行处理，当服务器端需要处理大量客户端时，性能急剧下降。 非阻塞：做一件事情，尝试着做，如果说不能做完，就不做了，意思就是直接返回，如果能够做完，就做。 Linux中说的非阻塞I/O和Java的NIO1.0中的非阻塞I/O不是相同的概念，从根本上来说，阻塞就是进程“被”休息，CPU处理其他进程去了。非阻塞可以理解成将大的整片时间的阻塞分成多个小的阻塞，所以进程不断地有机会被CPU处理，理论上可以做点其他的事。看上去非阻塞比阻塞好，但CPU会很大几率因socket没数据而空转。虽然当前线程可以一直运行，但是从整个机器效率来看，浪费更大。 Java NIO 是非阻塞模式的。当线程从某通道进行读写数据时，若没有数据可用时，该线程可以进行其他任务。线程通常将非阻塞 IO 的空闲时间用于在其他通道上执行 IO 操作，所以单独的线程可以管理多个输入和输出通道。因此，NIO 可以让服务器端使用一个或有限几个线程来同时处理连接到服务器端的所有客户端。其中的Selector.select()函数还是阻塞的，所以不会有所谓的CPU浪费，Java NIO与其说是非阻塞I/O还不如说是多路复用I/O。 同步：执行一个操作后，等待结果。 异步：执行一个操作后，可以去执行其他操作，然后等待通知再回来执行刚才没执行完的操作。 异步和非阻塞的区别就在于，异步只需要等待通知就可以了，非阻塞需要时不时查看任务是否完成（轮询）。因此真正的异步操作需要CPU的深度参与，用户线程不用去考虑，只需要等待完成信号就可以。 BIO NIO AIO这几个名词也是比较容易让人犯晕的 BIO（Block IO）：同步阻塞IO,传统socket编程。 NIO（Non-Blocking IO）：一般只针对Java，基于Reactor模型。下文具体介绍 AIO（Asynchronous IO）：一般只针对Java。 select poll epollI/O多路复用的本质就是用select/poll/epoll，去监听多个socket对象，如果其中的socket对象有变化，只要有变化，用户进程就知道了。 select是不断轮询去监听的socket，socket个数有限制，一般为1024个； poll还是采用轮询方式监听，只不过没有个数限制； epoll并不是采用轮询方式去监听了，而是当socket有变化时通过回调的方式主动告知用户进程。 epoll有EPOLLLT和EPOLLET两种触发模式，LT(lever-trigger)是默认的模式，ET(edge-trigger)是“高速”模式。LT(水平触发)模式下，只要这个fd(file description)还有数据可读，每次 epoll_wait都会返回它的事件，提醒用户程序去操作，而在ET（边缘触发）模式中，它只会提示一次，直到下次再有数据流入之前都不会再提示了，无论fd中是否还有数据可读。所以在ET模式下，read一个fd的时候一定要把它的buffer读光，也就是说一直读到read的返回值小于请求值，或者遇到EAGAIN错误。还有一个特点是，epoll使用“事件”的就绪通知方式，通过epoll_ctl注册fd，一旦该fd就绪，内核就会采用类似callback的回调机制来激活该fd，epoll_wait便可以收到通知。 通道和缓冲区Java NIO系统的核心在于：通道(Channel)和缓冲区(Buffer)。 通道表示打开到 IO 设备(例如：文件、套接字)的连接。若需要使用 NIO 系统，需要获取用于连接 IO 设备的通道以及用于容纳数据的缓冲区。然后操作缓冲区，对数据进行处理。 简而言之，Channel 负责传输， Buffer 负责存储 缓冲区 一个用于特定基本数据类型的容器。由 java.nio 包定义的，所有缓冲区都是 Buffer 抽象类的子类。 Java NIO 中的 Buffer 主要用于与 NIO 通道进行交互，数据是从通道读入缓冲区，从缓冲区写入通道中的。 缓冲区的几个重要概念： 容量 (capacity) ：表示 Buffer 最大数据容量，缓冲区容量不能为负，并且创建后不能更改。 限制 (limit)：第一个不应该读取或写入的数据的索引，即位于 limit 后的数据不可读写。缓冲区的限制不能为负，并且不能大于其容量。 位置 (position)：下一个要读取或写入的数据的索引。缓冲区的位置不能为负，并且不能大于其限制 标记 (mark)与重置 (reset)：标记是一个索引，通过 Buffer 中的 mark() 方法指定 Buffer 中一个特定的 position，之后可以通过调用 reset() 方法恢复到这个 position. 缓冲区的数据操作： Buffer 所有子类提供了两个用于数据操作的方法：get()与 put() 方法 获取 Buffer 中的数据 get() ：读取单个字节get(byte[] dst)：批量读取多个字节到 dst 中get(int index)：读取指定索引位置的字节(不会移动 position) 放入数据到 Buffer 中 put(byte b)：将给定单个字节写入缓冲区的当前位置put(byte[] src)：将 src 中的字节写入缓冲区的当前位置put(int index, byte b)：将指定字节写入缓冲区的索引位置(不会移动 position) 通道 通道（Channel）：由 java.nio.channels 包定义的。Channel 表示 IO 源与目标打开的连接。Channel 类似于传统的“流”。只不过 Channel本身不能直接访问数据，Channel 只能与Buffer 进行交互。 直接与非直接缓冲区非直接缓冲区：通过allocate()方法分配缓冲区，将缓冲区建立在JVM的内存中 直接缓冲区：通过allocateDirect()方法分配直接缓冲区。将缓冲区建立在物理内存中。可以提高效率 Java NIO与IO的主要区别IO面向流(Stream Oriented)阻塞IO(Blocking IO) NIO面向缓冲区(Buffer Oriented) 非阻塞IO(Non Blocking IO) Java NIO提供了与标准IO不同的IO工作方式： Channels and Buffers（通道和缓冲区）：标准的IO基于字节流和字符流进行操作的，而NIO是基于通道（Channel）和缓冲区（Buffer）进行操作，数据总是从通道读取到缓冲区中，或者从缓冲区写入到通道中。 Asynchronous IO（异步IO）：Java NIO可以让你异步的使用IO，例如：当线程从通道读取数据到缓冲区时，线程还是可以进行其他事情。当数据被写入到缓冲区时，线程可以继续处理它。从缓冲区写入通道也类似。 Selectors（选择器）：Java NIO引入了选择器的概念，选择器用于监听多个通道的事件（比如：连接打开，数据到达）。因此，单个的线程可以监听多个数据通道。 传统的IO模型，它存在哪些阻塞点传统IO Socket编程 12345678910111213141516171819202122232425262728293031323334public class TraditionalSocketDemo &#123; public static void main(String... args) throws IOException &#123; ExecutorService threadPool= Executors.newCachedThreadPool(); ServerSocket serverSocket=new ServerSocket(7777); System.out.println("服务端启动"); while (true)&#123; //获取socket客户端套接字 Socket socket=serverSocket.accept(); threadPool.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; System.out.println("有新的客户端连接上来了..."); InputStream is=socket.getInputStream(); byte[] b=new byte[1024]; while (true)&#123; int data=is.read(b); if(data!=-1)&#123; String info=new String(b,0,data,"GBK"); System.out.println(info); &#125;else&#123; break; &#125; &#125; &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; &#125;&#125; 阻塞点 1 Socket socket=serverSocket.accept(); 2int data=is.read(b); 使用线程池可以让多个客户端连接上来 缺点：高并发的情况下占用资源，一个线程要开辟一个栈内存 优点：一个线程为一个用户服务，一对一服务，服务质量好 NIO模型增加了一个重要角色（Selector），主要负责调度和监控客户端和服务端（调度器） 由阻塞方式改成了非阻塞（non-blocking）,Thread+Selector，因此只需要一个线程，不再是一对一的情况，当有需求服务的客户去服务，不要服务时选择为其他客户服务 Java NIO中的SocketChannel是一个连接到TCP网络套接字的通道 Java NIO中的DatagramChannel是一个能收发UDP包的通道。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class NIOSocketDemo &#123; private Selector selector;//通道选择器（管理器） public static void main(String... args) throws IOException &#123; NIOSocketDemo nioSocketDemo=new NIOSocketDemo(); nioSocketDemo.initServer(8888); nioSocketDemo.listenSelector(); &#125; public void initServer (int port) throws IOException &#123; ServerSocketChannel serverChannel=ServerSocketChannel.open(); serverChannel.configureBlocking(false);//设置成非阻塞 serverChannel.socket().bind(new InetSocketAddress(port)); this.selector=Selector.open(); serverChannel.register(selector, SelectionKey.OP_ACCEPT);//连接事件 System.out.println("服务已启动..."); &#125; public void listenSelector() throws IOException &#123; //轮询监听Selector while (true)&#123; //等待客户连接 //select模型，多路复用 this.selector.select(); System.out.println("新客户端连接上来了..."); Iterator&lt;SelectionKey&gt; iteKey = this.selector.selectedKeys().iterator(); while (iteKey.hasNext())&#123; SelectionKey key = iteKey.next(); iteKey.remove(); //处理请求 handler(key); &#125; &#125; &#125;//处理客户端请求 private void handler(SelectionKey key) throws IOException &#123; if(key.isAcceptable())&#123; //处理客户端连接请求时间 ServerSocketChannel serverChannel= (ServerSocketChannel) key.channel(); SocketChannel socketChannel = serverChannel.accept(); serverChannel.configureBlocking(false); //接受客户端发送的信息时，需要给通道设置读的权限 socketChannel.register(selector,SelectionKey.OP_READ); &#125;else if(key.isReadable())&#123; //处理读事件 SocketChannel socketChannel= (SocketChannel) key.channel(); ByteBuffer buffer=ByteBuffer.allocate(1024); socketChannel.read(buffer); int readData=socketChannel.read(buffer); if(readData&gt;0)&#123; String info =new String(buffer.array(),"GBK").trim(); System.out.println("服务端收到数据："+info); &#125;else &#123; System.out.println("客户端关闭了"); key.cancel(); &#125; &#125; &#125;&#125; 阻塞点：this.selector.select(); 为什么非阻塞的NIO会有阻塞点呢？我们真正关心的阻塞点是读取数据，读取数据时是没有阻塞的。selector有自带的方法可以设置超时时间，或者wait唤醒，可以根据需要调整阻塞情况。其实Selector看似是一个线程在工作，其实内部也是有多个线程的，不过线程是有限的。线程数小于连接数量。 NIO非阻塞技术实现的原理是reactor模式，这个模式不仅在NIO出现过，而且在redis等其他地方也出现过，经常在多线程高并发的情况下使用。 http://blog.csdn.net/u010168160/article/details/53019039 reactor模式 NIO.2(AIO)JDK 7 Java对NIO进行了极大的扩展，增强了对文件处理和文件系统特性的支持，以至于我们称他们为 NIO.2。因为 NIO 提供的一些功能，NIO已经成为文件处理中越来越重要的部分。 Path 与 Paths: java.nio.file.Path 接口代表一个平台无关的平台路径，描述了目录结构中文件的位置。 Files 类 java.nio.file.Files 用于操作文件或目录的工具类。 Java 7 增加了一个新特性，该特性提供了另外一种管理资源的方式，这种方式能自动关闭文件。这个特性有时被称为自动资源管理(Automatic Resource Management, ARM)， 该特性以 try 语句的扩展版为基础。自动资源管理主要用于，当不再需要文件（或其他资源）时，可以防止无意中忘记释放它们。 自动资源管理基于 try 语句的扩展形式： 123456789try(需要关闭的资源声明)&#123;//可能发生异常的语句&#125;catch(异常类型 变量名)&#123;//异常的处理语句&#125;……finally&#123;//一定执行的语句&#125; 当 try 代码块结束时，自动释放资源。因此不需要显示的调用 close() 方法。该形式也称为“带资源的 try 语句”。注意：①try 语句中声明的资源被隐式声明为 final ，资源的作用局限于带资源的 try 语句②可以在一条 try 语句中管理多个资源，每个资源以“;” 隔开即可。③需要关闭的资源，必须实现了 AutoCloseable 接口或其自接口 Closeable NIO与AIOJava NIO ： 同步非阻塞，服务器实现模式为一个请求一个线程，即客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接有I/O请求时才启动一个线程进行处理。Java AIO(NIO.2) ： 异步非阻塞，服务器实现模式为一个有效请求一个线程，客户端的I/O请求都是由OS先完成了再通知服务器应用去启动线程进行处理， NIO方式适用于连接数目多且连接比较短（轻操作）的架构，比如聊天服务器，并发局限于应用中，编程比较复杂，JDK1.4开始支持。AIO方式使用于连接数目多且连接比较长（重操作）的架构，比如相册服务器，充分调用OS参与并发操作，编程比较复杂，JDK7开始支持 I/O属于底层操作，需要操作系统支持，并发也需要操作系统的支持，所以性能方面不同操作系统差异会比较明显。另外NIO的非阻塞，需要一直轮询，也是一个比较耗资源的。所以出现AIO 相关阅读5种IO模型 https://blog.csdn.net/tjiyu/article/details/52959418 深入理解非阻塞同步IO和非阻塞异步IO https://blog.csdn.net/iter_zc/article/details/39291647 IO多路复用机制http://www.cnblogs.com/Anker/p/3265058.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>IO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[session集群解决方案]]></title>
    <url>%2F2018%2F02%2F10%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%2F%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93%2Fsession%E9%9B%86%E7%BE%A4%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[随着需求和并发量的增大，现有架构已经不能满足我们的需求，因此要对应用进行拆分，分成很多模块。这时就会遇到一个SSO登陆问题。 大家都知道http短链接超文本传输协议，http协议支持会话，但是本身是无状态。所以区分是否登录就是用会话机制session来建立（基于cookie）。Session是存在服务器当中的，随着互联网技术的发展用户量的增大，单台服务器已经不能存储这么多会话。因此采用集群的方式来实现。有以下四种实现方式 Session Sticky通过对访问的ip进行hash通过nginx将用户的访问分发到不同的服务器上。 问题：如果某台服务器宕机，那么这台机器保留的session会话数据都会丢失，不符合高可用性 Session Relicationsession复制解决方案，一台服务器收到会话数据后，会通知到其他服务器 优点：实现简单tomcat有集成的插件 缺点：增加了内部网络的开销，每台服务器都要保留全局的session数据，内存占用严重 cookie based基于cookie，因为cookie数据是存在客户端的，也就是说将session数据存在客户端本地（当然是经过加密之后的），但是安全性差，尽管经过加密，但是还能用本地的数据进行一些例如重放攻击，一般电商是不会这么做的，而且每次http请求都要带上参数，增加了带宽的消耗 Session集群存储Redis现在互联网用得最多的方案 优点：没有单点故障、扩展性强、适用于大的数据访问 缺点：增加了网络消耗、需要增加插件、维护成本]]></content>
      <categories>
        <category>Session</category>
      </categories>
      <tags>
        <tag>Session</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java常用原码分析]]></title>
    <url>%2F2018%2F02%2F09%2Fjava%2FJava%E5%B8%B8%E7%94%A8%E5%8E%9F%E7%A0%81%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[本文总结面试中常会问到的java原生代码分析，包括HashMap，ArrayList，String等，以及不同JDK版本的变化 String StringBuffer StringBuilder这三个类的区别主要分别在字符串的拼接上 源码分析123456789public class StringTest &#123;@Testpublic void testStringPlus() &#123; String str = "111"; str += "222"; str += "333"; System.out.println(str);&#125;&#125; 这样的代码在开发中经常使用，不过编译器每次碰到”+”的时候，会new一个StringBuilder出来，接着调用append方法，在调用toString方法，生成新字符串 ，这种方式是对内存的浪费效率不好。 在String中自带了一个连接的方法源码如下： 12345678910111213141516171819public String concat(String str) &#123; int olen = str.length(); if (olen == 0) &#123; return this; &#125; if (coder() == str.coder()) &#123; byte[] val = this.value; byte[] oval = str.value; int len = val.length + oval.length; byte[] buf = Arrays.copyOf(val, len); System.arraycopy(oval, 0, buf, val.length, oval.length); return new String(buf, coder); &#125; int len = length(); byte[] buf = StringUTF16.newBytesFor(len + olen); getBytes(buf, 0, UTF16); str.getBytes(buf, len, UTF16); return new String(buf, UTF16);&#125; 意思就是通过两次字符串的拷贝，产生一个新的字符数组buf[]，再根据字符数组buf[]，new一个新的String对象出来，这意味着concat方法调用N次，将发生N*2次数组拷贝以及new出N个String对象，无论对于时间还是空间都是一种浪费。 不论是“+”还是concat方法拼接字符串都是比较低效的这时我们需要StringBuilder和StringBuffer来拼接字符串 123456789101112public class TestMain&#123; public static void main(String[] args) &#123; StringBuilder sb = new StringBuilder("111"); sb.append("222"); sb.append("111"); sb.append("111"); sb.append("444"); System.out.println(sb.toString()); &#125;&#125; StringBuffer和StringBuilder原理一样，无非是在底层维护了一个char数组，每次append的时候就往char数组里面放字符而已，在最终sb.toString()的时候，用一个new String()方法把char数组里面的内容都转成String，这样，整个过程中只产生了一个StringBuilder对象与一个String对象，非常节省空间。StringBuilder唯一的性能损耗点在于char数组不够的时候需要进行扩容，扩容需要进行数组拷贝，一定程度上降低了效率。 StringBuffer和StringBuilder用法一模一样，唯一的区别只是StringBuffer是线程安全的，它对所有方法都做了同步，StringBuilder是线程非安全的，所以在不涉及线程安全的场景，比如方法内部，尽量使用StringBuilder，避免同步带来的消耗。 另外，StringBuffer和StringBuilder还有一个优化点，上面说了，扩容的时候有性能上的损耗，那么如果可以估计到要拼接的字符串的长度的话，尽量利用构造函数指定他们的长度。 那么不能用String的+了吗？ 1234public static void main(String[] args)&#123; String str = "111" + "222" + "333" + "444"; System.out.println(str);&#125; 就这种连续+的情况，实际上编译的时候JVM会只产生一个StringBuilder并连续append等号后面的字符串。但实际上大量的“+”会导致代码的可读性非常差。待拼接的内容可能从各种地方获取，比如调用接口、从.properties文件中、从.xml文件中，这样的场景下尽管用多个“+”的方式也不是不可以，但会让代码维护性不太好 Java1.9后的变化从Java1.9之后，对于String类进行了方法的扩充，便于更加方便对于字符串进行各种操作，其中有一个很有意思的地方就是新增了COMPACT_STRINGS常量和LATIN1常量。 我们知道java支持Unicode编码，最新的java11已经支持Unicode10支持的越多也就说我们需要用很长的编码来表示字符，这样带来一个坏处就是字符串占用空间会变得很长。有时候我们更多地只需要使用英文与阿拉伯数字字符，那么ASCII码就能满足我们的要求，因此Java9之后String默认开启了对字符串的压缩，使用Latin1编码（扩展ASCII，8位表示一个字符），而Latin1是使用@Native注解修饰的，而且COMPACT_STRINGS是在static代码块中初始化的，也就是说当String类初始化的时候执行了static的代码块就开启了对字符串的压缩。 StringBuffer和StringBuilder都继承自AbstractStringBuilder，在其import里有如下 123import static java.lang.String.COMPACT_STRINGS;import static java.lang.String.UTF16;import static java.lang.String.LATIN1; 因此也都开启了压缩 HashMapHash散列将一个任意长度通过哈希算法转换成一个固定值 java中用的是移位的哈希算法 通过哈希出来的值然后通过这个值定位到这个map，然后value存储到这个map中 如果位置i上没有元素，则直接添加成功。 如果位置i上已经存在元素,则需要通过循环的方法，往链表中插入元素 源码分析 负载因子值的大小，对HashMap有什么影响 默认情况下，如果添加元素的长度 &gt;= DEFAULT_INITIAL_CAPACITY *DEFAULT_LOAD_FACTOR (临界值threshold默认值为12)且新要添加的数组位置不为null的情况下，就进行扩容。默认扩容为原有长度的2倍。将原有的数据复制到新的数组中。 负载因子的大小决定了HashMap的数据密度，负载因子越大密度越大，发生碰撞的几率越高，数组中的链表越容易长， 造成查询或插入时的比较次数增多，性能会下降。 负载因子越小，就越容易触发扩容，数据密度也越小，意味着发生碰撞的几率越小，数组中的链表也就越短，查询和插入时比较的次数也越小，性能会更高。但是会浪费一定的内容空间。而且经常扩容也会影响性能，建议初始化预设大一点的空间。 HashMap底层创建 123HashMap map = new HashMap()//底层创建了长度为16的Entry数组 //JDK1.8之后默认情况下，先不创建长度为16的数组。map.put()//JDK1.8开始创建数据 HashMap什么时候扩容？ put的时候，高于或等于0.75的时候，扩位原来的2倍， 初始大小为16 123static final float DEFAULT_LOAD_FACTOR = 0.75f;//在容量的3/4的时候进行扩容static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; HashMap的key可以为空，空值hash为0（JDK1.8之后） 1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; Hash key如果重复不会被覆盖掉 1234567891011121314151617181920212223242526272829303132333435363738394041final V putVal(int hash, K key, V value, boolean onlyIfAbsent,boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; java1.8之后HashMap的底层 JDK1.7中 使用一个Entry数组来存储数据，用key的hashcode取模来决定key会被放到数组里的位置，如果hashcode相同，或者hashcode取模后的结果相同（hash collision），那么这些key会被定位到Entry数组的同一个格子里，这些key会形成一个链表。 在hashcode特别差的情况下，比方说所有key的hashcode都相同，这个链表可能会很长，那么put/get操作都可能需要遍历这个链表 也就是说时间复杂度在最差情况下会退化到O(n) JDK1.8中 使用一个Node数组来存储数据，但这个Node可能是链表结构，也可能是红黑树结构 如果插入的key的hashcode相同，那么这些key也会被定位到Node数组的同一个格子里。 如果同一个格子里的key不超过8个，使用链表结构存储。 如果达到7(TREEIFY_THRESHOLD(常量默认值8) - 1)个，那么会调用treeifyBin函数，将链表转换为红黑树。 那么即使hashcode完全相同，由于红黑树的特点，查找某个特定元素，也只需要O(log n)的开销 也就是说put/get的操作的时间复杂度最差只有O(log n) 两个版本都是使用table来存储数据，JDK1.7中使用的是数组+链表的数据结构，JDK1.8之后使用的是数组+链表+红黑树来存储。听起来挺不错，但是真正想要利用JDK1.8的好处，有一个限制：key的对象，必须正确的实现了Compare接口如果没有实现Compare接口，或者实现得不正确（比方说所有Compare方法都返回0）那JDK1.8的HashMap其实还是慢于JDK1.7的。 为什么要这么操作呢？ 应该是为了避免Hash Collision DoS攻击，Java中String的hashcode函数的强度很弱，有心人可以很容易的构造出大量hashcode相同的String对象。如果向服务器一次提交数万个hashcode相同的字符串参数，那么可以很容易的卡死JDK1.7版本的服务器。但是String正确的实现了Compare接口，因此在JDK1.8版本的服务器上，Hash Collision DoS不会造成不可承受的开销。 HashMap的不足之处每当HashMap扩容的时候需要重新Hash然后放入新的entry table里面影响效率，最好使用时先指定他们的扩容大小，防止put的时候再次进行扩容 多线程put的时候可能会形成循环链,这个问题会当调用get的时候显示出来问题， 第一，如果多个线程同时使用put方法添加元素 假设正好存在两个put的key发生了碰撞(hash值一样)，那么根据HashMap的实现，这两个key会添加到数组的同一个位置，这样最终就会发生其中一个线程的put的数据被覆盖。 第二，如果多个线程同时检测到元素个数超过数组门限值之后 这样会发生多个线程同时对hash数组进行扩容，都在重新计算元素位置以及复制数据，但是最终只有一个线程扩容后的数组会赋给table，也就是说其他线程的都会丢失，并且各自线程put的数据也丢失。且会引起死循环的错误。 第三、多线程put非null元素后，get操作得到null值 关于死循环的问题，博主搜了好多资料，但都是基于JDK1.7讲解得为什么没有JDK1.8的版本呢，因为内部实现修复了这个问题。1.7链表新节点采用的是头插法，这样在线程一扩容迁移元素时，会将元素顺序改变，导致两个线程中出现元素的相互指向而形成循环链表，1.8采用了尾插法，从根源上杜绝了这种情况的发生。当然HashMap仍然不是线程安全的。请使用ConcurrentHashMap或者HashTable，曾经有面试官变态问如果就想在多线程使用HashMap怎么办，我能想到的只有把HashMap用Synchronized同步起来，面试官提到一个方法是Collections类里面定义了对常见的集合类的同步化的操作。 相关面试总结https://www.cnblogs.com/lchzls/p/6714474.html HashTable和ConcurrentHashMapHashtable和ConcurrentHashMap都可以用于多线程的环境，不可以存储null的key和value。 但是当Hashtable的大小增加到一定的时候，性能会急剧下降，因为迭代时需要被锁定很长的时间，因为ConcurrentHashMap引入了分割(segmentation)，不论它变得多么大，仅仅需要锁定map的某个部分，而其它的线程不需要等到迭代完成才能访问map。简而言之，在迭代的过程中，ConcurrentHashMap仅仅锁定map的某个部分，而Hashtable则会锁定整个map。 ArrayList、LinkedList、Vector区别？ArrayList:线程不安全的，效率高；底层使用数组实现(Collections中定义了synchronizedList(Listlist)将此ArrayList转化为线程安全的) LinkedList：对于频繁的插入、删除操作，我们建议使用此类，因为效率高；内存消耗较ArrayList大；底层使用双向链表实现 Vector：线程安全的，效率低；底层使用数组实现 ArrayList 源码分析：jdk7： 123456789101112131415ArrayList list= new ArrayList();//初始化一个长度为10的Object[] elementDatasysout(list.size());//返回存储的元素的个数:0list.add(123);list.add(345);...当添加第11个元素时，需要扩容，默认扩容为原来的1.5倍。还需要将原有数组中的数据复制到新的数组中。删除操作：如果删除某一个数组位置的元素，需要其后面的元素依次前移。remove(Object obj) /remove(intindex) jdk8: 1234567891011ArrayList list= new ArrayList();//初始化一个长度为0的Object[] elementDatasysout(list.size());//返回存储的元素的个数:0list.add(123);//此时才创建一个长度为10的Object[] elementDatalist.add(345);...当添加第11个元素时，需要扩容，默认扩容为原来的1.5倍。还需要将原有数组中的数据复制到新的数组中。 开发时的启示： 建议使用：ArrayList list= new ArrayList(intlength); jdk8延迟了底层数组的创建：内存的使用率；对象的创建更快]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>原码</tag>
        <tag>HashMap</tag>
        <tag>String</tag>
        <tag>StringBuffer</tag>
        <tag>StringBuilder</tag>
        <tag>ArrayList</tag>
        <tag>Vector</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring]]></title>
    <url>%2F2018%2F01%2F05%2Fjava%2FSpring%2F</url>
    <content type="text"><![CDATA[学Java没有不知道Spring的。简单来说Spring 是一个开源框架，是一个IOC (DI)和AOP容器框架，为了简化企业级应用开发而生。从一开始的Spring到Spring MVC到现在的SpringBoot和SpringCloud，Spring社区越来与庞大，项目的封装越来越便于使用。但不管怎么封装Spring的核心思想是不会变的。 Spring家族产品虽然Spring家族产品众多，但都是基于最原始Spring特性IOC(DI)和AOP发展起来的 Spring首先有一点就是Spring不要和WEB联系起来，虽然他更多用于解决web问题，但本身是用于解决java开发中问题，而不仅仅局限于web问题。 Spring的框架如下 Spring特性有以下特性 方便解耦，简化开发 通过Spring提供的IoC容器，我们可以将对象之间的依赖关系交由Spring进行控制，避免硬编码所造成的过度程序耦合。 AOP编程的支持 通过Spring提供的AOP功能，方便进行面向切面的编程。 声明事物的支持 在Spring中，我们可以从单调烦闷的事务管理代码中解脱出来，通过声明式方式灵活地进行事务的管理，提高开发效率和质量。 方便程序的测试 可以用非容器依赖的编程方式进行几乎所有的测试工作。例如：Spring对Junit4支持，可以通过注解方便的测试Spring程序。 方便集成各种优秀框架 Spring不排斥各种优秀的开源框架，相反，Spring可以降低各种框架的使用难度，Spring提供了对各种优秀框架（如Struts,Hibernate、Hessian、Quartz）等的直接支持。 降低Java EE API的使用难度 Spring对很多难用的Java EE API（如JDBC，JavaMail，远程调用等）提供了一个薄薄的封装层，通过Spring的简易封装，这些Java EE API的使用难度大为降低。 SpringMVCSpring MVC是一个基于Servlet API的框架，它使用同步阻塞IO架构，一个请求一个线程的模式。主要是用于解决Spring开发web应用的问题 SpringBoot用过SpringMVC开发的都知道，光配置就让初学者头疼，尤其是不了解IOC和AOP特性的情况下只能照葫芦画瓢也不懂什么意思去配置一大堆文件。 因此SpringBoot出现了，SpringBoot使用默认大于配置的方式，而且现在都用maven配置项目，避免的到处去下Jar包，让初学者很简单就能开启一个HelloWorld服务器。 SpringBoot的初衷是为了更注重微服务后台开发，不开发前端视图，其前端推荐使用Thymeleaf。SpringBoot会将项目打包成jar包而不是以前MVC时代的WAR包需要放在单独的tomcat服务器运行，jar包作为单文件直接可以运行，其内部已经整合tomcat基本组件。 这里推荐一个博客去学习SpringBoot http://www.ityouknow.com/ SpringCloudSpringCloud也是注重为服务开发，因此可以理解成和SpringBoot一样，不过SpringCloud更关注全局微服务整合和管理，可以理解成管理多个SpringBoot搭建的单体微服务的集合。 Spring WebFluxSpring WebFlux是Spring Framework 5.0中引入的新的反应式Web框架。与Spring MVC不同，它不需要Servlet API，完全异步且无阻塞，并通过Reactor项目实现Reactive Streams规范。 Spring5必须使用java8，那么函数式编程就是java8重要的特点之一，而WebFlux支持函数式编程来定义路由端点处理请求。 Spring MVC和Spring WebFlux都可以用注解式编程模型，都可以运行在tomcat，jetty，undertow等servlet容器当中。但是SpringMVC采用命令式编程方式，代码一句一句的执行，这样更有利于理解与调试，而WebFlux则是基于异步响应式编程，对于初次接触的码农们来说会不习惯。对于这两种框架官方给出的建议是： 如果原先使用用SpringMVC好好的话，则没必要迁移。因为命令式编程是编写、理解和调试代码的最简单方法。因为老项目的类库与代码都是基于阻塞式的。 如果你的团队打算使用非阻塞式web框架，WebFlux确实是一个可考虑的技术路线，而且它支持类似于SpringMVC的Annotation的方式实现编程模式，也可以在微服务架构中让WebMvc与WebFlux共用Controller，切换使用的成本相当小 在SpringMVC项目里如果需要调用远程服务的话，你不妨考虑一下使用WebClient，而且方法的返回值可以考虑使用Reactive Type类型的，当每个调用的延迟时间越长，或者调用之间的相互依赖程度越高，其好处就越大 Spring核心思想IOC(DI)和AOP是Spring至今仍然火热的最核心的设计思想 IOC(DI)IOC全称Inversion of Control直译”控制反转”，这是一种设计思想。 既然是控制反转，那么没反转之前是什么样的？我们为什么要反转？ 在传统的Java程序中我们习惯直接使用new关键字创建对象，是程序去主动创建对象。而IOC中是有一个专门的容器去创建这些对象，也就是说IOC容器控制了对象。IOC控制了外部资源的获取，不仅仅是对象还有文件等。大家一开始配置Spring配置文件的时候都有写过那个巨长的xml文件，其中有一个属性是bean，我们只需要指定需要自动获取的类，以及这个类所需要参数的获取方式。Ioc容器就可以自动装配好这个类供我们使用，而不是我们使用new去主动创建。比如我需要一个汽车，我只需要告诉他汽车需要四个轮子，一个发动机，一个方向盘，那么他就可以自己组装好一个汽车。所有对象的创建，销毁都由Spring来控制。 DI（Dependency Injection）依赖注入就很好理解了，我们把轮胎，发动机，方向盘，这些组件，注入到汽车这个对象里面就可以完成一辆汽车的生产。我们并不用关心对象来自何处，由谁实现，有对象拿来用即可。 IoC和DI说的其实是同一种东西，不过是不同角度去描述，下面推荐两个博客可以更加深理解。 http://blog.csdn.net/qq_22654611/article/details/52606960 这篇博客介绍的简单易懂 https://www.imooc.com/learn/901 虽然这个课程是介绍Google Juice的但是里面很巧妙的介绍了依赖注入 IOC注入方式 构造函数注入(Spring支持) 通过构造方法注入Bean 的属性值或依赖的对象，它保证了Bean实例在实例化后就可以使用。 构造器注入在 &lt;constructor-arg&gt;元素里声明属性,&lt;constructor-arg&gt;中没有 name属性 属性注入(Spring支持) 属性注入即通过 setter方法注入Bean的属性值或依赖的对象,是实际应用中最常用的注入方式 属性注入使用 &lt;property&gt;元素,使用name属性指定Bean的属性名称，value属性或&lt;value&gt;子节点指定属性值 接口注入 如果采用接口注入一个Bean，那么通过注入的Bean就必须要实现这个接口，也就是如果类中含有一个接口类型比如下面这种情况，那么通过注入的bean必须实现这个接口。不常用也不建议用。 1234567public interface A&#123; &#125;class B&#123; int value; A a;&#125; AOPAOP全称Aspect Oriented Program直译”面向切面编程“, 我们知道，面向对象的特点是继承、多态和封装。而封装就要求将功能分散到不同的对象中去，这在软件设计中往往称为职责分配。实际上也就是说，让不同的类设计不同的方法。这样代码就分散到一个个的类中去了。这样做的好处是降低了代码的复杂程度，使类可重用。 但是人们也发现，在分散代码的同时，也增加了代码的重复性。什么意思呢？比如说，我们在两个类中，可能都需要在每个方法中做日志。按面向对象的设计方法，我们就必须在两个类的方法中都加入日志的内容。也许他们是完全相同的，但就是因为面向对象的设计让类与类之间无法联系，而不能将这些重复的代码统一起来。 也许有人会说，那好办啊，我们可以将这段代码写在一个独立的类独立的方法里，然后再在这两个类中调用。但是，这样一来，这两个类跟我们上面提到的独立的类就有耦合了，它的改变会影响这两个类。那么，有没有什么办法，能让我们在需要的时候，随意地加入代码呢？这种在运行时，动态地将代码切入到类的指定方法、指定位置上的编程思想就是面向切面的编程。具体思想是：定义一个切面，在切面的纵向定义处理方法，处理完成之后，回到横向业务流 AOP强调我们只需要关注业务的主要流程，而对于不重要的流程（比如日志打印），我们不需要每次都写入到业务流程的代码中，而是将打印日志功能单独抽取出来，然后告诉程序哪些业务需要打印日志功能（具体操作就是在函数上面通过注解来标识）。 AOP其实只是OOP的补充而已。OOP从横向上区分出一个个的类来，而AOP则从纵向上向对象中加入特定的代码。有了AOP，OOP变得立体了。从技术上来说，AOP基本上是通过代理机制实现的。 解耦是程序员编码开发过程中一直追求的。AOP也是为了解耦所诞生。AOP 在Spring框架中被作为核心组成部分之一，的确Spring将AOP发挥到很强大的功能。最常见的就是事务控制。 动态代理关于普通静态代理就是设计模式中常讲的代理模式，而普通动态代理虽然实现了隐藏了具体实现，但是当扩展接口方法时所有代理类和委托类都需要实现接口方法，增加了维护的工作量。动态代理是AOP的核心，而动态代理实现的核心就是反射，反射为程序员提供了运行时创建类对象的可能性。我们只需要在代理类用反射获取类的运行状态，传递给Proxy.newProxyInstance()就可以返回这个类的代理类因为返回的是Object对象可以随意转换。 spring aop的应用场景 AOP用来封装横切关注点，具体可以在下面的场景中使用 Authentication 权限 Caching 缓存 Context passing 内容传递 Error handling 错误处理 Lazy loading 懒加载 Debugging 调试 logging, tracing, profiling and monitoring 记录跟踪 优化 校准 Performance optimization 性能优化 Persistence 持久化 Resource pooling 资源池 Synchronization 同步 Transactions 事务 Spring事务关于事务是什么，事务的四大特性ACID，请参见Redis的文章 Spring事务管理接口 PlatFormTransactionManager 事务管理器 Spring为不同的持久化框架提供了不同 PlatFormTransactionManager 接口实现 TransactionDefinition 事务定义信息(隔离、传播、超时、只读) 12345678910111213int PROPAGATION_REQUIRED = 0;//支持当前事务，如果不存在，就新建一个int PROPAGATION_SUPPORTS = 1;//支持当前事务，如果不存在，就不使用事务int PROPAGATION_MANDATORY = 2;//支持当前事务，如果不存在，就抛出异常int PROPAGATION_REQUIRES_NEW = 3;//如果有事务存在，挂起当前事务，创建一个新的事务int PROPAGATION_NOT_SUPPORTED = 4;//以非事务方式运行，如果有事务存在，挂起当前事务int PROPAGATION_NEVER = 5;//以非事务方式运行，如果有事务存在，就抛出异常int PROPAGATION_NESTED = 6;//如果当前事务存在，则嵌套事务执行int ISOLATION_DEFAULT = -1;int ISOLATION_READ_UNCOMMITTED = 1;int ISOLATION_READ_COMMITTED = 2;int ISOLATION_REPEATABLE_READ = 4;int ISOLATION_SERIALIZABLE = 8;int TIMEOUT_DEFAULT = -1; TransactionStatus 事务具体运行状态 对事务抛出异常的注意 不要随意使用事务的传播属性 不要随意对事务中的代码进行try/catch操作 不要随意让事务方法方法抛出异常]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库]]></title>
    <url>%2F2018%2F01%2F05%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[数据库相关问题 Mysql的数据库表锁、行锁、页级锁？ 表级，直接锁定整张表，在你锁定期间，其它进程无法对该表进行写操作。如果你是写锁，则其它进程则读也不允许 行级,，仅对指定的记录进行加锁，这样其它进程还是可以对同一个表中的其它记录进行操作。 页级，表级锁速度快，但冲突多，行级冲突少，但速度慢。所以取了折衷的页级，一次锁定相邻的一组记录。 数据库设计中常讲的三范式是指什么？ 1）第一范式1NF(域的原子性) 如果数据库表中的所有字段值都是不可分解的原子值，就说明该数据库表满足了第一范式 2）第二范式2NF(表中除主键外的字段都完全依赖主键) 第二范式是在第一范式基础上建立的。第二范式有两个重点:(1)表中必须有主键；(2)其他非主属性必须完全依赖主键，不能只依赖主键的一部分（主要针对联合主键而言）。 3）第三范式3NF（表中除主键外的字段都完全直接依赖，不能是传递依赖） 不能是传递依赖，即不能存在：非主键列 A 依赖于非主键列 B，非主键列 B 依赖于主键的情况。第二范式和第三范式区分的关键点：2NF：非主键列是否完全依赖于主键，还是依赖于主键的一部分；3NF：非主键列是直接依赖于主键，还是直接依赖于非主键列。 Mybaits中#和$区别 #{key}：获取参数的值，预编译到SQL中。安全。 ${key}：获取参数的值，拼接到SQL中。有SQL注入问题。ORDER BY ${name} 1）$ \{\} 是Properties文件中的变量占位符，它可以用于标签属性值和sql内部，属于静态文本替换，比如$\{driver\}会被静态替换为com.mysql.jdbc.Driver。 2）#{}是sql的参数占位符，Mybatis会将sql中的#{}替换为?号，在sql执行前会使用PreparedStatement的参数设置方法，按序给sql的?号占位符设置参数值，比如ps.setInt(0, parameterValue)，#{item.name}的取值方式为使用反射从参数对象中获取item对象的name属性值，相当于param.getItem().getName()。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Redis]]></title>
    <url>%2F2018%2F01%2F05%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2FRedis%2F</url>
    <content type="text"><![CDATA[本文从零开始总结Redis技术相关问题，Redis的内容都在这 Nosql概述在介绍Redis之前，首先先要介绍Nosql的概念。 互联网架构发展在90年代的时候，计算机访问量一般不大，单个数据库足以应付，网页更多的是静态网页，动态交互性。于是出现了下面的架构 上述架构在数据存储的瓶颈有以下几点： 1.数据量的总大小 一个机器放不下2.数据的索引（B+ Tree）一个机器的内存放不下3.访问量(读写混合)一个实例不能承受 后来，随着访问量的上升，几乎大部分使用MySQL架构的网站在数据库上都开始出现了性能问题，web程序不再仅仅专注在功能上，同时也在追求性能。程序员们开始大量的使用缓存技术来缓解数据库的压力，优化数据库的结构和索引。开始比较流行的是通过文件缓存来缓解数据库压力，但是当访问量继续增大的时候，多台web机器通过文件缓存不能共享，大量的小文件缓存也带了了比较高的IO压力。在这个时候，Memcached就自然的成为一个非常时尚的技术产品。 Memcached作为一个独立的分布式的缓存服务器，为多个web服务器提供了一个共享的高性能缓存服务，在Memcached服务器上，又发展了根据hash算法来进行多台Memcached缓存服务的扩展，然后又出现了一致性hash来解决增加或减少缓存服务器导致重新hash带来的大量缓存失效的弊端 由于数据库的写入压力增加，Memcached只能缓解数据库的读取压力。读写集中在一个数据库上让数据库不堪重负，大部分网站开始使用主从复制技术来达到读写分离，以提高读写性能和读库的可扩展性。Mysql的master-slave模式成为这个时候的网站标配了。 在Memcached的高速缓存，MySQL的主从复制，读写分离的基础之上，这时MySQL主库的写压力开始出现瓶颈，而数据量的持续猛增，由于MyISAM使用表锁，在高并发下会出现严重的锁问题，大量的高并发MySQL应用开始使用InnoDB引擎代替MyISAM。 同时，开始流行使用分表分库来缓解写压力和数据增长的扩展问题。这个时候，分表分库成了一个热门技术，是面试的热门问题也是业界讨论的热门技术问题。也就在这个时候，MySQL推出了还不太稳定的表分区，这也给技术实力一般的公司带来了希望。虽然MySQL推出了MySQL Cluster集群，但性能也不能很好满足互联网的要求，只是在高可靠性上提供了非常大的保证。 MySQL数据库也经常存储一些大文本字段，导致数据库表非常的大，在做数据库恢复的时候就导致非常的慢，不容易快速恢复数据库。比如1000万4KB大小的文本就接近40GB的大小，如果能把这些数据从MySQL省去，MySQL将变得非常的小。关系数据库很强大，但是它并不能很好的应付所有的应用场景。MySQL的扩展性差（需要复杂的技术来实现），大数据下IO压力大，表结构更改困难，正是当前使用MySQL的开发人员面临的问题。 今天架构是下面这个样子： 为什么使用NoSQL ? 今天我们可以通过第三方平台（如：Google,Facebook等）可以很容易的访问和抓取数据。用户的个人信息，社交网络，地理位置，用户生成的数据和用户操作日志已经成倍的增加。我们如果要对这些用户数据进行挖掘，那SQL数据库已经不适合这些应用了, NoSQL数据库的发展也却能很好的处理这些大的数据。 Nosql是什么NoSQL(NoSQL = Not Only SQL )，意即“不仅仅是SQL”，泛指非关系型的数据库。随着互联网web2.0网站的兴起，传统的关系数据库在应付web2.0网站，特别是超大规模和高并发的SNS类型的web2.0纯动态网站已经显得力不从心，暴露了很多难以克服的问题，而非关系型的数据库则由于其本身的特点得到了非常迅速的发展。NoSQL数据库的产生就是为了解决大规模数据集合多重数据种类带来的挑战，尤其是大数据应用难题，包括超大规模数据的存储。 （例如谷歌或Facebook每天为他们的用户收集万亿比特的数据）。这些类型的数据存储不需要固定的模式，无需多余操作就可以横向扩展。 Nosql能做什么1.易扩展：NoSQL数据库种类繁多，但是一个共同的特点都是去掉关系数据库的关系型特性。数据之间无关系，这样就非常容易扩展。也无形之间，在架构的层面上带来了可扩展的能力。 2.大数据量高性能：NoSQL数据库都具有非常高的读写性能，尤其在大数据量下，同样表现优秀。这得益于它的无关系性，数据库的结构简单。一般MySQL使用Query Cache，每次表的更新Cache就失效，是一种大粒度的Cache，在针对web2.0的交互频繁的应用，Cache性能不高。而NoSQL的Cache是记录级的，是一种细粒度的Cache，所以NoSQL在这个层面上来说就要性能高很多了 3.多样灵活的数据模型：NoSQL无需事先为要存储的数据建立字段，随时可以存储自定义的数据格式。而在关系数据库里，增删字段是一件非常麻烦的事情。如果是非常大数据量的表，增加字段简直就是一个噩梦 4.传统RDBMS VS NOSQL： RDBMS 高度组织化结构化数据 结构化查询语言（SQL） 数据和关系都存储在单独的表中。 数据操纵语言，数据定义语言 严格的一致性 基础事务 NoSQL 代表着不仅仅是SQL 没有声明性查询语言 没有预定义的模式-键 - 值对存储，列存储，文档存储，图形数据库 最终一致性，而非ACID属性 非结构化和不可预知的数据 CAP定理 高性能，高可用性和可伸缩性 Nosql数据模型简介KV键值 Bson（类似Json） 列族：顾名思义，是按列存储数据的。最大的特点是方便存储结构化和半结构化数据，方便做数据压缩，对针对某一列或者某几列的查询有非常大的IO优势。 图 NoSQL数据库的四大分类KV键值：新浪：BerkeleyDB+redis，美团：redis+tair，阿里、百度：memcache+redis 文档型数据库(bson格式比较多)：CouchDB，MongoDB。MongoDB 是一个基于分布式文件存储的数据库。由 C++ 语言编写。旨在为 WEB 应用提供可扩展的高性能数据存储解决方案。MongoDB 是一个介于关系数据库和非关系数据库之间的产品，是非关系数据库当中功能最丰富，最像关系数据库的。 列存储数据库：Cassandra, HBase，分布式文件系统 图关系数据库：社交网络，推荐系统等。专注于构建关系图谱，Neo4J, InfoGrid。 分类 举例 应用场景 数据模型 优点 缺点 键值（key-value） TokyoCabinet/TyrantRedisVoldemortOracle BDB 内容缓存，主要用于处理大量数据的高访问负载，也用于一些日志系统等等 Key指向Value的键值对，通常用hash table来实现 查找速度快 数据无结构化，通常只当作字符串或者二进制数据 列存储数据库 CassandraHBaseRiak 分布式的文件系统 以列簇式存储，将同一列数据存在一起 查找速度快，可扩展性强，更容易进行分布式扩展 功能相对局限 文档型数据库 CouchDBMongodb Web应用（与Key-Value类似，value是结构化的，不同的是数据库能够了解Value的内容） Key-Value对应的键值对，Value为结构化数据 数据结构要求不严格，表结构可变，不需要像关系型数据哭一样预先定义表结构 查询性能不高，而且缺乏统一的查询语法 图形数据库 Neo4JInfoGridInfiniteGraph 社交网络，推荐系统等，专注于构建关系图谱 图结构 利用图结构相关算法。比如最短路径寻址，N度关系查找等 很多时候需要对整个图做计算才能得出需要的信息，而且这种结构不太好做分布式的集群方案 CAP+Base关系型数据库遵循ACID规则事务在英文中是transaction，和现实世界中的交易很类似，它有如下四个特性： 1、A (Atomicity) 原子性原子性很容易理解，也就是说事务里的所有操作要么全部做完，要么都不做，事务成功的条件是事务里的所有操作都成功，只要有一个操作失败，整个事务就失败，需要回滚。比如银行转账，从A账户转100元至B账户，分为两个步骤：1）从A账户取100元；2）存入100元至B账户。这两步要么一起完成，要么一起不完成，如果只完成第一步，第二步失败，钱会莫名其妙少了100元。 2、C (Consistency) 一致性一致性也比较容易理解，也就是说数据库要一直处于一致的状态，事务的运行不会改变数据库原本的一致性约束。 3、I (Isolation) 独立性所谓的独立性是指并发的事务之间不会互相影响，如果一个事务要访问的数据正在被另外一个事务修改，只要另外一个事务未提交，它所访问的数据就不受未提交事务的影响。比如现有有个交易是从A账户转100元至B账户，在这个交易还未完成的情况下，如果此时B查询自己的账户，是看不到新增加的100元的 4、D (Durability) 持久性持久性是指一旦事务提交后，它所做的修改将会永久的保存在数据库上，即使出现宕机也不会丢失。 CAP的含义C:Consistency（强一致性） A:Availability（可用性） P:Partition tolerance（分区容错性） CAP的的3进2CAP理论就是说在分布式存储系统中，最多只能实现上面的两点。因此，根据 CAP 原理将 NoSQL 数据库分成了满足 CA 原则、满足 CP 原则和满足 AP 原则三 大类。而由于当前的网络硬件肯定会出现延迟丢包等问题，所以分区容忍性是我们必须需要实现的。所以我们只能在一致性和可用性之间进行权衡，没有NoSQL系统能同时保证这三点。 CA - 单点集群，满足一致性，可用性的系统，通常在可扩展性上不太强大。传统Oracle数据库 CP - 满足一致性，分区容忍必的系统，通常性能不是特别高。 Redis、Mongodb AP - 满足可用性，分区容忍性的系统，通常可能对一致性要求低一些。大多数网站架构的选择 注意：分布式架构的时候必须做出取舍。一致性和可用性之间取一个平衡。多余大多数web应用，其实并不需要强一致性。因此牺牲C换取P，这是目前分布式数据库产品的方向 一致性与可用性的决择对于web2.0网站来说，关系数据库的很多主要特性却往往无用武之地 数据库事务一致性需求 很多web实时系统并不要求严格的数据库事务，对读一致性的要求很低， 有些场合对写一致性要求并不高。允许实现最终一致性。 数据库的写实时性和读实时性需求 对关系数据库来说，插入一条数据之后立刻查询，是肯定可以读出来这条数据的，但是对于很多web应用来说，并不要求这么高的实时性，比方说发一条消息之 后，过几秒乃至十几秒之后，我的订阅者才看到这条动态是完全可以接受的。 对复杂的SQL查询，特别是多表关联查询的需求 任何大数据量的web系统，都非常忌讳多个大表的关联查询，以及复杂的数据分析类型的报表查询，特别是SNS类型的网站，从需求以及产品设计角 度，就避免了这种情况的产生。往往更多的只是单表的主键查询，以及单表的简单条件分页查询，SQL的功能被极大的弱化了。 BASE的含义BASE就是为了解决关系数据库强一致性引起的问题而引起的可用性降低而提出的解决方案。 BASE其实是下面三个术语的缩写： 基本可用（Basically Available） 软状态（Soft state） 最终一致（Eventually consistent） 它的思想是通过让系统放松对某一时刻数据一致性的要求来换取系统整体伸缩性和性能上改观。为什么这么说呢，缘由就在于大型系统往往由于地域分布和极高性能的要求，不可能采用分布式事务来完成这些指标，要想获得这些指标，我们必须采用另外一种方式来完成，这里BASE就是解决这个问题的办法 分布式系统分布式系统（distributed system）由多台计算机和通信的软件组件通过计算机网络连接（本地网络或广域网）组成。分布式系统是建立在网络之上的软件系统。正是因为软件的特性，所以分布式系统具有高度的内聚性和透明性。因此，网络和分布式系统之间的区别更多的在于高层软件（特别是操作系统），而不是硬件。分布式系统可以应用在在不同的平台上如：Pc、工作站、局域网和广域网上等。 简单来讲：1分布式：不同的多台服务器上面部署不同的服务模块（工程），他们之间通过Rpc/Rmi之间通信和调用，对外提供服务和组内协作。 2集群：不同的多台服务器上面部署相同的服务模块，通过分布式调度软件进行统一的调度，对外提供服务和访问。 Redis是什么——概念Redis:REmote DIctionary Server(远程字典服务器)是完全开源免费的，用C语言编写的，遵守BSD协议，是一个高性能的(key/value)分布式内存数据库，基于内存运行并支持持久化的NoSQL数据库，是当前最热门的NoSql数据库之一,也被人们称为数据结构服务器 Redis 与其他 key - value 缓存产品有以下三个特点： Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用 Redis不仅仅支持简单的key-value类型的数据，同时还提供list，set，zset，hash等数据结构的存储 Redis支持数据的备份，即master-slave模式的数据备份 能做啥 内存存储和持久化：redis支持异步将内存中的数据写到硬盘上，同时不影响继续服务 取最新N个数据的操作，如：可以将最新的10条评论的ID放在Redis的List集合里面 模拟类似于HttpSession这种需要设定过期时间的功能 发布、订阅消息系统 定时器、计数器 安装——基于Linux 下载获得redis-XXX.tar.gz后将它放入我们的Linux目录/opt /opt目录下，解压命令:tar -zxvf redis-XXX.tar.gz 进入目录:cd redis-XXX 在redis-XXX目录下执行make命令 如果make完成后继续执行make install 查看默认安装目录：usr/local/bin 修改redis.conf文件将里面的daemonize no 改成 yes，让服务在后台启动 将默认的redis.conf拷贝到自己定义好的一个路径下，比如/myconf /usr/local/bin目录下运行redis-server，运行拷贝出存放了自定义conf文件目录下的redis.conf文件 启动redis客户端：redis-cli -p 6379 测试连通性：输入ping 返回 PONG 测试成功 单实例关闭：redis-cli shutdown；多实例关闭，指定端口关闭:redis-cli -p 6379 shutdown 杂项知识 Redis 是单进程 默认16个数据库，初始默认使用零号库（Redis索引都是从零开始），统一密码管理，16个库都是同样密码，要么都OK要么一个也连接不上 select命令切换数据库 select 2切换到标号为2的库 dbsize查看当前数据库的key的数量 flushdb：清空当前库 Flushall：清空全部库 Redis数据类型Redis支持5大数据类型： string（字符串） string 是Redis的基本类型，一个key对应一个Value，String是二进制安全的，及时说Redis的String可以包含任何数据，比如jpg图片或者序列化的对象，一个Redis中字符串value最多是512M hash（哈希，类似java里的Map） Redis hash是一个键值对集合，是一个string类型的field和value的映射表，特别适合用于存储对象，类似于Java里面的Map&lt;String,Object&gt; list（列表） Redis List 是简单的字符串列表，按照插入的顺序排序，可以添加一个元素到列表的头部或者尾部，底层其实是一个链表 set（集合） 是String类型的无序集合，通过HashTable实现 zset(sorted set：有序集合) 和set一样也是String类型元素的集合，且不允许重复的成员。不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。zset的成员是唯一的，但分数score却可以重复。 每种数据类型相关命令请参考http://redisdoc.com/本文不再赘述 Redis配置文件配置文件放在解压目录中redis.conf 开头：配置文件开头定义了一些基本度量单位，只支持bytes不支持bit，对大小写不敏感 INCLUDE：Redis可以作为总闸，包含一些其他的配置文件 NETWORK：配置一些网络参数，如设置有效监听的客户端连接地址，监听端口等 tcp-backlog：backlog其实是一个链接队列，backlog队列总和=未完成三次握手队列+已完成三次握手队列 在高并发的环境下你需要一个高backlog值来避免慢客户端链接问题。Linux内核会将这个值减小到/proc/sys/net/core/somaxconn的值，所以需要确保提高somaxconn和tcp_max_syn_backlog两个值来达到目标效果 tcp-keepalive：间隔多长时间进行keepalive检测，从Redis3.2.1之后默认值为300此前为60 (不同版本Redis配置文件结构不一样，老版本没有此分类，原来放在GENERAL里面) GENERAL： daemonize 后台启动 loglevel 设置日志级别，级别越高日志越多越详细 logfile 日志文件 databases 默认Redis数据库数量——16个 SNAPSHOTTING：RDB的持久化配置将Redis数据库保存在磁盘上，详见本文持久化部分 REPLICATION：设置主从复制 SECURITY：访问密码的查看、设置和取消。默认Redis是没有密码的，因为它相信安装环境Linux是安全的 LIMITS：对Redis进行一些限制，如最大连接客户端数量，使用内存等 APPEND ONLY MODE：Redis AOF持久化的一些配置，详见本文持久化部分 Redis持久化Redis 有两种持久化技术RDB和AOF http://redisdoc.com/topic/persistence.html 推荐参考官方文档 RDB(Redis DataBase)在指定的时间间隔内将内存中的数据集快照写入磁盘，也就是Snapshot，它恢复时是将快照文件直接读到内存里。 Redis会单独创建（fork）一个子进程来进行持久化，会先将数据写入到一个临时文件中，待持久化过程都结束了，再用这个临时文件替换上次持久化好的文件。整个过程中，主进程是不进行任何IO操作的，这就确保了极高的性能如果需要进行大规模数据的恢复，且对于数据恢复的完整性不是非常敏感，那RDB方式要比AOF方式更加的高效。RDB的缺点是最后一次持久化后的数据可能丢失。 Fork：fork的作用是复制一个与当前进程一样的进程。新进程的所有数据（变量、环境变量、程序计数器等）数值都和原进程一致，但是是一个全新的进程，并作为原进程的子进程。 RDB 保存的是dump.rdb文件 如何触发RDB快照 Save命令：save时只管保存，其它不管，全部阻塞 bgsave命令：Redis会在后台异步进行快照操作，快照同时还可以响应客户端请求。可以通过lastsave命令获取最后一次成功执行快照的时间 flushall命令：也会产生dump.rdb文件，但里面是空的，无意义 RDB默认配置触发是： 1分钟改了1万次 5分钟改了10次 15分钟改了1次 如何恢复将备份文件 (dump.rdb) 移动到 redis 安装目录并启动服务即可 如何停止停止RDB保存规则的方法：redis-cli config set save “” AOF(Append Only File)以日志的形式来记录每个写操作，将Redis执行过的所有写指令记录下来(读操作不记录)，只许追加文件但不可以改写文件，redis启动之初会读取该文件重新构建数据，换言之，redis重启的话就根据日志文件的内容将写指令从前到后执行一次以完成数据的恢复工作。 Aof保存的是appendonly.aof文件 如何启动AOF修改默认的appendonly no，改为yes AOF损坏修复redis-check-aof —fix进行修复 rewriteAOF采用文件追加方式，文件会越来越大为避免出现此种情况，新增了重写机制,当AOF文件的大小超过所设定的阈值时，Redis就会启动AOF文件的内容压缩，只保留可以恢复数据的最小指令集.可以使用命令bgrewriteaof 重写原理 AOF文件持续增长而过大时，会fork出一条新进程来将文件重写(也是先写临时文件最后再rename)，遍历新进程的内存中数据，每条记录有一条的Set语句。重写aof文件的操作，并没有读取旧的aof文件，而是将整个内存中的数据库内容用命令的方式重写了一个新的aof文件，这点和快照有点类似 触发机制 Redis会记录上次重写时的AOF大小，默认配置是当AOF文件大小是上次rewrite后大小的一倍且文件大于64M时触发 总结RDB持久化方式能够在指定的时间间隔能对你的数据进行快照存储 AOF持久化方式记录每次对服务器写的操作,当服务器重启的时候会重新执行这些命令来恢复原始的数据,AOF命令以redis协议追加保存每次写的操作到文件末尾.Redis还能对AOF文件进行后台重写,使得AOF文件的体积不至于过大 只做缓存：如果你只希望你的数据在服务器运行的时候存在,你也可以不使用任何持久化方式. 同时开启两种持久化方式： 在这种情况下,当redis重启的时候会优先载入AOF文件来恢复原始的数据,因为在通常情况下AOF文件保存的数据集要比RDB文件保存的数据集要完整. RDB的数据不实时，同时使用两者时服务器重启也只会找AOF文件。那要不要只使用AOF呢？作者建议不要，因为RDB更适合用于备份数据库(AOF在不断变化不好备份)，快速重启，而且不会有AOF可能潜在的bug，留着作为一个万一的手段。 性能建议： 因为RDB文件只用做后备用途，建议只在Slave上持久化RDB文件， 如果使用AOF，好处是最恶劣的情况下只会丢失不超过2秒的数据，代价是持续的IO，AOF 的Rewrite过程中产生的新数据写到新文件造成的阻塞几乎是不可避免的，应尽量减少rewrite的频率，默认重写基础大小是64M太小，可以根据需求设置GB级别的写入大小。 如果不使用AOF，仅适用Master-Slave Replication实现高可用也可以，能省掉一大笔的IO，代价是如果M和S同时挂掉，会丢失十几分钟的数据，启动脚本只要比较两个RDB的文件载入较新的。新浪微博选用这种架构 Redis的事务事务流程 开启：以MULTI开始一个事务 入队：将多个命令入队到事务中，接到这些命令并不会立即执行，而是放到等待执行的事务队列里面 执行：由EXEC命令触发事务 事务执行分类 正常执行 放弃事务：DISCARD放弃事务执行 全体连坐：有一个错误全部失败 冤头债主：谁有错谁不执行，不影响其他的语句 watch监控 watch监控 乐观锁： 乐观锁(Optimistic Lock), 顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制判断是否修改。乐观锁适用于多读的应用类型，这样可以提高吞吐量，乐观锁策略:提交版本必须大于记录当前版本才能执行更新 悲观锁： 悲观锁(Pessimistic Lock), 顾名思义，就是很悲观，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁 Watch指令，类似乐观锁，事务提交时，如果Key的值已被别的客户端改变，比如某个list已被别的客户端push/pop过了，整个事务队列都不会被执行 通过WATCH命令在事务执行之前监控了多个Keys，倘若在WATCH之后有任何Key的值发生了变化，EXEC命令执行的事务都将被放弃，同时返回Nullmulti-bulk应答以通知调用者事务执行失败 Redis事务特性 单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 没有隔离级别的概念：队列中的命令没有提交之前都不会实际的被执行，因为事务提交前任何指令都不会被实际执行，也就不存在”事务内的查询要看到事务里的更新，在事务外查询不能看到”这个让人万分头痛的问题 不保证原子性：redis同一个事务中如果有一条命令执行失败，其后的命令仍然会被执行，没有回滚 Redis的发布和订阅进程间的一种消息通信模式：发送者(pub)发送消息，订阅者(sub)接收消息。 一次订阅多个：SUBSCRIBE c1 c2 c3（可以使用通配符，例如SUBSCRIBE news*） 消息发布 ：PUBLISH c2 helloworld 一般企业中发布消息中间件不会使用Redis去做，了解即可 Redis的复制也就是我们所说的主从复制，主机数据更新后根据配置和策略，自动同步到备机的master/slaver机制，Master以写为主，Slave以读为主 可以做什么：读写分离，容灾备份 使用遵循配从(库)不配主(库)的原则 从库配置：slaveof 主库IP 主库端口 每次与master断开之后，都需要重新连接，除非你配置进redis.conf文件(使用info replication 可以查看当先信息) 需要修改的配置： 拷贝多个redis.conf文件开启daemonize yespid文件名字指定端口log文件名字dump.rdb名字 常用配置结构 一主二仆：一个Master两个Slave，主机挂掉之后，剩下的机器还是slave状态，主机复活之后维持原来的主机地位，从机挂掉，除非是写过配置文件，否则不会恢复从机的身份。 薪火相传：上一个Slave可以是下一个slave的Master，Slave同样可以接收其他slaves的连接和同步请求，那么该slave作为了链条中下一个的master,可以有效减轻master的写压力 反客为主：SLAVEOF no one，使当前数据库停止与其他数据库的同步，转成主数据库 Redis复制原理slave启动成功连接到master后会发送一个sync命令。Master接到命令启动后台的存盘进程，同时收集所有接收到的用于修改数据集命令，在后台进程执行完毕之后，master将传送整个数据文件到slave,以完成一次完全同步 全量复制：而slave服务在接收到数据库文件数据后，将其存盘并加载到内存中。增量复制：Master继续将新的所有收集到的修改命令依次传给slave,完成同步但是只要是重新连接master,一次完全同步（全量复制)将被自动执行 哨兵模式反客为主的自动版，能够后台监控主机是否故障，如果故障了根据投票数自动将从库转换为主库 使用 新建sentinel.conf文件 填写内容： sentinel monitor 被监控数据库名字(自己起名字) 127.0.0.1 6379 1（1表示主机挂掉后salve投票看让谁接替成为主机，得票数多少后成为主机） 启动 redis-sentinel sentinel.conf 哨兵开始监控主机，主机挂掉选取主机，原主机重新启动之后将会变成Slave 一组sentinel能同时监控多个Master 复制的缺点由于所有的写操作都是先在Master上操作，然后同步更新到Slave上，所以从Master同步到Slave机器有一定的延迟，当系统很繁忙的时候，延迟问题会更加严重，Slave机器数量的增加也会使这个问题更加严重。 问题 Redis和Memcache区别对比？如何选择这两个技术？ 区别： 1） Redis和Memcache都是将数据存放在内存中，都是内存数据库。不过memcache还可用于缓存其他东西，例如图片、视频等等。 2）Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，hash等数据结构的存储。 3）虚拟内存—Redis当物理内存用完时，可以将一些很久没用到的value 交换到磁盘 4）过期策略—memcache在set时就指定，例如set key1 0 0 8,即永不过期。Redis可以通过例如expire 设定，例如expire name 10 5）分布式—设定memcache集群，利用magent做一主多从;redis可以做一主多从。都可以一主一从 6）存储数据安全—memcache挂掉后，数据没了；redis可以定期保存到磁盘（持久化） 7）灾难恢复—memcache挂掉后，数据不可恢复; redis数据丢失后可以通过aof恢复 8）Redis支持数据的备份，即master-slave模式的数据备份。 选型： 若是简单的存取key-value这样的数据用memcache好一些 若是要支持数据持久化，多数据类型(如集合、散列之类的)，用列表类型做队列之类的高级应用，就用redis Redis的持久化机制是什么？各自的优缺点？ redis提供两种持久化机制RDB和AOF机制。 1）RDB持久化方式： 是指用数据集快照的方式记录redis数据库的所有键值对。 优点： 1.只有一个文件dump.rdb，方便持久化。 2.容灾性好，一个文件可以保存到安全的磁盘。 3.性能最大化，fork子进程来完成写操作，让主进程继续处理命令，所以是IO最大化。 4.相对于数据集大时，比AOF的启动效率更高。 缺点： 1.数据安全性低。 2）AOF持久化方式： 是指所有的命令行记录以redis命令请求协议的格式保存为aof文件。 优点： 1.数据安全，aof持久化可以配置appendfsync属性，有always，每进行一次命令操作就记录到aof文件中一次。 2.通过append模式写文件，即使中途服务器宕机，可以通过redis-check-aof工具解决数据一致性问题。 3.AOF机制的rewrite模式。 缺点： 1.文件会比RDB形式的文件大。 2.数据集大的时候，比rdb启动效率低。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo]]></title>
    <url>%2F2018%2F01%2F05%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%2F%E6%9F%A5%E9%98%85%E5%B0%8F%E7%AC%94%E8%AE%B0%2Fhexo%2F</url>
    <content type="text"><![CDATA[ssh-keygen -t rsa -C “COKIDCC@outlook.com” ssh git@github.com]]></content>
  </entry>
  <entry>
    <title><![CDATA[docker笔记]]></title>
    <url>%2F2017%2F12%2F03%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%2F%E6%9F%A5%E9%98%85%E5%B0%8F%E7%AC%94%E8%AE%B0%2Fdocker%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[docker常用命令 Docker命令总结 命令 作用 docker pull 获取镜像 docker build 创建镜像 docker commit 保存改动的镜像 docker images 列出所有镜像 docker rmi 删除镜像 docker run 运行容器 docker ps 列出正在运行的容器 docker rm 删除容器 docker cp 在host和container之间拷贝文件 docker inspect 列出容器信息 docker search 搜索registry上的镜像 docker pull 拉去镜像 docker push 上传镜像 docker exec 在容器中执行命令 docker exec -i保证输入有效 -t分配伪终端 docker run常用参数 参数 用法 -p 开放容器端口到映射的主机的端口上 -d 后台运行 -P 随机开放宿主机端口映射到容器中 docker build 参数 参数 用法 -t 指定构建镜像的名字以及标签 名字:标签 Dockerfile语法 命令 用途 FROM 基础镜像来源 RUN 执行命令 ADD 添加文件 COPY 拷贝文件 CMD 执行命令 EXPOSE 暴露端口 WORKDIR 制定工作路径 MAINTAINER 维护者 ENV 设定环境变量 ENTRYPOINT 容器入口 USER 指定执行命令用户 VOLUME 容器挂载的卷 Docker网络Docker提供了三种网络，分别是Bridge,Host,None。 桥接模式：会自动虚拟出独立的network namespace，虚拟网卡，虚拟网卡和网桥相连，此模式下可以进行端口映射—就是访问宿主机的端口时会映射到容器里面的端口 主机模式：直接使用宿主机的ip端口，直接连接到主机的网卡 None：不会和外界通讯 Dockerfile中的每一行都产生一个新层]]></content>
      <tags>
        <tag>-技术 -Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows下安装Ruby防坑]]></title>
    <url>%2F2017%2F11%2F07%2F%E5%AE%89%E8%A3%85%E7%BB%8F%E9%AA%8C%2FWindows%E4%B8%8B%E5%AE%89%E8%A3%85Ruby%E9%98%B2%E5%9D%91%2F</url>
    <content type="text"><![CDATA[刚从图书馆借了一本Ruby书，准备在电脑安一下真是到处都是坑 不要安最新版！！！这是最坑的地方，Windows下用rubyinstaller安装官网最新版本是2.4的，这个版本和以前版本安装的时候有少许不一样，我一开始就是安装的这个搞了一晚上。 一开始我安上ruby以为就完事了，可是RubyMine报了一个错说是没有rubocop。 我就用他自动安装修复一下，修了好长时间不管用，用命令gem install rubocop还是不管用（gem是类似python里面pip用来安装模块的），老是报 Failed to build gem native extension. 这个错。 找了好多资料最后发现说是需要DevKit这个包，我装了之后还是不管事最后回归官网发现DEVELOPMENT KIT下面有一句话For use with Ruby 2.0 to 2.3。。。。。。。。。。。。。 如何安装ruby说完辛酸史，说怎么装。其实很简单（过来人都这样说） 下载地址https://rubyinstaller.org/downloads/ 不要选2.4及以上版本，博主选的是2.2 (×64) 页面往下拉下载DEVELOPMENT KIT 安装RubyInstallers，最好选择没有空格的路径，安装时勾选就全选上 安装DEVELOPMENT KIT这个其实就是一个解压过程，新建个目录解压进去就行，还是最好不要有空格 ruby会自动添加环境变量进入cmd运行 ruby -v 测试 然后CMD切换到你解压的DEVELOPMENT KIT(下称DevKit)的目录 运行ruby dk.rb init 命令 提示让你修改config.yml文件，其实就是把你安装ruby的绝对路径加进去，而且注释中已经给了例子例子中有两个目录，第一个应该是ruby所在目录，第二个应该是你想要安装你解压的DevKit的目录，博主将两个目录都写成了Ruby所在目录 改完后再运行 ruby dk.rb install 当然还是在DevKit目录中 之后运行 gem install rubocop 就能安装了，这会打开RubyMine就不报错了。以后安装其他扩展包也能顺利了]]></content>
      <categories>
        <category>安装</category>
      </categories>
      <tags>
        <tag>Ruby</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微信小程序吐槽]]></title>
    <url>%2F2017%2F11%2F05%2F%E6%97%A5%E5%B8%B8%2F%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F%E5%90%90%E6%A7%BD%2F</url>
    <content type="text"><![CDATA[最近有时间搞一下微信小程序最后决定弃坑 微信小程序api很不成熟，现在还一直处于功能开发阶段。我谈一下我用的时候感受吧。 首先是IDE，我还不如用vscode下一个.wxml文件关联插件写，快捷键少得可怜快速复制一行都没有，你一个前端框架肯定要复制上一行的内容的情况不少。 其次最脑残的设计就是几乎每一个步骤都要微信扫码，登录IDE，登陆微信开发者平台（我输对了密码你还要我扫码，要密码有什么用），进去了是吧，还不算完更改几乎每一个配置都要扫码。 最后让我决定弃坑的就是发送网络请求的部分。我一个request请求还要去你网页端后台注册（还最多是5个），注册完还不说，还必须是https请求，我想去API商城调用一个查手机号码归属地的请求要什么https。 好吧我觉得这个微信小程序还是不适合个人开发者，可能只适用于企业，反正我是玩不起了，我都懒得截图了截图又要扫好多好多码。 建议玩一下适可而止，个人开发我是不看的好的，限制太多，有时间还是正儿八经去学安卓。微信小程序写起来和你写网页前端一个样。]]></content>
      <categories>
        <category>日常</category>
      </categories>
      <tags>
        <tag>吐槽</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式]]></title>
    <url>%2F2017%2F10%2F24%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%2F%E6%9F%A5%E9%98%85%E5%B0%8F%E7%AC%94%E8%AE%B0%2F%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[最近正在学正则表达式，总结一下语法，便于以后查找 基本字符 语法 描述 示例 . 匹配任何单个字符 \ 转义用 [] 匹配字符为[]里面的东西 [abcd] - 连续字符简写 a-z [0-9] ^ 取非 0-9除了0-9 + 想要匹配的字符多次重复—至少一次 [0-9]+ * 想要匹配的字符多次重复—可以为0次 [0-9]* ? 想要匹配的字符有一次或零次，等价于{0,1} https?:// {} 前面字符或字符集合重复次数 {3}, {1,3}, {3, } *? 懒惰元字符，匹配尽量少的字符防止过度匹配 &lt; B &gt;&lt;/ B&gt; and &lt; B &gt; &lt; / B &gt; +? 同上 如果不使用?将匹配&lt; B&gt;&lt; /B&gt; and &lt; B&gt;&lt;/ B&gt; {n, }? 同行 使用?将匹配&lt; B&gt;&lt; /B&gt; &lt; B&gt;&lt;/ B&gt; ^ 匹配一个字符串的开头 $ 匹配一个字符串的结尾 (?m) 把行分隔符当作字符串分隔符看待(类似于^)，必须出现在开头的位置,许多正则表达式不支持 (?m)^\s //.$匹配代码中的注释// () 子表达式，将被视为一个独立的元素 (\d{1,3}\ .){3}\d{1,3}匹配ip地址 \ 或 19\ 20 19或20 \1 \2 回溯引用，代表前边第一个（第二个）子表达式，javaScript用&amp;代替\ ?&lt;= 向后查找操作符，javaScript不支持 (?&lt;=\$)[0-9.]+只要找到 $但不显示 ?= 向前查找操作符。?=后面的东西找到就行了但是不显示它，术语就是”不消费“它。写在子表达式中 (?=:)只找到:但是不显示它 ?&lt;! 负向后查找 \b(?&lt;!\$)\d+\b匹配不以$开头的数值 ?! 负向前查找 \r\n\r\n 匹配回车和换行—匹配一个空行 特殊含义字符 语法 描述 示例 \d 等价于[0-9] \D 等价于[ ^0-9 ] \w 字母数字下划线[a-zA-Z0-9_] \W 字母数字下划线[ ^a-zA-Z0-9_ ] \s 任何一个空白字符 等价于[\f\n\r\t\v] \S 任何一个非空白字符 等价于[ ^ \f\n\r\t\v] \b 匹配一个单词的开头或结尾 \bcat\b 只匹配cat 不匹配scatter \B 不匹配一个单词的边界 \B-\B 匹配 - \E 放在结束位置，结束\L或\U转换 \l 把下一个字符转换成小写 \L 把\L到\E之间的字符全部转换成小写 \u 把下一个字符转换成大写 \U 把\U到\E之间的字符全部转换成大写 \f换页符 \v 垂直制表符 常用正则表达式 用途 表达式 用户名为3-16位数字英文_- 字符组合 /^[a-z0-9_-]{3,16}$/ 密码 /^[a-z0-9_-]{6,18}$/ URL /^(https?:\/\/)?([\da-z.-]+).([a-z.]{2,6})([\/\w .-])\/?$/ 电子邮箱 /^([a-z0-9_.-]+)@([\da-z.-]+).([a-z.]{2,6})$/ 校验基本日期格式 var reg1 = /^\d{4}(-\ \/\ .)\d{1,2}\1\d{1,2}$/; var reg2 = /^(^(\d{4}\ \d{2})(-\ \/\ .)\d{1,2}\3\d{1,2}$)\ (^\d{4}年\d{1,2}月\d{1,2}日$)$/; 校验密码强度 密码的强度必须是包含大小写字母和数字的组合，不能使用特殊字符，长度在8-10之间。var reg = /^(?=.*\\\d)(?=.*[a-z])(?=.*[A-Z]).{8,10}$/; 校验中文 var reg = /^[\\\u4e00-\\\u9fa5]{0,}$/; 由数字、26个英文字母或下划线组成的字符串 var reg = /^\\\w+$/; 校验E-Mail 地址 var reg =/[\\w!#$%&amp;’*+/=?^_`{ }~-]+(?:\\\.[\\\\w!#$%&amp;’*+/=?^_`{\ }~-]+)*@(?:[\\\w](?:[\\\w-]*[\\\w])?\\\.)+[\\\w](?:[\\\w-]*[\\\w])?/; 校验身份证号码 var reg = /^[1-9]\\\d{5}[1-9]\\\d{3}((0\\\d)\ (1[0-2]))(([0\ 1\ 2]\\\d)\ 3[0-1])\\\d{3}([0-9]\ X)$/; “yyyy-mm-dd” 格式的日期校验，已考虑平闰年。 var reg = /^(?:(?!0000)[0-9]{4}-(?:(?:0[1-9]\ 1[0-2])-(?:0[1-9]\ 1[0-9]\ 2[0-8])\ (?:0[13-9]\ 1[0-2])-(?:29\ 30)\ (?:0[13578]\ 1[02])-31)\ (?:[0-9]{2}(?:0[48]\ [2468][048]\ [13579][26])\ (?:0[48]\ [2468][048]\ [13579][26])00)-02-29)$/; 金额校验，精确到2位小数。 var reg = /^[0-9]+(.[0-9]{2})?$/; 校验手机号 var reg = /^(13[0-9]\ 14[5\ 7]\ 15[0\ 1\ 2\ 3\ 5\ 6\ 7\ 8\ 9]\ 18[0\ 1\ 2\ 3\ 5\ 6\ 7\ 8\ 9])\\d{8}$/; 判断IE的版本 var reg = /^.*MSIE [5-8](?:\\\.[0-9]+)?(?!.*Trident\\\\/[5-9]\\\.0).*$/; 校验IP-v4地址 var reg = /\\\b(?:(?:25[0-5]\ 2[0-4][0-9]\ [01]?[0-9][0-9]?)\\\.){3}(?:25[0-5]\ 2[0-4][0-9]\ [01]?[0-9][0-9]?)\\\b/; 校验IP-v6地址 var reg = /(([0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}\ ([0-9a-fA-F]{1,4}:){1,7}:\ ([0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}\ ([0-9a-fA-F]{1,4}:){1,5}(:[0-9a-fA-F]{1,4}){1,2}\ ([0-9a-fA-F]{1,4}:){1,4}(:[0-9a-fA-F]{1,4}){1,3}\ ([0-9a-fA-F]{1,4}:){1,3}(:[0-9a-fA-F]{1,4}){1,4}\ ([0-9a-fA-F]{1,4}:){1,2}(:[0-9a-fA-F]{1,4}){1,5}\ [0-9a-fA-F]{1,4}:((:[0-9a-fA-F]{1,4}){1,6})\ :((:[0-9a-fA-F]{1,4}){1,7}\ :)\ fe80:(:[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}\ ::(ffff(:0{1,4}){0,1}:){0,1}((25[0-5]\ (2[0-4]\ 1{0,1}[0-9]){0,1}[0-9])\\\.){3,3}(25[0-5]\ (2[0-4]\ 1{0,1}[0-9]){0,1}[0-9])\ ([0-9a-fA-F]{1,4}:){1,4}:((25[0-5]\ (2[0-4]\ 1{0,1}[0-9]){0,1}[0-9])\\\.){3,3}(25[0-5]\ (2[0-4]\ 1{0,1}[0-9]){0,1}[0-9]))/; 检查URL的前缀 if (!s.match(/^[a-zA-Z]+:\/\//)) { s = ‘http://‘ + s; } 提取URL链接 var reg = /^(f\ ht){1}(tp\ tps):\/\/([\w-]+.)+[\w-]+(\/[\w- ./?%&amp;=]*)?/; 文件路径及扩展名校验 var reg = /^([a-zA-Z]\\\\:\ \\\\\\\)\\\\\\\\([^\\\]+\\\)*[^\\\/:*?”&lt;&gt;\ ]+\\\.txt(l)?$/; 提取Color Hex Codes var reg = /^#([A-Fa-f0-9]{6}\ [A-Fa-f0-9]{3})$/; 提取网页图片 var reg = /\\\\&lt; *[img][^\\\>]*[src] *= *[\\\\”\’]{0,1}([^\\\”\’\ &gt;]*)/; 提取页面超链接 var reg = /(\]*)(href=”https?:\/\/)((?!(?:(?:www\\\.)?’.implode(‘\ (?:www\\\.)?’, $follow_list).’))[^”]+)”((?!.*\\\brel=)[^&gt;]*)(?:[^&gt;]*)&gt;/; 查找CSS属性 var reg = /^\\\s*[a-zA-Z\\\-]+\\\s*[:]{1}\\\\s[a-zA-Z0-9\\\s.#]+[;]{1}/; 抽取注释 var reg = /\/;]]></content>
      <categories>
        <category>查阅</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中德教务系统总结]]></title>
    <url>%2F2017%2F10%2F23%2F%E6%97%A5%E5%B8%B8%2F%E4%B8%AD%E5%BE%B7%E6%95%99%E5%8A%A1%E7%B3%BB%E7%BB%9F%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[中德教务系统开发完成之后发现很多知识点需要学习 RESTful 风格的乱用 虽然知道需要用RESTful风格来请求链接，但是还是一知半解，项目中链接还是写的很混乱。正好前些天读了篇博客如何正确书写RESTful 风格的链接 http://blog.jobbole.com/112710 tomcat因为自身的原因前台发送的DELETE和UPDATE请求会被忽略掉因此需要配置Spring使其支持RESTful风格的请求 正则表达式完全不会 以前看资料也发现使用正则表达式可以有效快速验证前台请求，邮箱用户名等是否符合要求。最近借了一本《正则表达式必会知识》在补 对于企业级网站知之甚少 因为学长推荐读了《大型分布式网站架构设计与实践》发现自己做的项目，和企业实际开发还有很多差距。看之前兴趣补了一点“redis”的基础知识，然后再看这本书会更好理解一些]]></content>
      <categories>
        <category>日常</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[kotlin入坑]]></title>
    <url>%2F2017%2F10%2F17%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%2F%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93%2Fkotlin%E5%85%A5%E5%9D%91%2F</url>
    <content type="text"><![CDATA[谷歌钦定kotlin为开发安卓的官方语言，和java无死角衔接，而且自kotlin1.1之后，JavaScript ⽬标平台不再是实验性的所有语⾔功能都⽀持，一直想学习但是一直没时间。前段时间又在搞JVM，学了一半又扔了（其实是在很难看懂）。 本来想买本书看kotlin，最后发现还是直接看官方文档最好（关键是有中文版的,还不用花钱） 中文版网站：https://www.kotlincn.net/ 接下来的几篇博文将会逐一介绍kotlin的特性语法（更新看心情） Kotlin概述Kotlin可以进行以下方向的开发 使⽤ Kotlin 进⾏服务器端开发 使⽤ Kotlin 进⾏ Android 开发(也是谷歌推举的) Kotlin&amp;JavaScript Kotlin1.1新特性 首先就是正式支持JavaScript 协程（实验性的） 语言性的功能 类型别名 1typealias OscarWinners = Map&lt;String, String&gt; 已绑定的可调⽤引⽤ 123//现在可以使⽤ :: 操作符来获取指向特定对象实例的⽅法或属性的成员引⽤。以前这只能⽤ lambda 表达式表⽰。这⾥有⼀个例⼦：val numberRegex = "\\d+".toRegex()val numbers=listOf("abc","123","456").filter(numberRegex::matches) 密封类和数据类 lambda 表达式中的解构 现在可以使⽤解构声明语法来解开传递给 lambda 表达式的参数。这⾥有⼀个例⼦ 12345val map = mapOf(1 to "one", 2 to "two")// 之前println(map.mapValues &#123; entry -&gt;val (key, value) = entry "$key -&gt; $value!"&#125;)// 现在println(map.mapValues &#123; (key, value) -&gt; "$key -&gt; $value!" &#125;) 有时候在开发当中需要用到key这样可以很方便的取出来 下划线⽤于未使⽤的参数 12//对于具有多个参数的 lambda 表达式，可以使⽤ _ 字符替换不使⽤的参数的名称： map.forEach &#123; _, value -&gt; println("$value!") &#125; 当然对于上面的解构也是可以使用的 数字字⾯值中的下划线 123456//正如在 Java 8 中⼀样，Kotlin 现在允许在数字字⾯值中使⽤下划线来分隔数字分组：val oneMillion = 1000000val hexBytes = 0xFF_EC_DE_5Eval bytes = 0b110100100110100110010100_10010010 对于属性的更短语法 1234//对于没有⾃定义访问器、或者将 getter 定义为表达式主体的属性，现在可以省略属性的类型：data class Person(val name: String, val age: Int) &#123;val isAdult get() = age &gt;= 20 // 属性类型推断为 “Boolean”&#125; 内联属性访问器 123//如果属性没有幕后字段，现在可以使⽤ inline 修饰符来标记该属性访问器。这些访问器的编译⽅式与内联函数相同。public val &lt;T&gt; List&lt;T&gt;.lastIndex: Intinline get() = this.size - 1 局部委托属性 委托属性绑定的拦截 泛型枚举值访问 12345//现在可以⽤泛型的⽅式来对枚举类的值进⾏枚举：enum class RGB &#123; RED, GREEN, BLUE &#125;inline fun &lt;reified T : Enum&lt;T&gt;&gt; printAllValues() &#123;print(enumValues&lt;T&gt;().joinToString &#123; it.name &#125;)&#125; 对于 DSL 中隐式接收者的作⽤域控制 rem 操作符 标准库 字符串到数字的转换 1234//在 String 类中有⼀些新的扩展，⽤来将它转换为数字，⽽不会在⽆效数字上抛出异常：String.toIntOrNull(): Int? 、String.toDoubleOrNull(): Double? 等。val port = System.getenv("PORT")?.toIntOrNull() ?: 80//还有整数转换函数，如 Int.toString() 、String.toInt() 、String.toIntOrNull() ，每个都有⼀个带有 radix 参数的重载，它允许指定转换的基数（2 到 36）。 顺便提一句kotlin没有提供类型的隐式转换，必须使用显示转换转换所需类型。 类似数组的列表实例化函数 抽象集合 数组处理函数 JVM后端 Java 8 字节码⽀持 Kotlin 现在可以选择⽣成 Java 8 字节码（命令⾏选项 -jvm-target 1.8 或者Ant/Maven/Gradle 中 的相应选项）。⽬前这并不改变字节码的语义（特别是，接⼝和 lambda 表达式中的默认⽅法 的⽣成与 Kotlin 1.0 中完全⼀样），但我们计划在以后进⼀步使⽤它。 Java 8 标准库⽀持 现在有⽀持在 Java 7 和 8 中新添加的 JDK API 的标准库的独⽴版本。如果你需要访问新的 API，请使⽤ kotlin-stdlib-jre7 和 kotlinstdlib-jre8 maven 构件，⽽不是标准的 kotlin-stdlib 。这些构件是在 kotlin-stdlib 之上的微⼩扩展，它们将它作为传递依赖项带到项⽬中。 字节码中的参数名 Kotlin 现在⽀持在字节码中存储参数名。这可以使⽤命令⾏选项 -java-parameters 启⽤。 常量内联 可变闭包变量 javax.scripting ⽀持 为 Java 9 ⽀持准备，在 kotlin-reflect.jar 库中的扩展函数和属性已移动 到 kotlin.reflect.full 包中。旧包（ kotlin.reflect ）中的名称已弃⽤，将在 Kotlin 1.2 中删除。请注意，核⼼反射接⼝（如 KClass ）是 Kotlin 标准库（⽽不是 kotlin-reflect ）的⼀部分，不受移动影响。 JavaScript后端 统⼀的标准库 更好的代码⽣成 external 修饰符 改进的导⼊处理 总结以上特性示例都引自于官方文档https://www.kotlincn.net/docs/kotlin-docs.pdf 有些部分并未给出示例，因为有些地方博主知识有限并未能理解，因此并未粘贴，kotlin部分博客主要用于博主学习总结。 接下来的几篇博文将对具体有趣的kotlin特性进行单独介绍。!]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Kotlin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java细节]]></title>
    <url>%2F2017%2F03%2F30%2Fjava%2Fjava%E7%BB%86%E8%8A%82%2F</url>
    <content type="text"><![CDATA[java细节 java访问修饰符 访问级别 访问控制修饰符 同类 同包 子类 不同的包 公开 public √ √ √ √ 受保护 protected √ √ √ — 默认 没有访问控制修饰符 √ √ — — 私有 private √ — — — 接口修饰符 Java接口中的方法并非没有修饰符，而是省略了。在Java的接口中，只有常量和抽象方法。所有的数据域均用 public static final 修饰，所有的方法都用 public abstract 修饰，所以修饰符是可以省略的。 Java 8 中，接口中可以定义默认方法和静态方法了。 instanceof关键字 A instanceof B判断A是否是B类型 transient ​ java语言的关键字，变量]修饰符，如果用transient声明一个实例变量，当对象存储时，它的值不需要维持。换句话来说就是，用transient关键字标记的成员变量不参与序列化过程。 通过函数传递对象，函数不能修改对象的引用地址（意思就是当把一个类传递给函数的时候，是把对象的引用地址复制了一份给了函数参数，函数参数对象，能对当前地址进行操作，但是不能指向新的地址空间）。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>细节</tag>
      </tags>
  </entry>
</search>
